{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGSW6CxQ9pA2"
      },
      "source": [
        "# Project 3a: Self-Attention and Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZhBwjM89pA4"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# Install required packages\n",
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiossM_m9pA5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4MgKJn9pA5"
      },
      "source": [
        "## 1.1: Implementing Self-Attention from Scratch\n",
        "\n",
        "This assignment is adapted from the code by Yegor Kuznetsov, Liwei Jiang, and Jaehun Jung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1Wt8TMt9pA5"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao0Rzn5y9pA5"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you.\n",
        "######################################################\n",
        "\n",
        "\n",
        "def MHA_wrapper(query, key, value, n_heads=1, causal=False):\n",
        "    \"\"\"\n",
        "    This is a wrapper around the PyTorch implementation of multi-head attention.\n",
        "    You will use this implementation to compare to your implementation for code testing.\n",
        "    \"\"\"\n",
        "    assert query.shape == key.shape == value.shape\n",
        "    _, n_tok, n_embd = query.shape\n",
        "\n",
        "    query = query.transpose(0, 1)\n",
        "    key = key.transpose(0, 1)\n",
        "    value = value.transpose(0, 1)\n",
        "\n",
        "    in_proj_weight = torch.eye(n_embd, dtype=key.dtype, device=key.device).repeat(\n",
        "        (3, 1)\n",
        "    )\n",
        "    out_proj_weight = torch.eye(n_embd, dtype=key.dtype, device=key.device)\n",
        "\n",
        "    attn_mask = None\n",
        "    if causal:\n",
        "        attn_mask = torch.tril(\n",
        "            torch.ones(n_tok, n_tok, dtype=bool, device=key.device)\n",
        "        ).logical_not()\n",
        "\n",
        "    out, _ = F.multi_head_attention_forward(\n",
        "        query,\n",
        "        key,\n",
        "        value,\n",
        "        n_embd,\n",
        "        n_heads,\n",
        "        in_proj_weight=in_proj_weight,\n",
        "        in_proj_bias=None,\n",
        "        bias_k=None,\n",
        "        bias_v=None,\n",
        "        add_zero_attn=False,\n",
        "        dropout_p=0,\n",
        "        out_proj_weight=out_proj_weight,\n",
        "        out_proj_bias=None,\n",
        "        attn_mask=attn_mask,\n",
        "        need_weights=False,\n",
        "    )\n",
        "\n",
        "    return out.transpose(0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61ZrfGDK9pA5"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you.\n",
        "######################################################\n",
        "\n",
        "# use cpu for now\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "# make these bigger if you want a stricter test of your code\n",
        "part1_n_tok = 10\n",
        "part1_n_emb = 6\n",
        "\n",
        "# generate fixed pseudo-random Q,K,V for testing attn function\n",
        "torch.manual_seed(447)\n",
        "\n",
        "# Initialize random testing Q,K,V\n",
        "part1_key = torch.randn(1, part1_n_tok, part1_n_emb)\n",
        "part1_value = torch.randn(1, part1_n_tok, part1_n_emb)\n",
        "part1_query = torch.randn(1, part1_n_tok, part1_n_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvZR2eDZ9pA6"
      },
      "source": [
        "## Step 0: Set up the projections for attention.\n",
        "**You will complete the following code blocks denoted by `TODO:`.** For now you don't need to implement anything here, as you go through the following steps, you will keep coming back to this cell and fill in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b9ddabd8d7c4616b34b5146c9a3f57bc",
          "grade": false,
          "grade_id": "cell-452a25f05b3731e0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "GraiB0_M9pA6"
      },
      "outputs": [],
      "source": [
        "def init_qkv_proj(n_embd: int):\n",
        "    \"\"\"\n",
        "    This function is given to you.\n",
        "    :return: A tuple of length 3 containing the projections for Q, K, V.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "    )\n",
        "\n",
        "def self_attention(Q, K, V, n_heads=1, causal=True):\n",
        "    \"\"\"\n",
        "    Self-attention block.\n",
        "\n",
        "    Note: You will keep coming back to this cell and fill in more of this function\n",
        "    after completing each of the following steps! Don't forget to re-run this\n",
        "    cell each time you change it. Make sure that once you're done, all the testing\n",
        "    cells should work.\n",
        "\n",
        "    :return: A tensor containing the result of the self-attention operation.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    assert Q.shape == K.shape == V.shape\n",
        "    B, n_tok, n_embd = Q.shape\n",
        "\n",
        "    # TODO: Step 3 -- split heads\n",
        "    if n_heads > 1:\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # TODO: Step 1 -- Calculate raw attention scores, i.e., before softmax\n",
        "    # Hint: you need two lines here.\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "    # TODO: Step 2 -- create and apply the causal mask to attention.\n",
        "    if causal:\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # TODO: Step 1 -- softmax the raw attention and use it to get outputs.\n",
        "    # Hint: you need two lines here.\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    # TODO: Step 3 -- merge heads.\n",
        "    if n_heads > 1:\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "    # output should have the same shape as input\n",
        "    assert y.shape == (B, n_tok, n_embd)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipoxp2b9pA6"
      },
      "source": [
        "## Step 1: Implement the core components of attention.\n",
        "\n",
        "Note that a self attention block consists of the following computations:\n",
        "\n",
        "Given a set of query vectors $Q$, key vectors $K$, and value vectors $V$,\n",
        "\n",
        "Compute the raw attention scores $A$ as:\n",
        "$$ A = \\frac{1}{\\sqrt{d_{\\text{head}}}}QK^T $$\n",
        "\n",
        "Apply a softmax to the scaled scores to get the attention weights:\n",
        "$$ A = \\texttt{softmax} (A) $$\n",
        "\n",
        "Compute the output vectors by taking a weighted sum of the value vectors, weighted by the attention weights:\n",
        "$$ O = AV $$\n",
        "\n",
        "Note that $d_{\\text{head}}$ is the dimensionality of the feature vectors i.e. query, key, and value vectors and is given by $\\frac{d_{\\text{model}}}{n_{\\text{heads}}}$, where $d_{\\text{model}}$ is the embedding dimension of the model and $n_{\\text{heads}}$ is the number of attention heads.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYAjzphQ9pA6"
      },
      "source": [
        "Useful tips:\n",
        "- You can use the `@` operator to perform matrix multiplication. Alternatively, you can use the `torch.matmul` function.\n",
        "- For the batched cases, for e.g., you have a tensor X of shape (B, m, n) and another tensor Y of shape (B, n, m), then X@Y performs the matrix multiplication for each of the batch elements, giving you a tensor of shape (B, m, m).\n",
        "- You can use the `transpose` method to transpose matrices in pytorch. If you have a tensor X of shape (B, n, m), then X.transpose(-2, -1) gives you a tensor of shape (B, m, n).\n",
        "- You might find the `F.softmax` function useful. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) for more details.\n",
        "- Softmax is applied along the last dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uZekmx29pA7"
      },
      "source": [
        "**You will complete the following code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7494548dfc26db5544b8ca7c0cf6989f",
          "grade": false,
          "grade_id": "cell-92dccd6159197a28",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QtYjCDZt9pA7"
      },
      "outputs": [],
      "source": [
        "def pairwise_similarities(Q, K):\n",
        "    \"\"\"\n",
        "    Dot product attention is computed via the dot product between each query and each key.\n",
        "\n",
        "    Inputs:\n",
        "    - Q: torch.Tensor, shape (B, n_tok, n_embd) or (B, n_heads, n_tok, n_embd) containing the queries, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens, and n_embd is the embedding dimension.\n",
        "    - K: torch.Tensor, shape (B, n_tok, n_embd) or (B, n_heads, n_tok, n_embd) containing the keys, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens, and n_embd is the embedding dimension.\n",
        "\n",
        "    :return: The raw attention scores, A = QK^T.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def attn_scaled(A, d_model: float, n_heads: float):\n",
        "    \"\"\"\n",
        "    Scale the raw attention scores.\n",
        "    Inputs:\n",
        "    - A: torch.Tensor, shape (B, n_tok, n_tok) or (B, n_heads, n_tok, n_tok) containing the raw attention scores, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens.\n",
        "    - d_model: int, the embedding dimension.\n",
        "    - n_heads: int, the number of attention heads.\n",
        "    :return: Scaled raw attention scores.\n",
        "\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def attn_softmax(A):\n",
        "    \"\"\"\n",
        "    Normalize the scaled raw attention scores with softmax.\n",
        "    Inputs:\n",
        "    - A: torch.Tensor, shape (B, n_tok, n_tok) or (B, n_heads, n_tok, n_tok) containing the scaled raw attention scores, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens.\n",
        "    :return: Normalized attention scores, A' = softmax(A).\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def compute_outputs(A, V):\n",
        "    \"\"\"\n",
        "    Get outputs as a weighted sum of values by attention scores, using matrices.\n",
        "    Inputs:\n",
        "    - A: torch.Tensor, shape (B, n_tok, n_tok) or (B, n_heads, n_tok, n_tok) containing the normalized attention scores, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens.\n",
        "    - V: torch.Tensor, shape (B, n_tok, n_embd) or (B, n_heads, n_tok, n_embd) containing the value vectors, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens, and n_embd is the embedding dimension.\n",
        "    :return: weighted sum of values by attention scores.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDHiZg7t9pA7"
      },
      "source": [
        "## Test 1: Building Single-headed Self-attention without Masking\n",
        "\n",
        "Implement lines denoted by `Step 1` in `self-attention()`, and run the following test code to verify your implementation produces close enough results to the PyTorch implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIGq-fgn9pA7"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "\n",
        "out_A = self_attention(part1_query, part1_key, part1_value, n_heads=1, causal=False)\n",
        "out_B = MHA_wrapper(part1_query, part1_key, part1_value, n_heads=1, causal=False)\n",
        "assert out_A.shape == out_B.shape == part1_query.shape\n",
        "\n",
        "print(\"max diff:\", (out_A - out_B).abs().max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVvPq_G99pA7"
      },
      "source": [
        "Max difference should be very small, our reference implementation gets a difference of the order of 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTuA3XFx9pA7"
      },
      "source": [
        "## Step 2: Implement causal masking for language modeling.\n",
        "\n",
        "Note that for language modeling, since we are predicting the next token at every step, we cannot attend to future tokens. Therefore, we need to apply a causal mask to the attention scores, which is a lower triangular matrix, i.e., all the values above the diagonal are set to zero. Hence, the model can attend to the current and past tokens only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7rHd6ao9pA7"
      },
      "source": [
        "Useful tips:\n",
        "\n",
        "- Note that the mask is applied to the attention scores **before** the softmax operation.\n",
        "- A 0 raw attention score (pre-softmax) doesn't mean that after applying softmax it will also be 0! Think what score should be used to represent a raw attention score so that when softmaxed it will be 0.\n",
        "- You might find the `masked_fill` method useful. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK_jK6ZD9pA7"
      },
      "source": [
        "**You will complete the following code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "add8173a9470c5d0c95e195cfd7bc3ae",
          "grade": false,
          "grade_id": "cell-58647279b9ee6bba",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ta2e3pXm9pA7"
      },
      "outputs": [],
      "source": [
        "def make_causal_mask(n_tok: int):\n",
        "    \"\"\"\n",
        "    Create a mask matrix that masks future context for the attention.\n",
        "    Inputs:\n",
        "    - n_tok: int, the number of tokens in the sequence.\n",
        "    :return: A mask matrix which is a tensor of shape (n_tok, n_tok)\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "def apply_causal_mask(mask, A):\n",
        "\n",
        "    \"\"\"\n",
        "    Apply mask to attention.\n",
        "    Inputs:\n",
        "    - mask: torch.Tensor, shape (n_tok, n_tok) containing the causal mask.\n",
        "    - A: torch.Tensor, shape (B, n_tok, n_tok) or (B, n_heads, n_tok, n_tok) containing the attention scores, where B is the batch size, n_heads is the number of attention heads, n_tok is the number of tokens.\n",
        "    :return: A masked attention matrix.\n",
        "    \"\"\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d51ZqaHS9pA8"
      },
      "source": [
        "## Test 2: Adding Causal Masks\n",
        "\n",
        "Implement lines denoted by `Step 2` in `self-attention()`, and run the following test code to verify your implementation produces close enough results to the PyTorch implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H9OVRg19pA8"
      },
      "outputs": [],
      "source": [
        "out_A = self_attention(part1_query, part1_key, part1_value, n_heads=1, causal=True)\n",
        "out_B = MHA_wrapper(part1_query, part1_key, part1_value, n_heads=1, causal=True)\n",
        "assert out_A.shape == out_B.shape == part1_query.shape\n",
        "\n",
        "print(\"max diff:\", (out_A - out_B).abs().max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCEvl7P79pA8"
      },
      "source": [
        "The max difference should be very small, our reference implementation gets a difference of the order of 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I44BefWM9pA8"
      },
      "source": [
        "## Step 3: Implement multi-head attention.\n",
        "\n",
        "Recall from the lecture that multi-head attention is implemented by running self-attention independently on each head and then concatenating the results. Specifically, we use the following steps:\n",
        "\n",
        "1. Split the queries, keys, and values into multiple heads.\n",
        "$$ Q = [Q_1, Q_2, \\ldots, Q_{n_{\\text{heads}}}], K = [K_1, K_2, \\ldots, K_{n_{\\text{heads}}}], V = [V_1, V_2, \\ldots, V_{n_{\\text{heads}}}] $$\n",
        "where $n_{\\text{heads}}$ is the number of heads. This is done by simply splitting the embedding dimension into $n_{\\text{heads}}$ chunks.\n",
        "\n",
        "2. Run self-attention independently on each head.\n",
        "$$ O_i = \\text{self\\_attention}(Q_i, K_i, V_i, n_{\\text{heads}}=n_{\\text{heads}}, causal=causal) $$\n",
        "for $i=1, 2, \\ldots, n_{\\text{heads}}$.\n",
        "Note that you don't actually need to call the self-attention function separately for each head. If your implementation is correct, you should be able to perform a single call to self_attention with the split Q, K, and V and get the same result.\n",
        "\n",
        "3. Concatenate the results of the heads.\n",
        "$$ O = \\text{concat}(O_1, O_2, \\ldots, O_{n_{\\text{heads}}}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m5sN5vr9pA8"
      },
      "source": [
        "Useful tips:\n",
        "- You might find the `.view` method useful for splitting and merging heads. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) for more details.\n",
        "- You might find the `.contiguous()` method useful for merging heads. Check the [documentation](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YXZTb4u9pA8"
      },
      "source": [
        "**You will complete the following code blocks denoted by `TODO:`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4dc9002e93f029e1a5ee8fcbd94953fe",
          "grade": false,
          "grade_id": "cell-6f76a75118763d8f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kihml1g09pA8"
      },
      "outputs": [],
      "source": [
        "def split_heads_qkv(Q, K, V, n_heads: int):\n",
        "    \"\"\"\n",
        "    Provided as a utility -- you can choose to not use it if you'd like.\n",
        "    \"\"\"\n",
        "    return (split_heads(Q, n_heads), split_heads(K, n_heads), split_heads(V, n_heads))\n",
        "\n",
        "\n",
        "def split_heads(x, n_heads: int):\n",
        "    \"\"\"\n",
        "    Splitting x across multiple heads.\n",
        "    Inputs:\n",
        "    - x: torch.Tensor, shape (B, n_tok, n_embd) can be the queries, or keys, or values, where B is the batch size, n_tok is the number of tokens, and n_embd is the embedding dimension.\n",
        "    - n_heads: int, the number of heads.\n",
        "    :return: A split x, shape (B,n_heads, n_tok, n_embd // n_heads)\n",
        "    \"\"\"\n",
        "    B, n_tok, n_embd = x.size()\n",
        "    assert n_embd % n_heads == 0, \"d must be divisible by number of heads\"\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def merge_heads(y):\n",
        "    \"\"\"\n",
        "    Reversing splitting action of y.\n",
        "    Inputs:\n",
        "    - y: torch.Tensor, shape (B, n_heads, n_tok, n_embd // n_heads)\n",
        "    :return: A merged y, shape (B, n_tok, n_embd)\n",
        "    \"\"\"\n",
        "    B, nh, n_tok, nc = y.size()\n",
        "    # TODO:\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhRE0WjG9pA8"
      },
      "source": [
        "## Test 3: Adding Multi-Head Attention\n",
        "\n",
        "Implement lines denoted by `Step 3` in `self-attention()`, and run the following test code to verify your implementation produces close enough results to the PyTorch implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1seSHtK9pA8"
      },
      "outputs": [],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "\n",
        "out_A = self_attention(part1_query, part1_key, part1_value, n_heads=3, causal=True)\n",
        "out_B = MHA_wrapper(part1_query, part1_key, part1_value, n_heads=3, causal=True)\n",
        "assert out_A.shape == out_B.shape == part1_query.shape\n",
        "\n",
        "print(\"max diff:\", (out_A - out_B).abs().max().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFsf0v_G9pA8"
      },
      "source": [
        "The max difference should be very small, our reference implementation gets a difference of the order of 1e-7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9JybA7k9pA8"
      },
      "source": [
        "## 1.2: Experiment with Your Transformer\n",
        "\n",
        "In this part, you will train a transformer-based language model. We will provide you the starter code for training the model and evaluating perplexity on a dataset. You will then explore changes to the network architecture and see how they affect the training and performance of the language model.\n",
        "\n",
        "**Note: We will NOT grade this part of the assignment.** This is an open-ended exercise. The following codes provide you with a basis to experiment with some perspectives of your attention block, and you will summarize your explorations in the write-up. We will only grade the write-up for part 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBUGaaoj9pA8"
      },
      "source": [
        "## Preparation: Modifications to Your Attention Implementation for section 2.2\n",
        "\n",
        "Below is space for you to work on a *modified* version of your attention implementation above for your experimentation in section 2.2. **Ensure your submitted code does not break the tests in section 2.1**; the easiest way to do this is to *copy* your implementation above (and probably condense it) and modify it below. We provide you pointers on what explorations you can do in the handout.\n",
        "\n",
        "You can also put any code anywhere in section 2.2; putting that code here is just a suggestion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "addddf000a8e7380e6b5ddcaf39dbe4f",
          "grade": false,
          "grade_id": "cell-2834ef85f422e30d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5OZchpO39pA8"
      },
      "outputs": [],
      "source": [
        "# TODO: Have your modified attention implementation here.\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError()\n",
        "\n",
        "\n",
        "def split_heads_qkv(Q, K, V, n_heads: int):\n",
        "    \"\"\"\n",
        "    Provided as a utility -- you can choose to not use it if you'd like.\n",
        "    \"\"\"\n",
        "    return (split_heads(Q, n_heads), split_heads(K, n_heads), split_heads(V, n_heads))\n",
        "\n",
        "\n",
        "def split_heads(x, n_heads: int):\n",
        "    \"\"\"\n",
        "    Splitting x across multiple heads.\n",
        "    Inputs:\n",
        "    - x: torch.Tensor, shape (B, n_tok, n_embd) can be the queries, or keys, or values, where B is the batch size, n_tok is the number of tokens, and n_embd is the embedding dimension.\n",
        "    - n_heads: int, the number of heads.\n",
        "    :return: A split x, shape (B,n_heads, n_tok, n_embd // n_heads)\n",
        "    \"\"\"\n",
        "    B, n_tok, n_embd = x.size()\n",
        "    assert n_embd % n_heads == 0, \"d must be divisible by number of heads\"\n",
        "    # TODO:\n",
        "    x = x.view(B, n_tok, n_heads, n_embd // n_heads)\n",
        "    x = x.transpose(1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def merge_heads(y):\n",
        "    \"\"\"\n",
        "    Reversing splitting action of y.\n",
        "    Inputs:\n",
        "    - y: torch.Tensor, shape (B, n_heads, n_tok, n_embd // n_heads)\n",
        "    :return: A merged y, shape (B, n_tok, n_embd)\n",
        "    \"\"\"\n",
        "    B, nh, n_tok, nc = y.size()\n",
        "    # TODO:\n",
        "    y = y.transpose(1, 2)\n",
        "    y = y.contiguous().view(B, n_tok, nh * nc)\n",
        "    return y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae5G6CCQ9pA8"
      },
      "source": [
        "## Guideline for Section 1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv8MZjXC9pA8"
      },
      "source": [
        "What follows is the **starter code** for section 1.2. It includes the following:\n",
        "- Download the ngram data from Project 1\n",
        "- Download the fork of minGPT by Yegor, modified to receive an implementation of attention from an external source (i.e., this notebook)\n",
        "- Simple implementation of tokenization which is very similar to what you did for Project 1. Differences from Project 1:\n",
        "    - include `<START>` in the vocab\n",
        "    - truncate to a fixed maximum sequence length of 100 tokens\n",
        "    - pad with `<PAD>` to the max length\n",
        "- `<PAD>` is new -- the loss is set to ignore anything with this token, such that the model doesn't get optimized for learning how to pad, and instead gets trained for the actual text.\n",
        "- Model initialization is set up, but has not been tuned. Feel free to modify anything about it.\n",
        "- Simple trainer code to loop over the data and optimize on the model. We included a nice progress bar for you to watch while waiting.\n",
        "    - On the free-tier GPU in Colab, our provided starter code trains for an epoch (less than 2000 steps) within a few minutes. On CPU, it takes over an hour for even the smallest configuration.\n",
        "- Demonstration/explanation of loss calculation.\n",
        "- Per-document perplexity calculation like in Project 1, though it's a bit different:\n",
        "    - Due to the max training length, the learned positional embeddings won't extrapolate past 100 tokens. So, we actually only test on a *truncated* version of the data\n",
        "    - Most documents are less than 100 tokens and end up with a long chain of `<PAD>`s at the end. We don't want to include that in our loss/perplexity, so we show how to omit it from the calculation.\n",
        "- minGPT defines a very convenient `generate` function. We have a few example prompts you can try with it -- take a look at the kind of text your trained model generates.\n",
        "\n",
        "----\n",
        "\n",
        "**Feel free to modify the starter code as much as you want to!** We will focus on your report and not your code for section 2.2, so change anything. In particular, you will want to add more analysis/logging/comparisons for anything that's relevant to your experiment. Of course, that is in addition to the changes on top of part 2.1, as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMM9LS5U9pA8"
      },
      "source": [
        "### Utilities, data, and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rr7S8629pA8"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# We will use the Shakespeare dataset we used in Project 1.\n",
        "mkdir -p data\n",
        "wget https://homes.cs.washington.edu/~kahuja/cse447/project1/data/shakespear_train.txt -O data/shakespear_train.txt\n",
        "wget https://homes.cs.washington.edu/~kahuja/cse447/project1/data/shakespear_dev.txt -O data/shakespear_dev.txt\n",
        "wget https://homes.cs.washington.edu/~kahuja/cse447/project1/data/shakespear_test.txt -O data/shakespear_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEmFn2cz9pA8"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# clone Yegor's fork of minGPT and link to the code\n",
        "[ -d \"mingpt-cse447\" ] || git clone https://gitlab.cs.washington.edu/yegork/mingpt-cse447.git\n",
        "[ -e \"mingpt\" ] || ln -s mingpt-cse447/mingpt mingpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXuOG_zz9pA8"
      },
      "outputs": [],
      "source": [
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMhqMLeO9pBA"
      },
      "source": [
        "### Dataset processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3fYRgt29pBA"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# recall that our data is just a text file of space-separated tokens, with new lines separating documents\n",
        "cat data/shakespear_train.txt | head -n 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuYxR6f49pBA"
      },
      "outputs": [],
      "source": [
        "with open(\"data/shakespear_train.txt\", \"r\") as f: lines_train = f.readlines()\n",
        "with open(\"data/shakespear_dev.txt\", \"r\") as f: lines_dev = f.readlines()\n",
        "with open(\"data/shakespear_test.txt\", \"r\") as f: lines_test = f.readlines()\n",
        "\n",
        "# each element is a list of tokens\n",
        "tokens_train = [line.split() for line in lines_train]\n",
        "\n",
        "print(f\"train docs: {len(tokens_train)}\")\n",
        "print(f\"total train tokens: {sum(len(t) for t in tokens_train)}\")\n",
        "\n",
        "\n",
        "# utility fn to flatten the tokens structure\n",
        "def flat(tokens):\n",
        "    for t in tokens:\n",
        "        yield from t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aprd5ARE9pBA"
      },
      "outputs": [],
      "source": [
        "# get counts of each token sorted by count, descending\n",
        "# also add a few special tokens (with high counts) so they appear first\n",
        "token_counts = Counter(flat(tokens_train))\n",
        "token_counts[\"<START>\"] = 1000004\n",
        "token_counts[\"<STOP>\"] = 1000003\n",
        "token_counts[\"<UNK>\"] = 1000002\n",
        "token_counts[\"<PAD>\"] = 1000001\n",
        "sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"unique_tokens:\", len(token_counts))\n",
        "print(\"unique_tokens, count>=3:\", len([t for t in sorted_tokens if t[1] >= 3]))\n",
        "\n",
        "# make tokenizer for all tokens with count >= 3\n",
        "# note that our tokenizer ends up including START and STOP tokens too\n",
        "tokenizer = {t[0]: i for i, t in enumerate(sorted_tokens) if t[1] >= 3}\n",
        "\n",
        "\n",
        "def pad_to_length(tokens, max_len, tokenizer=tokenizer):\n",
        "    return tokens[:max_len] + [tokenizer[\"<PAD>\"]] * (max_len - len(tokens))\n",
        "\n",
        "\n",
        "def tokenize(sentence, pad_to_len=None, include_stop=True, tokenizer=tokenizer):\n",
        "    words = [tokenizer.get(w, tokenizer[\"<UNK>\"]) for w in sentence.split()]\n",
        "    # add START and STOP tokens\n",
        "    tokens = [tokenizer[\"<START>\"]] + words + ([tokenizer[\"<STOP>\"]] * include_stop)\n",
        "\n",
        "    if pad_to_len is not None:\n",
        "        tokens = pad_to_length(tokens, pad_to_len, tokenizer=tokenizer)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# invert tokenizer for decoding\n",
        "tokenizer_inv = {v: k for k, v in tokenizer.items()}\n",
        "\n",
        "\n",
        "def decode(tokens, tokenizer_inv=tokenizer_inv, end_at_stop=True, omit_pad=True):\n",
        "    tokens = [tokenizer_inv[t] for t in tokens]\n",
        "    if omit_pad:\n",
        "        tokens = [t for t in tokens if t != \"<PAD>\"]\n",
        "    if end_at_stop and \"<STOP>\" in tokens:\n",
        "        tokens = tokens[: tokens.index(\"<STOP>\") + 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "sentence = \"More people have said an Escher sentence than I have .\"\n",
        "tokenized = tokenize(sentence, pad_to_len=25)  # pad to only 25 so it looks nice\n",
        "decoded = decode(tokenized, end_at_stop=False, omit_pad=False)\n",
        "print(f\"{sentence=}\\n{tokenized=}\\n{decoded=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEGbxve39pBA"
      },
      "outputs": [],
      "source": [
        "plt.hist([len(t) for t in tokens_train], bins=50)\n",
        "plt.title(\"sequence lengths in the train dataset by tokens\")\n",
        "plt.ylabel(\"# of sequences\")\n",
        "plt.xlabel(\"sequence length in tokens\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcjew_3l9pBA"
      },
      "outputs": [],
      "source": [
        "# Notice above that the vast majority of sequences have less than 100 tokens.\n",
        "# For performance we will thus truncate to 100 tokens.\n",
        "\n",
        "MAX_LEN = 100\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "data_train = torch.tensor(\n",
        "    [tokenize(t, MAX_LEN) for t in lines_train if len(t) > 0], dtype=torch.long\n",
        ")\n",
        "data_val = torch.tensor(\n",
        "    [tokenize(t, MAX_LEN) for t in lines_dev if len(t) > 0], dtype=torch.long\n",
        ")\n",
        "\n",
        "data_train.shape, data_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vfcs_iqR9pBA"
      },
      "outputs": [],
      "source": [
        "# X is all but last token, Y is all but first token\n",
        "train_dataset = torch.utils.data.TensorDataset(data_train[:, :-1], data_train[:, 1:])\n",
        "val_dataset = torch.utils.data.TensorDataset(data_val[:, :-1], data_val[:, 1:])\n",
        "\n",
        "# example X,Y pair from train dataset -- 2 is <START>, 3 is <STOP>\n",
        "train_dataset[447]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ke5CBL9pBA"
      },
      "source": [
        "### Model and Trainer code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avSapK1s9pBA"
      },
      "outputs": [],
      "source": [
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = None\n",
        "model_config.pad_token = tokenizer[\"<PAD>\"]\n",
        "\n",
        "# This configuration is the very small 'gpt-nano' defined in minGPT. we'd use a\n",
        "# bigger model like 'gpt2' but it would take a very long time to train :(\n",
        "# See minGPT/model.py for configurations of other models\n",
        "model_config.model_type = \"gpt-nano\"\n",
        "# 'gpt-nano' equivalent to:\n",
        "# model_config.n_layer = 3\n",
        "# model_config.n_head = 3\n",
        "# model_config.n_embd = 48\n",
        "\n",
        "model_config.vocab_size = max(tokenizer.values()) + 1\n",
        "# model_config.vocab_size = 50257 # openai's model vocabulary, if using gpt2 BPE\n",
        "\n",
        "# The model's context length\n",
        "# Note that minGPT has learned posemb, so outside the used maxlen wont really work\n",
        "model_config.block_size = 1024\n",
        "\n",
        "# We modified config to accept some functions for attention\n",
        "# Feel free to replace either of these!\n",
        "model_config.attn_init_fn = init_qkv_proj\n",
        "model_config.attn_fn = self_attention\n",
        "\n",
        "# Can use the wrapper around PyTorch's multi-head attention instead, but it's hard to modify for experiments\n",
        "# model_config.attn_fn = MHA_wrapper\n",
        "\n",
        "model = GPT(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktv4xUsm9pBB"
      },
      "outputs": [],
      "source": [
        "train_config = Trainer.get_default_config()\n",
        "train_config.device = DEVICE\n",
        "train_config.num_workers = 2\n",
        "\n",
        "# We didn't tune the hyperparameters at all, feel free to change\n",
        "train_config.learning_rate = 5e-4\n",
        "train_config.batch_size = 32\n",
        "train_config.max_iters = 3 * (\n",
        "    len(train_dataset) // train_config.batch_size\n",
        ")  # train for 3 epochs\n",
        "\n",
        "trainer = Trainer(train_config, model, train_dataset)\n",
        "log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5oibyIu9pBB"
      },
      "outputs": [],
      "source": [
        "model.to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "bar = tqdm(total=train_config.max_iters)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def on_batch_end(trainer):\n",
        "    log.append(trainer.loss.item())\n",
        "    bar.set_postfix(loss=trainer.loss.item())\n",
        "    bar.update()\n",
        "\n",
        "\n",
        "trainer.set_callback(\"on_batch_end\", on_batch_end)\n",
        "trainer.run()\n",
        "bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDrwr8_z9pBB"
      },
      "outputs": [],
      "source": [
        "plt.plot(log)\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK0jFvkR9pBB"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Like in Project 1, we will treat each line in the data file as a separate document. So, compute perplexity for each line separately and then take the average across all lines.\n",
        "\n",
        "However, the way we processed our data for batching makes it slightly more complicated, because the loss is averaged over tokens. While this would normally be simple, recall that we insert `<PAD>` tokens as padding until 100 tokens, and the vast majority of documents are significantly less than 100 tokens. This means that most of our tokens are the long tail of `<PAD>`s.\n",
        "\n",
        "So, we wish to exlude every `<PAD>` from perplexity calculations. Since the loss returned by the model is averaged for all tokens. PyTorch has great utilities for excluding a padding token from calculations, but we will also do this manually as a demonstration.\n",
        "\n",
        "However, this isn't the end of the story. Ideally, we would make `<PAD>` tokens have no effect whatsoever. For this to happen, we should apply a mask out all attention to and from `<PAD>` tokens, but we chose not to include that in your required implementation. The fact that we did not implement this has no measurable impact because our minGPT fork zeros out values for `<PAD>` tokens after attention anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoZDimfH9pBB"
      },
      "outputs": [],
      "source": [
        "sentence = \"Thank you so much Liwei and Taylor for all your help with this !\"\n",
        "\n",
        "tokens = torch.tensor([tokenize(sentence, pad_to_len=MAX_LEN)], dtype=torch.long)\n",
        "X_tokens, y_tokens = tokens[:, :-1], tokens[:, 1:]\n",
        "\n",
        "print(\"notice the long tail of PAD tokens: \", tokens.cpu()[0].tolist())\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, loss = model(X_tokens.to(DEVICE), y_tokens.to(DEVICE))\n",
        "    logits, loss = logits.cpu(), loss.cpu()\n",
        "\n",
        "# Preprocess logits to unpad -- will be (jagged) list of tensors\n",
        "# We impl looping over them\n",
        "# students have to: convert 1 document's raw logits + y_tokens into loss and ppl\n",
        "\n",
        "# There's more ways to get the loss!\n",
        "\n",
        "# We could use F.cross_entropy with the logits -- this is what the model does\n",
        "# F.cross_entropy can take an \"ignore_index\", which makes it ignore our pad token\n",
        "also_loss = F.cross_entropy(\n",
        "    logits.flatten(0, 1), y_tokens.flatten(0, 1), ignore_index=tokenizer[\"<PAD>\"]\n",
        ")\n",
        "\n",
        "# However, we can just do the calculations manually because we enjoy being perplexed\n",
        "\n",
        "# softmax the logits to get probabilities\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "# work with log of the probabilities for numerical stability\n",
        "log_probs = torch.log(probs)\n",
        "\n",
        "# this is weird pytorch screwery to index into last dimension of log_probs with y_tokens\n",
        "# this selects only the log probabilities of the target tokens\n",
        "y_log_probs = torch.gather(log_probs, -1, y_tokens[..., None])[..., 0]\n",
        "\n",
        "# get all the target log probabilities EXCEPT for when that target token is <PAD>\n",
        "not_pad_y_log_probs = y_log_probs[y_tokens != tokenizer[\"<PAD>\"]]\n",
        "\n",
        "# negative average of the log probs of the target tokens is exactly crossentropy loss here!\n",
        "also_loss_again = -not_pad_y_log_probs.mean()\n",
        "\n",
        "print()\n",
        "print(\"reported loss from model:\\t\", loss.item())\n",
        "print(\"manually calculated loss:\\t\", also_loss.item())\n",
        "print(\"manually calculated loss again:\\t\", also_loss_again.item())\n",
        "\n",
        "# we can calculate perplexity using the crossentropy loss\n",
        "perplexity = torch.exp(also_loss)\n",
        "print(\"perplexity:\", perplexity.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l3sYFfL9pBB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We've made a utility function to calculate loss per-document for some data.\n",
        "It accepts a list of strings, tokenizes, evaluates, and returns a list of floats.\n",
        "\"\"\"\n",
        "\n",
        "@torch.no_grad\n",
        "def evaluate_losses(data, model=model, bs=32, progress=True, pad_to_len=MAX_LEN):\n",
        "    it = range(0, len(data), bs)\n",
        "    if progress:\n",
        "        it = tqdm(it)\n",
        "\n",
        "    out = []\n",
        "    for b_start in it:\n",
        "        batch = slice(b_start, b_start + bs)\n",
        "        tokens = torch.tensor(\n",
        "            [tokenize(t, pad_to_len=pad_to_len) for t in data[batch]], dtype=torch.long\n",
        "        ).to(DEVICE)\n",
        "        X_tokens, y_tokens = tokens[:, :-1].contiguous(), tokens[:, 1:].contiguous()\n",
        "\n",
        "        model.eval()\n",
        "        logits, _ = model(X_tokens)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        y_log_probs = torch.gather(log_probs, 2, y_tokens[..., None])[..., 0]\n",
        "\n",
        "        for i in range(y_tokens.shape[0]):\n",
        "            not_pad = y_tokens[i] != tokenizer[\"<PAD>\"]\n",
        "            loss = -y_log_probs[i, not_pad].mean()\n",
        "            out.append(loss.item())\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-AmxtR29pBB"
      },
      "outputs": [],
      "source": [
        "# calculate loss and perplexity for a single sentence\n",
        "is_this_loss = evaluate_losses(\n",
        "    [\n",
        "        \"RICHARD II : I will not be afraid of death . I will not be afraid of death .\",\n",
        "    ],\n",
        "    progress=False,\n",
        ")[0]\n",
        "print(\"loss:\", is_this_loss)\n",
        "print(\"perplexity:\", np.exp(is_this_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccsppVr9pBB"
      },
      "outputs": [],
      "source": [
        "train_losses = evaluate_losses(lines_train)\n",
        "print(\"train perplexity:\", np.mean(np.exp(train_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olWXn_tJ9pBB"
      },
      "outputs": [],
      "source": [
        "dev_losses = evaluate_losses(lines_dev)\n",
        "print(\"dev perplexity:\", np.mean(np.exp(dev_losses)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CluHq9Fs9pBB"
      },
      "source": [
        "With the implementation on attention in Part 1, you should see that dev perplexity is close to the best perplexities that we achieved using N-gram models and interpolation in Project 1. The reason we don't do much better is that we are using a very small dataset and not doing much tuning. Transformer LMs become much more powerful when we have a whole lot of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jz9Wjwy9pBB"
      },
      "source": [
        "Your task now is to make modifications to the attention mechanisms based on the guidelines in the handout and explore the effects of these changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAsE8eKC9pBB"
      },
      "outputs": [],
      "source": [
        "# Here's an example of generating using the model -- see generate in minGPT's model.py\n",
        "\n",
        "sentence = \"\"  # empty prompt -> sample from model at random\n",
        "# sentence = 'unfortunately ,'          # can sample more negative stuff\n",
        "# sentence = 'fun fact : did you know'  # AI-generated fun facts\n",
        "\n",
        "tokens = torch.tensor([tokenize(sentence, include_stop=False)], dtype=torch.long).to(\n",
        "    DEVICE\n",
        ")\n",
        "\n",
        "for _ in range(10):\n",
        "    pred = model.generate(\n",
        "        tokens, MAX_LEN - tokens.shape[-1], temperature=1.0, do_sample=True, top_k=None\n",
        "    )\n",
        "\n",
        "    print(decode(pred[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_qkv_proj(n_embd: int):\n",
        "    \"\"\"\n",
        "    Modified initialization to work with minGPT's expected interface\n",
        "    \"\"\"\n",
        "    # Basic projections for K and V\n",
        "    wk = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    wv = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "    # For Q, we create both the projection and gating network\n",
        "    wq = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    wg = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd // 2),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(n_embd // 2, n_embd),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        nn.ModuleList([wq, wg]),  # Q projection and gate\n",
        "        wk,  # K projection\n",
        "        wv   # V projection\n",
        "    )\n",
        "\n",
        "def self_attention(Q, K, V, n_heads=1, causal=True):\n",
        "    \"\"\"\n",
        "    Self-attention implementation compatible with minGPT\n",
        "    \"\"\"\n",
        "    B, n_tok, n_embd = Q.shape\n",
        "    assert Q.shape == K.shape == V.shape\n",
        "\n",
        "    # Handle projection and gating for Q\n",
        "    if isinstance(Q, tuple):\n",
        "        # First time through - do projections\n",
        "        wq, wg = Q[0]  # Unpack ModuleList\n",
        "        wk = K\n",
        "        wv = V\n",
        "\n",
        "        # Project Q and apply gating\n",
        "        Q = wq(Q[1]) * wg(Q[1])  # Q[1] is the input tensor\n",
        "        K = wk(K)\n",
        "        V = wv(V)\n",
        "\n",
        "    # Split heads if using multi-head attention\n",
        "    if n_heads > 1:\n",
        "        Q = split_heads(Q, n_heads)\n",
        "        K = split_heads(K, n_heads)\n",
        "        V = split_heads(V, n_heads)\n",
        "\n",
        "    # Calculate attention scores\n",
        "    scale = 1.0 / math.sqrt(Q.size(-1))\n",
        "    scores = Q @ K.transpose(-2, -1) * scale\n",
        "\n",
        "    # Apply causal mask if needed\n",
        "    if causal:\n",
        "        mask = torch.triu(torch.ones(n_tok, n_tok, device=Q.device), diagonal=1).bool()\n",
        "        scores.masked_fill_(mask, float('-inf'))\n",
        "\n",
        "    # Apply attention\n",
        "    att = F.softmax(scores, dim=-1)\n",
        "    y = att @ V\n",
        "\n",
        "    # Merge heads if necessary\n",
        "    if n_heads > 1:\n",
        "        y = merge_heads(y)\n",
        "\n",
        "    assert y.shape == (B, n_tok, n_embd)\n",
        "    return y"
      ],
      "metadata": {
        "id": "2X8Aho0ZRWdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionAnalyzer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "    def get_gate_activations(self, text):\n",
        "        \"\"\"\n",
        "        Get gate activation values for a given input text\n",
        "        \"\"\"\n",
        "        tokens = torch.tensor([tokenize(text, pad_to_len=MAX_LEN)], dtype=torch.long).to(self.device)\n",
        "        embeddings = self.model.transformer.wte(tokens)\n",
        "\n",
        "        # Get first attention layer\n",
        "        attn_block = self.model.transformer.h[0].attn\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get gate values\n",
        "            gate_values = attn_block.attn_fn.Q[0][1](embeddings)  # [1] indexes the gate network\n",
        "\n",
        "        return {\n",
        "            'tokens': tokens[0].cpu().numpy(),\n",
        "            'gate_values': gate_values[0].cpu().numpy(),\n",
        "            'mean_gate': gate_values[0].mean(dim=-1).cpu().numpy()\n",
        "        }\n",
        "\n",
        "    def visualize_gates(self, text):\n",
        "        \"\"\"\n",
        "        Visualize gate activations for input text\n",
        "        \"\"\"\n",
        "        results = self.get_gate_activations(text)\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.plot(results['mean_gate'], marker='o')\n",
        "\n",
        "        # Add token labels\n",
        "        tokens = [tokenizer_inv[t] for t in results['tokens']]\n",
        "        plt.xticks(range(len(tokens)), tokens, rotation=45, ha='right')\n",
        "\n",
        "        plt.title('Gate Activation Values Across Sequence')\n",
        "        plt.xlabel('Token Position')\n",
        "        plt.ylabel('Average Gate Activation')\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        return plt\n",
        "\n",
        "    def compare_models(self, baseline_model, test_text, n_samples=5):\n",
        "        \"\"\"\n",
        "        Compare perplexity between baseline and gated model\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            'baseline': [],\n",
        "            'gated': []\n",
        "        }\n",
        "\n",
        "        for _ in range(n_samples):\n",
        "            # Get baseline perplexity\n",
        "            baseline_loss = evaluate_losses([test_text], baseline_model, progress=False)[0]\n",
        "            results['baseline'].append(np.exp(baseline_loss))\n",
        "\n",
        "            # Get gated model perplexity\n",
        "            gated_loss = evaluate_losses([test_text], self.model, progress=False)[0]\n",
        "            results['gated'].append(np.exp(gated_loss))\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "P0uEepr0Rh-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update model configuration\n",
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = 'gpt-nano'\n",
        "model_config.vocab_size = max(tokenizer.values()) + 1\n",
        "model_config.block_size = 1024\n",
        "model_config.attn_init_fn = init_qkv_proj\n",
        "model_config.attn_fn = self_attention\n",
        "\n",
        "# Create model and analyzer\n",
        "model = GPT(model_config)\n",
        "analyzer = AttentionAnalyzer(model)\n",
        "\n",
        "# Train as normal\n",
        "trainer = Trainer(train_config, model, train_dataset)\n",
        "trainer.run()\n",
        "\n",
        "# Analyze results\n",
        "analyzer.visualize_gates(\"To be or not to be\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ROnYWl21RnN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}