{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzIDhnUWNX98"
      },
      "source": [
        "# Project 3b: Natural Language Generation Generation\n",
        "\n",
        "In this part of the homework, you will implement decoding algorithms covered in class -- greedy decoding, random sampling, temperature sampling, top-k sampling, and top-p (nucleus) sampling. You will also learn how to use knowledge distillation to use strong teacher models to improve performance of weaker student models. The knowledge distillation exercise is only mandatory for CSE 547M students\n",
        "\n",
        "*The section on Decoding algorithms is adapted from the code by Jiacheng Liu.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYii6yUvQSvp"
      },
      "source": [
        "## Updates:\n",
        "\n",
        "### 12/02/2024:\n",
        "*   In section 1.5 \"Evaluate\", we have asked you to report the results in the write-up. You do not need to do this and only need to answer the write-up questions.\n",
        "*   For 1.5, some variance is expected in values, specially for the perplexity values. +/- 0.2 should be fine for the perplexity values. Also, we had a typo for the perplexity for topk sampling and it should be roughly around 12. We have updated the notebook.\n",
        "\n",
        "### 12/03/2024\n",
        "\n",
        "* For 1.5, we are noticing a lot of variance in the perplexity values across runs and you might see values which are well over the +/- 0.2 bound from the expected value. It is fine if your values do not lie within this interval. As long as your fluency and diversity values are within the bounds, you should be fine!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM-517mmNX99"
      },
      "source": [
        "## Section 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGHdl9b5NX99",
        "outputId": "c8ef96c7-c82e-4dab-cb31-0a409f708a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./env/lib/python3.12/site-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./env/lib/python3.12/site-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.12/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./env/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./env/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in ./env/lib/python3.12/site-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./env/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./env/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in ./env/lib/python3.12/site-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in ./env/lib/python3.12/site-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in ./env/lib/python3.12/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in ./env/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in ./env/lib/python3.12/site-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.12/site-packages (from evaluate) (2.1.3)\n",
            "Requirement already satisfied: dill in ./env/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./env/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in ./env/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./env/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in ./env/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in ./env/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in ./env/lib/python3.12/site-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in ./env/lib/python3.12/site-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install transformers\n",
        "pip install datasets\n",
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IdXWTdjNX99",
        "outputId": "369dcc63-b337-448d-d4fa-cc1390a5ddb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "\"\"\"set device and random seeds\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper functions are given to you.\n",
        "######################################################\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = device\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "\n",
        "def set_seed(seed=19260817):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQBKM6a9NX99"
      },
      "source": [
        "### 0.1 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3nf-2f3NX9-",
        "outputId": "dcae7c65-1396-4d3e-f192-4f2e20ffc490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'story_id': '080198fc-d0e7-42b3-8e63-b2144e59d816', 'prompt': 'On my way to work I stopped to get some coffee.', 'continuation': 'I went through the drive through and placed my order. I paid the cashier and patiently waited for my drink. When she handed me the drink, the lid came off and spilled on me. The coffee hurt and I had to go home and change clothes.', 'constraint_words': ['drive', 'order', 'drink', 'lid', 'coffee', 'hurt', 'home', 'change', 'clothes']}\n"
          ]
        }
      ],
      "source": [
        "\"\"\"load datasets\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Ximing/ROCStories\")\n",
        "train_data, dev_data, test_data = (\n",
        "    dataset[\"train\"],\n",
        "    dataset[\"validation\"],\n",
        "    dataset[\"test\"],\n",
        ")\n",
        "\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TIWiYBZNX9-"
      },
      "source": [
        "### 0.2 Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EG-bgQPNX9-",
        "outputId": "6ca6d6f5-a145-4342-ac75-0ab2c5eedba4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-CoLA were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "\"\"\"prepare evaluation\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from evaluate import load\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
        "\n",
        "perplexity_scorer = load(\"perplexity\", module_type=\"metric\")\n",
        "cola_model_name = \"textattack/roberta-base-CoLA\"\n",
        "cola_tokenizer = RobertaTokenizer.from_pretrained(cola_model_name)\n",
        "cola_model = RobertaForSequenceClassification.from_pretrained(cola_model_name).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    assert batch_size > 0\n",
        "\n",
        "    batch = []\n",
        "    for item in data:\n",
        "        # Yield next batch\n",
        "        if len(batch) == batch_size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "\n",
        "        batch.append(item)\n",
        "\n",
        "    # Yield last un-filled batch\n",
        "    if len(batch) != 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "12c823f863c544d5b24dfa2674486ee0",
            "f628e1abbfdc4aef959c080a53d9f2db",
            "ad84bbb2a4134a999a650e2063289728",
            "ddbad6291eb640beb3efd2f84670b7c2",
            "6fa4b9d5b1794b3b93b4d144a7ad5267",
            "4f4623b070f54d1892f36f4dbdfd6aea",
            "c68bf9a905ab48b3a039068f46ed7d66",
            "ec7118a5b75b4f3eb9e33ef4da7804b0",
            "07ad5fe1ee4e4005a016268689c23f21",
            "0d222aa7cc7042879d009ac7bc99b145",
            "bf34b64526e148a4803908ac09e616b8"
          ]
        },
        "id": "xgVMcubTNX9-",
        "outputId": "6b53064b-5386-407a-f9df-4d015ed175f9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31e51a0ea2634fca9e0d4660918ae7a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "debugging run\n",
            "perplexity = 178.64\n",
            "fluency = 0.98\n",
            "diversity = 0.87, 1.00, 1.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"set up evaluation metric\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "\n",
        "def compute_perplexity(texts, model=\"gpt2\", batch_size=8):\n",
        "    score = perplexity_scorer.compute(\n",
        "        predictions=texts, add_start_token=True, batch_size=batch_size, model_id=model\n",
        "    )\n",
        "    return score[\"mean_perplexity\"]\n",
        "\n",
        "\n",
        "def compute_fluency(texts, batch_size=8):\n",
        "    scores = []\n",
        "    for b_texts in batchify(texts, batch_size):\n",
        "        inputs = cola_tokenizer(\n",
        "            texts, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = cola_model(**inputs).logits\n",
        "            probs = logits.softmax(dim=-1)\n",
        "            scores.extend(probs[:, 1].tolist())\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "\n",
        "def compute_diversity(texts):\n",
        "    unigrams, bigrams, trigrams = [], [], []\n",
        "    total_words = 0\n",
        "    for gen in texts:\n",
        "        o = gen.split(\" \")\n",
        "        total_words += len(o)\n",
        "        for i in range(len(o)):\n",
        "            unigrams.append(o[i])\n",
        "        for i in range(len(o) - 1):\n",
        "            bigrams.append(o[i] + \"_\" + o[i + 1])\n",
        "        for i in range(len(o) - 2):\n",
        "            trigrams.append(o[i] + \"_\" + o[i + 1] + \"_\" + o[i + 2])\n",
        "    return (\n",
        "        len(set(unigrams)) / len(unigrams),\n",
        "        len(set(bigrams)) / len(bigrams),\n",
        "        len(set(trigrams)) / len(trigrams),\n",
        "    )\n",
        "\n",
        "\n",
        "def evaluate(generations, experiment):\n",
        "    generations = [_ for _ in generations if _ != \"\"]\n",
        "    perplexity = compute_perplexity(generations)\n",
        "    fluency = compute_fluency(generations)\n",
        "    diversity = compute_diversity(generations)\n",
        "    print(experiment)\n",
        "    print(f\"perplexity = {perplexity:.2f}\")\n",
        "    print(f\"fluency = {fluency:.2f}\")\n",
        "    print(f\"diversity = {diversity[0]:.2f}, {diversity[1]:.2f}, {diversity[2]:.2f}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "debug_sents = [\n",
        "    \"This restaurant is awesome\",\n",
        "    \"My dog is cute and I love it.\",\n",
        "    \"Today is sunny.\",\n",
        "]\n",
        "evaluate(debug_sents, \"debugging run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Srz0ZG6NX9-"
      },
      "source": [
        "### 0.3: Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVwg4AX4NX9_",
        "outputId": "e2729f77-0e68-41c9-8b4f-ce0fb6d21459"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"load model and tokenizer\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, pad_token=\"<|endoftext|>\")\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj0TVZyRNX9_"
      },
      "source": [
        "## **Section 1: Decoding Algorithms**\n",
        "\n",
        "In this section, you will implement a few basic decoding algorithms:\n",
        "1. Greedy decoding\n",
        "2. Vanilla sampling\n",
        "3. Temperature sampling\n",
        "4. Top-k sampling\n",
        "5. Top-p sampling\n",
        "\n",
        "We have provided a wrapper function `decode()` that takes care of batching, controlling max length, and handling the EOS token.\n",
        "You will be asked to implement the core function of each method: *given the pre-softmax logits of the next token, decide what the next token is.*\n",
        "\n",
        "**The wrapper calls the core function of each decoding algorithm, which you will implement in the subsections below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8GofKB8VNX9_"
      },
      "outputs": [],
      "source": [
        "\"\"\"decode main wrapper function\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "def _update_model_kwargs_for_generation(\n",
        "    outputs, model_kwargs, is_encoder_decoder: bool = False\n",
        "):\n",
        "    # update past\n",
        "    if \"past_key_values\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.past_key_values\n",
        "    elif \"mems\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.mems\n",
        "    elif \"past_buckets_states\" in outputs:\n",
        "        model_kwargs[\"past_key_values\"] = outputs.past_buckets_states\n",
        "    else:\n",
        "        model_kwargs[\"past_key_values\"] = None\n",
        "\n",
        "    # update token_type_ids with last value\n",
        "    if \"token_type_ids\" in model_kwargs:\n",
        "        token_type_ids = model_kwargs[\"token_type_ids\"]\n",
        "        model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n",
        "\n",
        "    # update attention mask\n",
        "    if not is_encoder_decoder:\n",
        "        if \"attention_mask\" in model_kwargs:\n",
        "            attention_mask = model_kwargs[\"attention_mask\"]\n",
        "            model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n",
        "            )\n",
        "\n",
        "    return model_kwargs\n",
        "\n",
        "def decode(prompts, max_len, method, **kwargs):\n",
        "    encodings_dict = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "    input_ids = encodings_dict[\"input_ids\"].to(device)\n",
        "    attention_mask = encodings_dict[\"attention_mask\"].to(device)\n",
        "\n",
        "    model_kwargs = {\"attention_mask\": attention_mask}\n",
        "    batch_size, input_seq_len = input_ids.shape\n",
        "\n",
        "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=device)\n",
        "\n",
        "    for step in range(max_len):\n",
        "        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                **model_inputs,\n",
        "                return_dict=True,\n",
        "                output_attentions=False,\n",
        "                output_hidden_states=False,\n",
        "            )\n",
        "\n",
        "        if step == 0:\n",
        "            last_non_masked_idx = torch.sum(attention_mask, dim=1) - 1\n",
        "            next_token_logits = outputs.logits[\n",
        "                range(batch_size), last_non_masked_idx, :\n",
        "            ]\n",
        "        else:\n",
        "            next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        log_prob = F.log_softmax(next_token_logits, dim=-1)\n",
        "\n",
        "        if method == \"greedy\":\n",
        "            next_tokens = greedy(next_token_logits)\n",
        "        elif method == \"sample\":\n",
        "            next_tokens = sample(next_token_logits)\n",
        "        elif method == \"temperature\":\n",
        "            next_tokens = temperature(next_token_logits, temperature=kwargs.get(\"temperature\", 0.8))\n",
        "        elif method == \"topk\":\n",
        "            next_tokens = topk(\n",
        "                next_token_logits,\n",
        "                k=kwargs.get(\"k\", 20),\n",
        "                temperature=kwargs.get(\"temperature\", 1.0),\n",
        "            )\n",
        "        elif method == \"topp\":\n",
        "            next_tokens = topp(\n",
        "                next_token_logits,\n",
        "                p=kwargs.get(\"p\", 0.7),\n",
        "                temperature=kwargs.get(\"temperature\", 1.0),\n",
        "            )\n",
        "\n",
        "        # finished sentences should have their next token be a padding token\n",
        "        next_tokens = next_tokens * unfinished_sequences + tokenizer.pad_token_id * (\n",
        "            1 - unfinished_sequences\n",
        "        )\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
        "        # model_kwargs[\"attention_mask\"] = torch.cat(\n",
        "        #     [attention_mask, torch.ones_like(next_tokens[:, None])], dim=-1\n",
        "        # )\n",
        "        model_kwargs = _update_model_kwargs_for_generation(\n",
        "            outputs, model_kwargs, is_encoder_decoder=model.config.is_encoder_decoder\n",
        "        )\n",
        "\n",
        "        # model_kwargs = model._update_model_kwargs_for_generation(\n",
        "        #     outputs,\n",
        "        #     model_kwargs,\n",
        "        #     is_encoder_decoder=model.config.is_encoder_decoder,\n",
        "        # )\n",
        "\n",
        "        # if eos_token was found in one sentence, set sentence to finished\n",
        "        unfinished_sequences = unfinished_sequences.mul(\n",
        "            (next_tokens != tokenizer.eos_token_id).long()\n",
        "        )\n",
        "\n",
        "        if unfinished_sequences.max() == 0:\n",
        "            break\n",
        "\n",
        "    response_ids = input_ids[:, input_seq_len:]\n",
        "    response_text = [\n",
        "        tokenizer.decode(\n",
        "            output, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "        for output in response_ids\n",
        "    ]\n",
        "\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qm-AmDIQNX9_"
      },
      "outputs": [],
      "source": [
        "\"\"\"debug helper code\"\"\"\n",
        "\n",
        "######################################################\n",
        "#  The following helper code is given to you.\n",
        "######################################################\n",
        "\n",
        "# For debugging, we duplicate a single prompt 10 times so that we obtain 10 generations for the same prompt\n",
        "dev_prompts = [dev_data[0][\"prompt\"]] * 10\n",
        "\n",
        "\n",
        "def print_generations(prompts, generations):\n",
        "    for prompt, generation in zip(prompts, generations):\n",
        "        print(f\"{[prompt]} ==> {[generation]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92B0-5qGNX9_"
      },
      "source": [
        "### 1.1: Greedy Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "id": "kVGYgrudNX9_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "67fa5b2e58c40210fe7eb3f97faae38c",
          "grade": false,
          "grade_id": "cell-105debff5fe08d80",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def greedy(next_token_logits):\n",
        "    \"\"\"\n",
        "    Applies greedy decoding to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "\n",
        "    \"\"\"\n",
        "    next_tokens = None\n",
        "    # YOUR CODE HERE\n",
        "    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztI2SoK0NX9_",
        "outputId": "d4c8f363-fc7b-43b9-dc32-77593659e76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I was like, \\'I\\'m going to go to work tomorrow,\\'\" he said.']\n"
          ]
        }
      ],
      "source": [
        "generations = decode(dev_prompts, max_len=20, method=\"greedy\")\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiUh0_jtNX9_"
      },
      "source": [
        "### 1.2: Vanilla Sampling and Temperature Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "deletable": false,
        "id": "AAl5RIKJNX9_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3bf0eb9b1bd75f68f4deb188543ab1f6",
          "grade": false,
          "grade_id": "cell-e7bd215427887e78",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def sample(next_token_logits):\n",
        "\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "\n",
        "    Hint: use torch.multinomial()\n",
        "    \"\"\"\n",
        "\n",
        "    next_tokens = None\n",
        "    # YOUR CODE HERE\n",
        "    next_tokens = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "    next_tokens = next_tokens.squeeze(-1)\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQMlgKdDNX9_",
        "outputId": "3d9ea692-7453-4304-c6df-c5e9e9cdb820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"Let\\'s walk through the window,\" the banner states (it didn\\'t say that at']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Tucked inside a basement window, he found a packed pack of cigarettes on a tin counter in the']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He said he wanted to enroll in school next Thursday.\\n\\nPolice said a woman and her child']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The person had taken him on a hike.\\n\\nWas filming normal? Yep, except that the']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Our co-author is a doctor at the Pepperdine University who is close with many of the']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He'd been bullied for a while now. He couldn't get up from his desk. He suddenly\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But leaving his $100,000 credit card when his plane landed in Miami and flying to Florida was']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Freddie Martin, who works at a non-exchangeable company in Brooklyn, stops by at a']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The local paper has just published a story from Penn State, but, boy! The junior leading six']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Then he made his way to meetings just where Metman was goofing up the rice-pickers']\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method=\"sample\")\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "id": "OGZPWs1tNX9_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "03d44ae38fa4e19ee941b97cff6be030",
          "grade": false,
          "grade_id": "cell-9c65b70fb33ce73e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def temperature(next_token_logits, temperature):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "    next_tokens = None\n",
        "    next_tokens = torch.multinomial(F.softmax(next_token_logits / temperature, dim=-1), num_samples=1)\n",
        "    next_tokens = next_tokens.squeeze(-1)\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kzU7asWNX-A",
        "outputId": "8425bc8d-e9a8-4239-9d91-dde54381a528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I told him if I listened to the job, (it would pay him) $']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The hunger strike was held in a tunnel deep beneath Northeast Cleveland, which was hosting a public hearing.']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He said he wanted to enroll in school.\\n\\n\"I didn\\'t feel safe and didn\\'t']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The person had taken him on a hike.\\n\\nWas filming normal?\\n\\nAnyone who saw']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Our co-author is a doctor at the University of California, Irvine. As a pioneer of child']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He'd been bullied for a while now. He couldn't get up from his desk. He couldn\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But he could have been paid well under $600,000 higher than the average American.\\n\\n']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police have yet to offer an explanation.\\n\\nThe last time police had a photo at a']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The local paper was just doing a story on Penn State, but, boy, does he feel that']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' Then he made his way to the school where the girl was goofing off, and he was told']\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method=\"temperature\", temperature=0.8)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz6KVNhRNX-A"
      },
      "source": [
        "### 1.3: Top-k Sampling\n",
        "\n",
        "Useful tips:\n",
        "- Recall that in Top-k sampling, we only sample from the top-k tokens with the highest probabilities. To ensure that we set the logits other than the top-k to be -inf. You can use `float(\"-inf\")` to represent infinity in python.\n",
        "- You will find `torch.topk()` useful for getting the top-k logits and indices. Check out the [documentation](https://pytorch.org/docs/stable/generated/torch.topk.html) for the function for more details.\n",
        "- Do not forget to divide the logits by the temperature before applying softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "id": "2HEJ7w9fNX-A",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "611732370e1db2672140fc432e6070ef",
          "grade": false,
          "grade_id": "cell-79739f6214d80a0e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def topk(next_token_logits, k, temperature = 1):\n",
        "\n",
        "    \"\"\"\n",
        "    Applies the top-k sampling decoding algorithm to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - k: int, the number of top tokens to consider\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "    next_tokens = None\n",
        "    # YOUR CODE HERE\n",
        "    next_tokens = next_token_logits / temperature\n",
        "    min = torch.topk(next_tokens, k=k, dim=-1).values[:,-1].unsqueeze(-1)\n",
        "    next_tokens[next_tokens < min] = float('-inf')\n",
        "    next_tokens = torch.multinomial(F.softmax(next_tokens, dim=-1), num_samples=1)\n",
        "    next_tokens = next_tokens.squeeze(-1)\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rRtxtoaNX-A",
        "outputId": "618abb2e-bd64-4aae-8659-9ff398994e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I told him if I got to the job, he\\'d tell me to skip lunch']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" The second time, however, he didn't make it because he'd lost interest in his job.\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He said he wanted to come home as soon as possible to have coffee, a meal and to do']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police had taken him on a search warrant that had to be approved as part of the search for']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' They would then drive home after work at the same times the other week. As the friend was talking']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He'd been scheduled to join the team on a vacation to Europe for a week, but that day\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" But he said he went to his friends and told them he wasn't going to work on his schedule\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" The police report indicates that he said he was told he couldn't go out in public with the other\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The next morning, when the train finally stopped, he felt his stomach begin to swell and his chest']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He wasn't home on Tuesday morning when it came time for a walk, but he got home that\"]\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method=\"topk\", k=20)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhbKJIPTNX-A"
      },
      "source": [
        "### 1.4: Top-p Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "id": "R1QudzR0NX-A",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5ab20a9d9cb6e6c50e109fed344c0b97",
          "grade": false,
          "grade_id": "cell-906fc971403533de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def topp(next_token_logits, p, temperature = 1):\n",
        "    \"\"\"\n",
        "    Applies the top-p sampling or nucleus sampling decoding algorithm to get the next token.\n",
        "    inputs:\n",
        "    - next_token_logits: Tensor(size = (B, V), dtype = float)\n",
        "    - p: float, the cutoff probability for the top-p sampling\n",
        "    - temperature: Temperature parameter float\n",
        "    outputs:\n",
        "    - next_tokens: Tensor(size = (B), dtype = long)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Sort the logits in descending order, and compute\n",
        "    # the cumulative probabilities `cum_probs` on the sorted logits\n",
        "    sorted_logits, sorted_indices = None, None\n",
        "    sorted_probs = None\n",
        "    cum_probs = None\n",
        "    # YOUR CODE HERE\n",
        "    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "    sorted_probs = F.softmax(sorted_logits / temperature, dim=-1)\n",
        "    cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "    # Create a mask to zero out all logits not in top-p\n",
        "    sorted_indices_to_remove = cum_probs > p\n",
        "    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "    sorted_indices_to_remove[:, 0] = 0\n",
        "    # Restore mask to original indices\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "\n",
        "    # Mask the logits\n",
        "    next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "    # TODO: Sample from the masked logits\n",
        "    next_tokens = None\n",
        "    # YOUR CODE HERE\n",
        "    next_tokens = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "    next_tokens = next_tokens.squeeze(-1)\n",
        "    return next_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx_60J_mNX-A",
        "outputId": "77da843b-3de4-4fae-8b67-8afb7bea668b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\"I told him I had a game plan for my game day and I told him,']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The owner said, \"This is the guy who shot us, who didn\\'t want to do this']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" He said he wanted to skip school one day. He said he didn't want to waste time,\"]\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The person had taken him to the hospital.\\n\\nZimmerman said the couple had planned to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> ['\\n\\n\\n\"The waiter came in and asked me what I was doing and I told him to come']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' He\\'d been promised to \"go back to the locker room.\"\\n\\n\"I\\'m going to']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' But he could have been paid a day off for his participation in the bombing and the American bomb was']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The police have yet to contact him.\\n\\nWhile there was little other than a photo of a']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [' The local paper has just published a story in which he admits to having some sort of sexual relationship with']\n",
            "['Ryan was called by his friend to skip work one day.'] ==> [\" Then he made his way to Kaitlyn's apartment. He was afraid that Kaitlyn would\"]\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "generations = decode(dev_prompts, max_len=20, method=\"topp\", p=0.7)\n",
        "print_generations(dev_prompts, generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSlonEf1NX-A"
      },
      "source": [
        "### 1.5: Evaluate!\n",
        "\n",
        "Run the following cell to obtain the evaluation results.\n",
        "Also don't forget to answer the questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771,
          "referenced_widgets": [
            "e47bde74cbdf44fb93dc4a3705428827",
            "2559491073d040fbbf4b128d8ffd8ef4",
            "81df800d520d4fbfaaf1453a726bfdd5",
            "6c6cf396ccbc4d2eb01ca72e6be646f9",
            "4a60e5af0cec40148dcd18dd9d71e894",
            "25fa2ac88ccf48ea9b669e4a030a9c7e",
            "1e99d3606d0c4f4eb65f01345e7899f5",
            "a30315c03c7d48679cdf1d702d3a88de",
            "3416784c4c3d4d368f714bcd32876811",
            "4c5787921c8641b48b8c498729cc0cae",
            "bf837e49d2334633b95d976a2e777e97",
            "daab73c3ee7945ec8548217db68fcd50",
            "4302920f04324c02afa69d62acd12fe4",
            "af8ec75ebddf47d5847b6817df04f1e0",
            "0a9d9a96f21a481a8853f9654e9887f4",
            "2f36ee03d67b4051aa5e6c31572a4670",
            "aa6786b6cfaf48f39f5c792296c8785c",
            "d4beb39e76f14b6cab3e92b1e2441bec",
            "87e5272e896448b780baafbcaede6669",
            "8ce95a9e130f41f7a7b4bb01ad34d3fc",
            "5f931d45a0fd4d67ac212fe57ba4c77a",
            "edf78046ee2c4d99917942916de3a3ac",
            "5764c110fb494e548068408c1f6f0a39",
            "1302aae912b64d65b99746da130df0f0",
            "b47455c08af84049bfcee26a9f94902b",
            "9f8ac4d6b28349f4b71e5e0766b461e1",
            "1e3a6e26a9b04e459662d65e3b81dc6a",
            "e20a307307a34310a9c3181a7b37fcd7",
            "5cc9d42e07b442349fed6fca9ae891b6",
            "08b60c893e954991ae0bf64a4fd04e18",
            "52909928eb4e4525b14c6612397253a8",
            "85a0018232f24d07bd9be4093d766fa3",
            "2c4274bebf9d4e88beb202b55ac1e8b4",
            "0a39d44d81ab41ac9104d2a787ad1d10",
            "dea714846de44ffdaceb66bdf6c4cd50",
            "522ff7c612654580a7bbef585f8796dc",
            "cd56ce6f651a42afbf7227d8de08d83d",
            "04f4f85404014b62be66d1426829080e",
            "5f0a736aeb324da3b6d38c762b6817aa",
            "424d79968dfd4870950d8c62f39e99d7",
            "6f9bae012e44466f850584f5702371b5",
            "f73045ef39ba458ba30219a9b2f008a1",
            "9944825679674618aebf69e5300ac922",
            "21a2ee88eeb64dcc8f714d6f82c516da",
            "8b807569247448cc90e815da1e035f47",
            "09cf1022197040ecb24d8b78e541d686",
            "37e2c882574042a7957c16e3d1589a62",
            "3e5531f60bf24b1aa10e554c7b17e3cf",
            "2234678abba549ed86e6183ac111d1cb",
            "b08ed45d57284684add54ee75dcc25d3",
            "8cac2c9087ea4030b950957dcebbb775",
            "4b91911ef5e649489357efbb861bd4d4",
            "bcdf90bc4b0a4918851a9c8508419f9e",
            "41cfab33fbf346c78a2782e3c8d94512",
            "4b032cafbf5940af893300d84d8236d8",
            "e28545e1e09b4a8dbd022495ad9444fa",
            "519c697bba9d41f5ac11e076a5246f22",
            "591a47cc73a3455cb8c2093eaee75285",
            "ffca8a23ba5944679de5a2fd8a6584d1",
            "26f6ed4ff17e4f0aa36fe7f96ae29cc1",
            "cd40d911e2a54ff28e9e728694a17422",
            "b4e51bfc19f54fbcb1b9c08f68637f03",
            "c31fa5e482d84a329009a6a2a6047518",
            "3e6c000848314849a8c69ff3d6348fb1",
            "00cf2f5a574140d4a5111aebba111127",
            "d2c8a6bfa33740c98c03ddc6e535641f",
            "e04af0731a5f43608f2f2e56b0cb8b69",
            "27523999de3249c59cd4df216503f06c",
            "e0d5ffa378d143a7851a8013c083c451",
            "1212dd3253fb49edb5e362f6b797ec29",
            "d924c0e6d25f480ba3e4d53b5bc8f15c",
            "513a8bf5e2d744c49502fa796d2e72b9",
            "9306ba1297684b70962bbd370b4c3e12",
            "8966e34911a148fcb133ef649cf1d694",
            "f735cca63c344f8fb2d2cea307d1b561",
            "d85fe4f1a46b4c329b20d931ade92360",
            "769ed1beb70e4732a16d71d4503336b0",
            "2707158304974681906467abd0474e2d",
            "1ed7772108974312b0f0d65af13d13e1",
            "f23ebc273c574980bcff2e8f514aa472",
            "be642fd8b7774de693a76948984c0e5f",
            "48202f7b8d3d49b9ae2bd3c6e8228ff9",
            "ff2f822cce56465181886e60b28f2e9e",
            "3d127a2d216a457698f54cc894ff929a",
            "8d667eba73924577bbd0e442744460bc",
            "66da42c134fc4153848e5b8e9e577f2c",
            "fc3c0b08d24e4cb0a128852fc655e538",
            "c75deccf62ed42ef9a0596c44ee69222",
            "bf023783acf54eefbcf4215831f03f99",
            "d61761b8ecf44d779a267693ff84b3a1",
            "1c563890b5bb40e888dc2de2baba68fd",
            "c12fca0c23004325b6fa2715d8ffd4bc",
            "41c7b16f4c41425091a3bba8dfd50dc1",
            "d06660b031934cf68845e8103b416e53",
            "4d3d83a0c48d471889a37dd595d907e2",
            "bbbcfffb7f224122a9dbcf2824b227ac",
            "2c64ec5d84874dd2a3dac0810c3c7471",
            "f4b884fecb4e4dca91ea8ea8d94fb6cf",
            "06478dbf7f2f4234a2ccb93ef900e43a",
            "57aef1b4e4074d41a317e3bfe58cda3b",
            "18161ad66ece4e9b9acffdfdadd3ae3e",
            "a9d882393ccf4d79890f943f8cae9049",
            "830b423d68d6428f8b72230e697907ca",
            "d559bf96a475449eb70b1d44a8f042d8",
            "1c9bb27828db438db7026190f14ecdb8",
            "55c59c7003754c788a80a1ab3568c3ea",
            "a1c329e1829b4e2daf0b5a4fb60a7096",
            "74b22ad8d7a44f3fa6e4e0dc0c4e7ea6",
            "ae615258a56d4d299ba4e84139d3c0db",
            "190ff54ebfe346a29b6536d595e4ce04"
          ]
        },
        "id": "EizmFimKNX-A",
        "outputId": "7e58ce71-fed6-433f-cee5-168b6214c575"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3bfe3b7fb26f400a8a766c2f55d1f705",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af86fb74e52b48e3b4eaaba3a6783899",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "greedy\n",
            "perplexity = 2.08\n",
            "fluency = 0.78\n",
            "diversity = 0.01, 0.02, 0.03\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12ad812bf27840a7b32a811d7bda3f21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ce7a263561a419787188771cd7c1feb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample\n",
            "perplexity = 73.70\n",
            "fluency = 0.36\n",
            "diversity = 0.43, 0.90, 0.99\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63b8b5bb8bb042e087db73d4d1c17102",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74d37ff464594bfbbf09deca687d9525",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "temperature\n",
            "perplexity = 15.84\n",
            "fluency = 0.67\n",
            "diversity = 0.31, 0.78, 0.96\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b8ca419ecc245ca81ad8fd2348bdb5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b4c158ce42f4c9da90383d89487e273",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topk\n",
            "perplexity = 12.39\n",
            "fluency = 0.73\n",
            "diversity = 0.26, 0.74, 0.95\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb700d3570524a34b8a9f0fb9c714b47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23db1239c2234414b6ecd1133e1ff82b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "topp\n",
            "perplexity = 12.48\n",
            "fluency = 0.72\n",
            "diversity = 0.28, 0.75, 0.94\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompts = [item[\"prompt\"] for item in test_data][:10]\n",
        "GENERATIONS_PER_PROMPT = 10\n",
        "MAX_LEN = 100\n",
        "\n",
        "for experiment in [\"greedy\", \"sample\", \"temperature\", \"topk\", \"topp\"]:\n",
        "    generations = []\n",
        "    for prompt in tqdm(prompts):\n",
        "        generations += decode(\n",
        "            [prompt] * GENERATIONS_PER_PROMPT, max_len=MAX_LEN, method=experiment\n",
        "        )\n",
        "    evaluate(generations, experiment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWvfTqmLNX-A"
      },
      "source": [
        "You should see the following values:\n",
        "\n",
        "- For greedy:\n",
        "perplexity = 2.08\n",
        "fluency = 0.78\n",
        "diversity = 0.01, 0.02, 0.03\n",
        "\n",
        "- For sample:\n",
        "perplexity = 61.54\n",
        "fluency = 0.37\n",
        "diversity = 0.42, 0.89, 0.99\n",
        "\n",
        "- For temperature:\n",
        "perplexity = 16.15\n",
        "fluency = 0.66\n",
        "diversity = 0.31, 0.77, 0.96\n",
        "\n",
        "- For topk:\n",
        "perplexity = 12.34\n",
        "fluency = 0.70\n",
        "diversity = 0.26, 0.74, 0.96\n",
        "\n",
        "- For topp:\n",
        "perplexity = 12.03\n",
        "fluency = 0.72\n",
        "diversity = 0.29, 0.76, 0.95\n",
        "\n",
        "**Update**: There is a lot of variance expected in the perplexity values here. It is fine if the values you get do not match the expected ones here. As long as your fluency and diversity metrics are within the bounds (+/- 0.2), you should be good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN3QMTWqNX-A"
      },
      "source": [
        "### *Do I always need to use all this code to generate text from a language model?*\n",
        "\n",
        "The exercises above were to help you understand the underlying mechanisms of different decoding methods. In practice, you don't need to implement all these decoding methods from scratch. You can use the `generate()` method in the 🤗 Transformers library to generate text from a language model. Below we provide an example of how to use the `generate()` method to generate text from a language model. Please pay close attention to this especially if you are going to be attempting the optional section (mandatory for masters students) on Knowledge Distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOcXWSceNX-A",
        "outputId": "b26757d4-d8b7-43f4-fa66-c0ee3c0adc65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer output:\n",
            "{'input_ids': tensor([[7454, 2402,  257,  640]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1]], device='cuda:0')}\n",
            "*******************\n",
            "Model Generate output:\n",
            "tensor([[ 7454,  2402,   257,   640,    11,   345,   481,   307,   287,   257,\n",
            "           995,   543,   345,   481,   691,   766,   530,   393,   517,   286,\n",
            "            13,   887,   611,   345,   765,   284,  3802,   340,   757,    11,\n",
            "           345,   481,   691,   307,  1498,   284,   766,  1115,   393,  1440,\n",
            "           287,   262,  2003,   526,   357,    36,   527, 10424,    11,   279,\n",
            "            13,  2681,  2014,   198,   198,  1722,   281,  1981,   356,   836,\n",
            "           470,   761,   284,   766,   284,  3802,   656,   257,   995,   286,\n",
            "           477,  1115,   286,   262,  7683, 16055,    11,   475,   644,   338,\n",
            "           517,  1593,   621,   257,  1598,  2000,   318,   257,   880,    12,\n",
            "         24071,  2858,   287,   543,   356,   460,   651,   284,   760,   290,\n",
            "          1254,   530,  1194,   290]], device='cuda:0')\n",
            "*******************\n",
            "Text output:\n",
            "Once upon a time, you will be in a world which you will only see one or more of. But if you want to enter it again, you will only be able to see three or four in the future.\" (Eberhard, p. 27.)\n",
            "\n",
            "As an individual we don't need to see to enter into a world of all three of the Three Brothers, but what's more important than a clear mind is a well-ordered environment in which we can get to know and feel one another and\n",
            "*******************\n",
            "Text output (ignoring model inputs):\n",
            ", you will be in a world which you will only see one or more of. But if you want to enter it again, you will only be able to see three or four in the future.\" (Eberhard, p. 27.)\n",
            "\n",
            "As an individual we don't need to see to enter into a world of all three of the Three Brothers, but what's more important than a clear mind is a well-ordered environment in which we can get to know and feel one another and\n",
            "*******************\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Step 0: Load a pre-trained language model, and it's the corresponding tokenizer from the Hugging Face model hub\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\") # Setting padding_side=left is important when using these models for open ended text generation\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "example_prompt = \"Once upon a time\"\n",
        "\n",
        "# Step 1: Tokenize the input prompt\n",
        "tokenized_input = tokenizer(example_prompt, return_tensors=\"pt\").to(device)\n",
        "print(\"Tokenizer output:\")\n",
        "print(tokenized_input)\n",
        "print(\"*******************\")\n",
        "\n",
        "# Step 2: Generate text from the model\n",
        "output = model.generate(**tokenized_input, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=1.0,)\n",
        "print(\"Model Generate output:\")\n",
        "print(output)\n",
        "print(\"*******************\")\n",
        "\n",
        "# Step 3: Convert the output ids to text\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Text output:\")\n",
        "print(output_text)\n",
        "print(\"*******************\")\n",
        "# Step 3 (Optional): .generate() returns the model inputs as well. We can ignore that by slicing the output\n",
        "output_text = tokenizer.decode(output[0][len(tokenized_input.input_ids[0]):], skip_special_tokens=True)\n",
        "print(\"Text output (ignoring model inputs):\")\n",
        "print(output_text)\n",
        "print(\"*******************\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42_bb9mfNX-A"
      },
      "source": [
        "Let's go over each of these one by one:\n",
        "\n",
        "- **Step 1:** The `tokenizer()` function takes the input prompt and converts it into a format that the model can understand. This is essentially converting the input prompt into a sequence of tokens. Note that the tokenizer returns a dictionary with the tokenized input ids and attention mask. The `input_ids` are the token ids that the model will use as input. `\"attention_mask`\" is a mask vector that indicates if a particular token corresponds to padding. Padding is extremely important when we are dealing with variable length sequences. Through padding, we can ensure that all the sequences in a batch are of the same size. When feeding sequences with padding to a transformer based model, we need to make sure that the model doesn't attend to the padding tokens. The `attention_mask` is used the tokens that are to be ignored in the attention operation. Note that here since we only used a single input sequence, there was no need of padding and that's why all the attention mask values are 1, i.e. none of the tokens in the sequences should be ignored by attention blocks.\n",
        "\n",
        "- **Step 2:** The `generate()` function takes the tokenized input and generates a sequence of tokens as output. The function takes in a number of arguments, the most important of which are `max_new_tokens` and `do_sample`. The `max_new_tokens` argument specifies the maximum number of new tokens to be generated. The `do_sample` argument is used to turn on sampling, if false, greedy decoding is used. If `do_sample` is set to `True`, then the `top_p` and `temperature` arguments are used to control the sampling process. The `top_p` argument is the value of $p$ parameter for top-p or nucleus sampling and the `temperature` argument is used to control the temperature sampling. Notice that the `generate()` function returns as output a sequence of token ids.\n",
        "\n",
        "- **Step 3:** The `tokenizer.decode` method converts the token ids into text. The `skip_special_tokens=True` argument is used to ignore the special tokens (e.g. start of sequence, end of sequence, padding etc.) that are added by the tokenizer.\n",
        "\n",
        "- **Step 3 (Optional):** The `generate()` function returns the model inputs as well. We can ignore that by slicing the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcH-6MglNX-A"
      },
      "source": [
        "## [Optional for CSE 447]  **Section 2: Knowledge Distillation**\n",
        "\n",
        "In this part of the homework, we will learn how we can use knowledge distillation from a larger teacher model to a smaller student model. Particularly, we will be focusing on the task of text summarization and using the CNN/Daily Mail dataset. We will use Qwen2.5-1.5B-Instruct as our teacher model, which is a 1.5B parameter decoder-only mode pre-trained on 18T tokens of data and then further fine-tuned to follow instructions to perform different tasks (similar to something like ChatGPT). You can read more about Qwen2.5 models [here](https://qwenlm.github.io/blog/qwen2.5/). For the student model, we will be using the default GPT-2 model, which is a 124M parameter model.\n",
        "\n",
        "Since Qwen2.5-1.5B-Instruct is a much bigger model and trained on a lot of data, it is more capable of generating better summaries as compared to the default GPT-2 model which is a smaller model and has seen less data. Knowledge distillation is a technique to transfer the knowledge of a larger teacher model to a smaller student model. In this way, we can leverage the large amount of data and compute resources used to train the teacher model to improve the performance of the student model.\n",
        "\n",
        "This assignment will also make heavy use of the [🤗 Transformers Library](https://huggingface.co/docs/transformers/index). Don't worry if you are not familiar with the library, we will discuss its usage in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyAd5ltjNX-A",
        "outputId": "2f17b438-b84d-454f-88b9-e9d7e14f2c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge-score in ./env/lib/python3.12/site-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in ./env/lib/python3.12/site-packages (from rouge-score) (2.1.0)\n",
            "Requirement already satisfied: nltk in ./env/lib/python3.12/site-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in ./env/lib/python3.12/site-packages (from rouge-score) (2.1.3)\n",
            "Requirement already satisfied: six>=1.14.0 in ./env/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in ./env/lib/python3.12/site-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in ./env/lib/python3.12/site-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in ./env/lib/python3.12/site-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in ./env/lib/python3.12/site-packages (from nltk->rouge-score) (4.67.1)\n",
            "Requirement already satisfied: evaluate in ./env/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in ./env/lib/python3.12/site-packages (from evaluate) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.12/site-packages (from evaluate) (2.1.3)\n",
            "Requirement already satisfied: dill in ./env/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./env/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./env/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in ./env/lib/python3.12/site-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in ./env/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in ./env/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in ./env/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in ./env/lib/python3.12/site-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in ./env/lib/python3.12/site-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in ./env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install rouge-score\n",
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lpi9RQDZNX-A"
      },
      "outputs": [],
      "source": [
        "# Load packages for this section\n",
        "import os\n",
        "from pprint import pprint\n",
        "from datasets import load_from_disk, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7r4SoYCNX-B"
      },
      "source": [
        "As always, we will start by loading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRVo35WQNX-B",
        "outputId": "05155312-2aa9-414c-84fd-82d482eaf8fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2024-12-08 01:49:12--  https://homes.cs.washington.edu/~kahuja/cse447/project3/data/cnn_dm_cse447_dataset.hf.zip\n",
            "Resolving homes.cs.washington.edu (homes.cs.washington.edu)... 128.208.3.226, 2607:4000:200:12::e2\n",
            "Connecting to homes.cs.washington.edu (homes.cs.washington.edu)|128.208.3.226|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12157788 (12M) [application/zip]\n",
            "Saving to: ‘data/cnn_dm_cse447_dataset.hf.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0%  541K 22s\n",
            "    50K .......... .......... .......... .......... ..........  0% 1.05M 16s\n",
            "   100K .......... .......... .......... .......... ..........  1% 10.8M 11s\n",
            "   150K .......... .......... .......... .......... ..........  1%  309M 8s\n",
            "   200K .......... .......... .......... .......... ..........  2% 1.16M 9s\n",
            "   250K .......... .......... .......... .......... ..........  2% 12.0M 7s\n",
            "   300K .......... .......... .......... .......... ..........  2%  106M 6s\n",
            "   350K .......... .......... .......... .......... ..........  3% 99.1M 5s\n",
            "   400K .......... .......... .......... .......... ..........  3% 1.20M 6s\n",
            "   450K .......... .......... .......... .......... ..........  4% 94.5M 5s\n",
            "   500K .......... .......... .......... .......... ..........  4% 12.9M 5s\n",
            "   550K .......... .......... .......... .......... ..........  5%  107M 4s\n",
            "   600K .......... .......... .......... .......... ..........  5%  124M 4s\n",
            "   650K .......... .......... .......... .......... ..........  5%  101M 4s\n",
            "   700K .......... .......... .......... .......... ..........  6% 24.7M 4s\n",
            "   750K .......... .......... .......... .......... ..........  6%  249M 3s\n",
            "   800K .......... .......... .......... .......... ..........  7%  231M 3s\n",
            "   850K .......... .......... .......... .......... ..........  7% 1.27M 3s\n",
            "   900K .......... .......... .......... .......... ..........  8%  107M 3s\n",
            "   950K .......... .......... .......... .......... ..........  8% 85.5M 3s\n",
            "  1000K .......... .......... .......... .......... ..........  8%  107M 3s\n",
            "  1050K .......... .......... .......... .......... ..........  9% 18.8M 3s\n",
            "  1100K .......... .......... .......... .......... ..........  9% 98.4M 3s\n",
            "  1150K .......... .......... .......... .......... .......... 10%  105M 3s\n",
            "  1200K .......... .......... .......... .......... .......... 10%  101M 2s\n",
            "  1250K .......... .......... .......... .......... .......... 10% 84.0M 2s\n",
            "  1300K .......... .......... .......... .......... .......... 11%  177M 2s\n",
            "  1350K .......... .......... .......... .......... .......... 11% 94.0M 2s\n",
            "  1400K .......... .......... .......... .......... .......... 12% 93.6M 2s\n",
            "  1450K .......... .......... .......... .......... .......... 12% 86.8M 2s\n",
            "  1500K .......... .......... .......... .......... .......... 13%  107M 2s\n",
            "  1550K .......... .......... .......... .......... .......... 13%  134M 2s\n",
            "  1600K .......... .......... .......... .......... .......... 13% 89.0M 2s\n",
            "  1650K .......... .......... .......... .......... .......... 14%  110M 2s\n",
            "  1700K .......... .......... .......... .......... .......... 14% 1.34M 2s\n",
            "  1750K .......... .......... .......... .......... .......... 15%  111M 2s\n",
            "  1800K .......... .......... .......... .......... .......... 15%  139M 2s\n",
            "  1850K .......... .......... .......... .......... .......... 16% 86.8M 2s\n",
            "  1900K .......... .......... .......... .......... .......... 16%  108M 2s\n",
            "  1950K .......... .......... .......... .......... .......... 16% 96.2M 2s\n",
            "  2000K .......... .......... .......... .......... .......... 17%  115M 2s\n",
            "  2050K .......... .......... .......... .......... .......... 17%  117M 2s\n",
            "  2100K .......... .......... .......... .......... .......... 18% 89.0M 1s\n",
            "  2150K .......... .......... .......... .......... .......... 18% 64.6M 1s\n",
            "  2200K .......... .......... .......... .......... .......... 18% 92.9M 1s\n",
            "  2250K .......... .......... .......... .......... .......... 19%  121M 1s\n",
            "  2300K .......... .......... .......... .......... .......... 19%  122M 1s\n",
            "  2350K .......... .......... .......... .......... .......... 20% 84.9M 1s\n",
            "  2400K .......... .......... .......... .......... .......... 20%  118M 1s\n",
            "  2450K .......... .......... .......... .......... .......... 21%  101M 1s\n",
            "  2500K .......... .......... .......... .......... .......... 21%  124M 1s\n",
            "  2550K .......... .......... .......... .......... .......... 21%  112M 1s\n",
            "  2600K .......... .......... .......... .......... .......... 22% 95.2M 1s\n",
            "  2650K .......... .......... .......... .......... .......... 22%  118M 1s\n",
            "  2700K .......... .......... .......... .......... .......... 23% 98.1M 1s\n",
            "  2750K .......... .......... .......... .......... .......... 23%  132M 1s\n",
            "  2800K .......... .......... .......... .......... .......... 24% 94.9M 1s\n",
            "  2850K .......... .......... .......... .......... .......... 24% 98.7M 1s\n",
            "  2900K .......... .......... .......... .......... .......... 24%  121M 1s\n",
            "  2950K .......... .......... .......... .......... .......... 25%  103M 1s\n",
            "  3000K .......... .......... .......... .......... .......... 25% 82.1M 1s\n",
            "  3050K .......... .......... .......... .......... .......... 26%  167M 1s\n",
            "  3100K .......... .......... .......... .......... .......... 26% 89.7M 1s\n",
            "  3150K .......... .......... .......... .......... .......... 26%  124M 1s\n",
            "  3200K .......... .......... .......... .......... .......... 27% 90.5M 1s\n",
            "  3250K .......... .......... .......... .......... .......... 27%  124M 1s\n",
            "  3300K .......... .......... .......... .......... .......... 28% 92.3M 1s\n",
            "  3350K .......... .......... .......... .......... .......... 28%  121M 1s\n",
            "  3400K .......... .......... .......... .......... .......... 29% 84.0M 1s\n",
            "  3450K .......... .......... .......... .......... .......... 29% 1.61M 1s\n",
            "  3500K .......... .......... .......... .......... .......... 29%  119M 1s\n",
            "  3550K .......... .......... .......... .......... .......... 30%  111M 1s\n",
            "  3600K .......... .......... .......... .......... .......... 30% 92.0M 1s\n",
            "  3650K .......... .......... .......... .......... .......... 31%  118M 1s\n",
            "  3700K .......... .......... .......... .......... .......... 31% 86.7M 1s\n",
            "  3750K .......... .......... .......... .......... .......... 32%  115M 1s\n",
            "  3800K .......... .......... .......... .......... .......... 32%  124M 1s\n",
            "  3850K .......... .......... .......... .......... .......... 32% 94.1M 1s\n",
            "  3900K .......... .......... .......... .......... .......... 33% 98.7M 1s\n",
            "  3950K .......... .......... .......... .......... .......... 33%  119M 1s\n",
            "  4000K .......... .......... .......... .......... .......... 34%  126M 1s\n",
            "  4050K .......... .......... .......... .......... .......... 34%  103M 1s\n",
            "  4100K .......... .......... .......... .......... .......... 34%  100M 1s\n",
            "  4150K .......... .......... .......... .......... .......... 35%  117M 1s\n",
            "  4200K .......... .......... .......... .......... .......... 35% 85.8M 1s\n",
            "  4250K .......... .......... .......... .......... .......... 36%  131M 1s\n",
            "  4300K .......... .......... .......... .......... .......... 36%  121M 1s\n",
            "  4350K .......... .......... .......... .......... .......... 37% 99.5M 1s\n",
            "  4400K .......... .......... .......... .......... .......... 37%  107M 1s\n",
            "  4450K .......... .......... .......... .......... .......... 37% 94.7M 1s\n",
            "  4500K .......... .......... .......... .......... .......... 38% 98.6M 1s\n",
            "  4550K .......... .......... .......... .......... .......... 38%  123M 1s\n",
            "  4600K .......... .......... .......... .......... .......... 39% 99.3M 1s\n",
            "  4650K .......... .......... .......... .......... .......... 39%  102M 1s\n",
            "  4700K .......... .......... .......... .......... .......... 40%  127M 1s\n",
            "  4750K .......... .......... .......... .......... .......... 40%  112M 1s\n",
            "  4800K .......... .......... .......... .......... .......... 40%  108M 1s\n",
            "  4850K .......... .......... .......... .......... .......... 41% 99.3M 1s\n",
            "  4900K .......... .......... .......... .......... .......... 41%  115M 1s\n",
            "  4950K .......... .......... .......... .......... .......... 42% 99.5M 1s\n",
            "  5000K .......... .......... .......... .......... .......... 42%  102M 1s\n",
            "  5050K .......... .......... .......... .......... .......... 42%  142M 1s\n",
            "  5100K .......... .......... .......... .......... .......... 43% 87.7M 1s\n",
            "  5150K .......... .......... .......... .......... .......... 43%  104M 0s\n",
            "  5200K .......... .......... .......... .......... .......... 44% 96.5M 0s\n",
            "  5250K .......... .......... .......... .......... .......... 44%  125M 0s\n",
            "  5300K .......... .......... .......... .......... .......... 45%  107M 0s\n",
            "  5350K .......... .......... .......... .......... .......... 45% 98.5M 0s\n",
            "  5400K .......... .......... .......... .......... .......... 45%  140M 0s\n",
            "  5450K .......... .......... .......... .......... .......... 46% 88.6M 0s\n",
            "  5500K .......... .......... .......... .......... .......... 46%  113M 0s\n",
            "  5550K .......... .......... .......... .......... .......... 47%  129M 0s\n",
            "  5600K .......... .......... .......... .......... .......... 47% 82.7M 0s\n",
            "  5650K .......... .......... .......... .......... .......... 48%  161M 0s\n",
            "  5700K .......... .......... .......... .......... .......... 48% 80.8M 0s\n",
            "  5750K .......... .......... .......... .......... .......... 48%  139M 0s\n",
            "  5800K .......... .......... .......... .......... .......... 49%  111M 0s\n",
            "  5850K .......... .......... .......... .......... .......... 49%  103M 0s\n",
            "  5900K .......... .......... .......... .......... .......... 50%  103M 0s\n",
            "  5950K .......... .......... .......... .......... .......... 50%  110M 0s\n",
            "  6000K .......... .......... .......... .......... .......... 50% 85.7M 0s\n",
            "  6050K .......... .......... .......... .......... .......... 51% 98.2M 0s\n",
            "  6100K .......... .......... .......... .......... .......... 51%  156M 0s\n",
            "  6150K .......... .......... .......... .......... .......... 52%  116M 0s\n",
            "  6200K .......... .......... .......... .......... .......... 52%  103M 0s\n",
            "  6250K .......... .......... .......... .......... .......... 53%  109M 0s\n",
            "  6300K .......... .......... .......... .......... .......... 53%  128M 0s\n",
            "  6350K .......... .......... .......... .......... .......... 53% 99.4M 0s\n",
            "  6400K .......... .......... .......... .......... .......... 54%  107M 0s\n",
            "  6450K .......... .......... .......... .......... .......... 54% 66.4M 0s\n",
            "  6500K .......... .......... .......... .......... .......... 55% 2.21M 0s\n",
            "  6550K .......... .......... .......... .......... .......... 55%  102M 0s\n",
            "  6600K .......... .......... .......... .......... .......... 56% 98.0M 0s\n",
            "  6650K .......... .......... .......... .......... .......... 56%  136M 0s\n",
            "  6700K .......... .......... .......... .......... .......... 56% 80.5M 0s\n",
            "  6750K .......... .......... .......... .......... .......... 57%  131M 0s\n",
            "  6800K .......... .......... .......... .......... .......... 57%  111M 0s\n",
            "  6850K .......... .......... .......... .......... .......... 58%  114M 0s\n",
            "  6900K .......... .......... .......... .......... .......... 58%  107M 0s\n",
            "  6950K .......... .......... .......... .......... .......... 58% 98.5M 0s\n",
            "  7000K .......... .......... .......... .......... .......... 59% 92.1M 0s\n",
            "  7050K .......... .......... .......... .......... .......... 59%  133M 0s\n",
            "  7100K .......... .......... .......... .......... .......... 60% 99.4M 0s\n",
            "  7150K .......... .......... .......... .......... .......... 60%  106M 0s\n",
            "  7200K .......... .......... .......... .......... .......... 61% 97.0M 0s\n",
            "  7250K .......... .......... .......... .......... .......... 61%  131M 0s\n",
            "  7300K .......... .......... .......... .......... .......... 61%  112M 0s\n",
            "  7350K .......... .......... .......... .......... .......... 62% 88.2M 0s\n",
            "  7400K .......... .......... .......... .......... .......... 62%  142M 0s\n",
            "  7450K .......... .......... .......... .......... .......... 63% 92.8M 0s\n",
            "  7500K .......... .......... .......... .......... .......... 63%  103M 0s\n",
            "  7550K .......... .......... .......... .......... .......... 64%  138M 0s\n",
            "  7600K .......... .......... .......... .......... .......... 64% 91.2M 0s\n",
            "  7650K .......... .......... .......... .......... .......... 64%  121M 0s\n",
            "  7700K .......... .......... .......... .......... .......... 65%  101M 0s\n",
            "  7750K .......... .......... .......... .......... .......... 65%  110M 0s\n",
            "  7800K .......... .......... .......... .......... .......... 66%  113M 0s\n",
            "  7850K .......... .......... .......... .......... .......... 66% 91.7M 0s\n",
            "  7900K .......... .......... .......... .......... .......... 66%  137M 0s\n",
            "  7950K .......... .......... .......... .......... .......... 67% 94.0M 0s\n",
            "  8000K .......... .......... .......... .......... .......... 67%  122M 0s\n",
            "  8050K .......... .......... .......... .......... .......... 68%  117M 0s\n",
            "  8100K .......... .......... .......... .......... .......... 68% 90.1M 0s\n",
            "  8150K .......... .......... .......... .......... .......... 69% 97.1M 0s\n",
            "  8200K .......... .......... .......... .......... .......... 69%  122M 0s\n",
            "  8250K .......... .......... .......... .......... .......... 69% 98.0M 0s\n",
            "  8300K .......... .......... .......... .......... .......... 70%  126M 0s\n",
            "  8350K .......... .......... .......... .......... .......... 70% 78.0M 0s\n",
            "  8400K .......... .......... .......... .......... .......... 71%  140M 0s\n",
            "  8450K .......... .......... .......... .......... .......... 71% 94.0M 0s\n",
            "  8500K .......... .......... .......... .......... .......... 72%  128M 0s\n",
            "  8550K .......... .......... .......... .......... .......... 72%  114M 0s\n",
            "  8600K .......... .......... .......... .......... .......... 72% 85.2M 0s\n",
            "  8650K .......... .......... .......... .......... .......... 73%  124M 0s\n",
            "  8700K .......... .......... .......... .......... .......... 73% 94.5M 0s\n",
            "  8750K .......... .......... .......... .......... .......... 74%  111M 0s\n",
            "  8800K .......... .......... .......... .......... .......... 74%  116M 0s\n",
            "  8850K .......... .......... .......... .......... .......... 74% 93.3M 0s\n",
            "  8900K .......... .......... .......... .......... .......... 75%  105M 0s\n",
            "  8950K .......... .......... .......... .......... .......... 75% 98.3M 0s\n",
            "  9000K .......... .......... .......... .......... .......... 76% 93.9M 0s\n",
            "  9050K .......... .......... .......... .......... .......... 76% 98.7M 0s\n",
            "  9100K .......... .......... .......... .......... .......... 77%  128M 0s\n",
            "  9150K .......... .......... .......... .......... .......... 77% 83.3M 0s\n",
            "  9200K .......... .......... .......... .......... .......... 77% 80.3M 0s\n",
            "  9250K .......... .......... .......... .......... .......... 78% 93.6M 0s\n",
            "  9300K .......... .......... .......... .......... .......... 78%  101M 0s\n",
            "  9350K .......... .......... .......... .......... .......... 79% 81.7M 0s\n",
            "  9400K .......... .......... .......... .......... .......... 79% 87.3M 0s\n",
            "  9450K .......... .......... .......... .......... .......... 80% 67.8M 0s\n",
            "  9500K .......... .......... .......... .......... .......... 80% 90.2M 0s\n",
            "  9550K .......... .......... .......... .......... .......... 80% 2.73M 0s\n",
            "  9600K .......... .......... .......... .......... .......... 81% 71.0M 0s\n",
            "  9650K .......... .......... .......... .......... .......... 81%  126M 0s\n",
            "  9700K .......... .......... .......... .......... .......... 82% 96.1M 0s\n",
            "  9750K .......... .......... .......... .......... .......... 82% 76.7M 0s\n",
            "  9800K .......... .......... .......... .......... .......... 82% 14.3M 0s\n",
            "  9850K .......... .......... .......... .......... .......... 83%  112M 0s\n",
            "  9900K .......... .......... .......... .......... .......... 83%  136M 0s\n",
            "  9950K .......... .......... .......... .......... .......... 84% 49.9M 0s\n",
            " 10000K .......... .......... .......... .......... .......... 84%  190M 0s\n",
            " 10050K .......... .......... .......... .......... .......... 85%  203M 0s\n",
            " 10100K .......... .......... .......... .......... .......... 85%  103M 0s\n",
            " 10150K .......... .......... .......... .......... .......... 85%  102M 0s\n",
            " 10200K .......... .......... .......... .......... .......... 86%  113M 0s\n",
            " 10250K .......... .......... .......... .......... .......... 86%  105M 0s\n",
            " 10300K .......... .......... .......... .......... .......... 87%  121M 0s\n",
            " 10350K .......... .......... .......... .......... .......... 87% 94.1M 0s\n",
            " 10400K .......... .......... .......... .......... .......... 88%  120M 0s\n",
            " 10450K .......... .......... .......... .......... .......... 88% 79.4M 0s\n",
            " 10500K .......... .......... .......... .......... .......... 88%  118M 0s\n",
            " 10550K .......... .......... .......... .......... .......... 89%  142M 0s\n",
            " 10600K .......... .......... .......... .......... .......... 89% 88.4M 0s\n",
            " 10650K .......... .......... .......... .......... .......... 90%  120M 0s\n",
            " 10700K .......... .......... .......... .......... .......... 90% 92.2M 0s\n",
            " 10750K .......... .......... .......... .......... .......... 90%  106M 0s\n",
            " 10800K .......... .......... .......... .......... .......... 91%  155M 0s\n",
            " 10850K .......... .......... .......... .......... .......... 91% 82.2M 0s\n",
            " 10900K .......... .......... .......... .......... .......... 92%  133M 0s\n",
            " 10950K .......... .......... .......... .......... .......... 92% 91.6M 0s\n",
            " 11000K .......... .......... .......... .......... .......... 93%  109M 0s\n",
            " 11050K .......... .......... .......... .......... .......... 93%  125M 0s\n",
            " 11100K .......... .......... .......... .......... .......... 93% 79.4M 0s\n",
            " 11150K .......... .......... .......... .......... .......... 94%  178M 0s\n",
            " 11200K .......... .......... .......... .......... .......... 94% 77.3M 0s\n",
            " 11250K .......... .......... .......... .......... .......... 95%  183M 0s\n",
            " 11300K .......... .......... .......... .......... .......... 95%  110M 0s\n",
            " 11350K .......... .......... .......... .......... .......... 96% 88.7M 0s\n",
            " 11400K .......... .......... .......... .......... .......... 96%  133M 0s\n",
            " 11450K .......... .......... .......... .......... .......... 96% 74.8M 0s\n",
            " 11500K .......... .......... .......... .......... .......... 97% 95.2M 0s\n",
            " 11550K .......... .......... .......... .......... .......... 97% 93.8M 0s\n",
            " 11600K .......... .......... .......... .......... .......... 98% 69.1M 0s\n",
            " 11650K .......... .......... .......... .......... .......... 98% 99.2M 0s\n",
            " 11700K .......... .......... .......... .......... .......... 98% 76.2M 0s\n",
            " 11750K .......... .......... .......... .......... .......... 99% 92.9M 0s\n",
            " 11800K .......... .......... .......... .......... .......... 99% 71.7M 0s\n",
            " 11850K .......... .......... ..                              100% 80.1M=0.5s\n",
            "\n",
            "2024-12-08 01:49:12 (23.6 MB/s) - ‘data/cnn_dm_cse447_dataset.hf.zip’ saved [12157788/12157788]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data/cnn_dm_cse447_dataset.hf.zip\n",
            "  inflating: data/__MACOSX/._cnn_dm_cse447_dataset.hf  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/dataset_dict.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/._dataset_dict.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/._test  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/._train  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/._val  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/test/state.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/test/._state.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/test/dataset_info.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/test/._dataset_info.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/test/data-00000-of-00001.arrow  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/test/._data-00000-of-00001.arrow  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/train/state.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/train/._state.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/train/dataset_info.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/train/._dataset_info.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/train/data-00000-of-00001.arrow  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/train/._data-00000-of-00001.arrow  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/val/state.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/val/._state.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/val/dataset_info.json  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/val/._dataset_info.json  \n",
            "  inflating: data/cnn_dm_cse447_dataset.hf/val/data-00000-of-00001.arrow  \n",
            "  inflating: data/__MACOSX/cnn_dm_cse447_dataset.hf/val/._data-00000-of-00001.arrow  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "--2024-12-08 01:49:13--  https://homes.cs.washington.edu/~kahuja/cse447/project3/data/kd_dataset.hf.zip\n",
            "Resolving homes.cs.washington.edu (homes.cs.washington.edu)... 128.208.3.226, 2607:4000:200:12::e2\n",
            "Connecting to homes.cs.washington.edu (homes.cs.washington.edu)|128.208.3.226|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1153651 (1.1M) [application/zip]\n",
            "Saving to: ‘data/kd_dataset.hf.zip’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  4%  550K 2s\n",
            "    50K .......... .......... .......... .......... ..........  8% 1.08M 1s\n",
            "   100K .......... .......... .......... .......... .......... 13% 46.5M 1s\n",
            "   150K .......... .......... .......... .......... .......... 17% 1.10M 1s\n",
            "   200K .......... .......... .......... .......... .......... 22% 55.9M 1s\n",
            "   250K .......... .......... .......... .......... .......... 26% 92.6M 1s\n",
            "   300K .......... .......... .......... .......... .......... 31% 93.3M 0s\n",
            "   350K .......... .......... .......... .......... .......... 35% 1.13M 0s\n",
            "   400K .......... .......... .......... .......... .......... 39% 45.8M 0s\n",
            "   450K .......... .......... .......... .......... .......... 44% 78.1M 0s\n",
            "   500K .......... .......... .......... .......... .......... 48%  116M 0s\n",
            "   550K .......... .......... .......... .......... .......... 53%  112M 0s\n",
            "   600K .......... .......... .......... .......... .......... 57% 99.9M 0s\n",
            "   650K .......... .......... .......... .......... .......... 62%  118M 0s\n",
            "   700K .......... .......... .......... .......... .......... 66% 95.3M 0s\n",
            "   750K .......... .......... .......... .......... .......... 71%  108M 0s\n",
            "   800K .......... .......... .......... .......... .......... 75% 1.16M 0s\n",
            "   850K .......... .......... .......... .......... .......... 79%  103M 0s\n",
            "   900K .......... .......... .......... .......... .......... 84%  109M 0s\n",
            "   950K .......... .......... .......... .......... .......... 88% 95.0M 0s\n",
            "  1000K .......... .......... .......... .......... .......... 93%  128M 0s\n",
            "  1050K .......... .......... .......... .......... .......... 97%  104M 0s\n",
            "  1100K .......... .......... ......                          100%  115M=0.3s\n",
            "\n",
            "2024-12-08 01:49:13 (3.99 MB/s) - ‘data/kd_dataset.hf.zip’ saved [1153651/1153651]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data/kd_dataset.hf.zip\n",
            "  inflating: data/__MACOSX/._kd_dataset.hf  \n",
            "  inflating: data/kd_dataset.hf/state.json  \n",
            "  inflating: data/kd_dataset.hf/dataset_info.json  \n",
            "  inflating: data/kd_dataset.hf/data-00000-of-00001.arrow  \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Download the dataset\n",
        "mkdir -p data\n",
        "wget https://homes.cs.washington.edu/~kahuja/cse447/project3/data/cnn_dm_cse447_dataset.hf.zip -O data/cnn_dm_cse447_dataset.hf.zip\n",
        "unzip -o data/cnn_dm_cse447_dataset.hf.zip -d data/\n",
        "wget https://homes.cs.washington.edu/~kahuja/cse447/project3/data/kd_dataset.hf.zip -O data/kd_dataset.hf.zip\n",
        "unzip -o data/kd_dataset.hf.zip -d data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW_3LjBKNX-F",
        "outputId": "927b3bd1-1aea-469c-dca1-99f22bda1505"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'summary'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['article', 'summary'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'summary'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "parent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "data_dir = os.path.join(parent_dir, \"data\")\n",
        "cnn_dm_cse447_dataset = load_from_disk(os.path.join(data_dir, \"cnn_dm_cse447_dataset.hf\"))\n",
        "cnn_dm_cse447_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OOfNJkhNX-F",
        "outputId": "05f1c09e-a8db-40ac-e8b5-ca7cce71da01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full article:\n",
            "('(CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he '\n",
            " 'is now \"feeling well\" after collapsing during Swansea\\'s 3-2 loss at '\n",
            " 'Tottenham in the Premier League on Wednesday. The worrying incident occurred '\n",
            " 'in the first half at White Hart Lane -- after Tottenham scored in the '\n",
            " 'seventh minute -- but the 29-year-old left the pitch conscious following '\n",
            " 'about five minutes of treatment. The Guardian added that he was wearing an '\n",
            " 'oxygen mask. Play was temporarily stopped before resuming. As the match '\n",
            " 'progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk '\n",
            " \"using the same word to describe Gomis' condition. Gomis spent the night in \"\n",
            " 'hospital as a precaution, Swansea said on its website. \"I wanted to reassure '\n",
            " 'you concerning my health,\" Gomis told the website. \"It actually looks much '\n",
            " 'scarier than it is physically dangerous, and I am feeling well now. \"I have '\n",
            " \"been under a great deal of stress and fatigue due to my father's health, \"\n",
            " 'which requires me to go back and forth from France. \"I was disappointed that '\n",
            " \"I couldn't help my team tonight, but now everything is back in order. I also \"\n",
            " 'want to thank everyone for their support and get well messages.\" Gomis had '\n",
            " 'similar fainting spells in France, which prompted the president of his '\n",
            " 'former club, Jean-Michel Aulas of Lyon, to tell French television in 2009: '\n",
            " '\"We can\\'t not be worried, it scares you each time.\" Swansea ran tests on '\n",
            " 'Gomis, said Monk, prior to signing him on a free transfer last July. \"He '\n",
            " 'just has a little bit of low blood pressure which causes you a little bit of '\n",
            " 'problems,\" Monk said in a televised interview on Sky. \"It\\'s been part of '\n",
            " \"his life. We were well aware of that when we signed him. He's done all the \"\n",
            " \"hospital checks and all the medical checks you can possibly do and it's just \"\n",
            " 'part of his life. \"It\\'s no problems whatsoever. It\\'s not as serious as it '\n",
            " 'looks.\" Gomis has scored two league goals for Swansea this season, mostly in '\n",
            " \"a backup role. He became the Welsh side's top striker when Wilfried Bony \"\n",
            " 'signed with Manchester City in January. Almost exactly three years ago at '\n",
            " 'White Hart Lane, then Bolton midfielder Fabrice Muamba collapsed after '\n",
            " 'suffering a cardiac arrest. He was near death,  according to Bolton, but '\n",
            " 'survived after being treated at the London Chest Hospital. He subsequently '\n",
            " 'retired. Other footballers, including Cameroon international Marc-Vivien Foe '\n",
            " \"in 2003 and Spanish international Antonio Puerta in 2007, didn't survive \"\n",
            " 'after collapsing on the pitch.')\n",
            "\n",
            "\n",
            "\n",
            "Gold summary:\n",
            "('Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\\n'\n",
            " 'But he reportedly left the pitch conscious and wearing an oxygen mask .\\n'\n",
            " 'Gomis later said that he was \"feeling well\"\\n'\n",
            " 'The incident came three years after Fabrice Muamba collapsed at White Hart '\n",
            " 'Lane .')\n"
          ]
        }
      ],
      "source": [
        "# Preview the dataset\n",
        "print(\"Full article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgXP92A6NX-F"
      },
      "source": [
        "### 2.1: Student Model\n",
        "\n",
        "In this exercise we will try to understand how well the student model i.e. GPT-2 does on the summarizaion task out of box. In later exercises we will try to improve the performance of the student model using knowledge distillation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93vxT9paNX-F"
      },
      "source": [
        "We start by loading the student model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyuhbgG1NX-F",
        "outputId": "748e5f06-8be1-410f-f606-3ac78a5322ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
        "\n",
        "student_model = student_model.to(device)\n",
        "student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "student_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmpHS4-rNX-F"
      },
      "source": [
        "Notice GPT-2 is a decoder-only model and has 12 layers. This is the smallest model in the GPT-2 family i.e. with only 124M parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_-2Y4zRNX-G"
      },
      "source": [
        "### 2.1.1 Preparing Data for Student Model\n",
        "\n",
        "Our student model is a language model and inherently a language model's job is to predict continuations of a sequence by predicting one token at a time. To perform specific tasks like summarization using language models, we need to prepare the data in such a format such that the possible continuation of the sequence is the output we want i.e. in this case the summary of the article.\n",
        "\n",
        "The GPT-2 paper found adding the TL;DR to the end of the article helps the model in generating better summaries. Implement the `prepare_articles` function that adds the string `\"\\nTL;DR:\"` to the end of each input articles and then tokenizes the articles.\n",
        "\n",
        "Useful tips:\n",
        "- While tokenizing the articles, by calling `tokenizer()` make sure to set `padding=\"max_length\"`, `truncation=True`, and `max_length=max_len`  so that the articles are padded to the same length and truncated if they are longer than the maximum length. Also, make sure to set `return_tensors=\"pt\"` so that the output is a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "id": "SnoeRG60NX-G",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b5a322ee8328a5a2d0bc97cf93cd0dd",
          "grade": false,
          "grade_id": "cell-0f3b37e5baff721d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def prepare_articles_for_student_model(articles, student_tokenizer, max_len=1024):\n",
        "\n",
        "    \"\"\"\n",
        "    Processes and tokenizes articles into a format that can be used for summarization by the student model\n",
        "    and then tokenizes the articles using the student tokenizer.\n",
        "\n",
        "    Inputs:\n",
        "    - articles: A list of articles to be summarized.\n",
        "    - student_tokenizer: The tokenizer to use for tokenizing the articles.\n",
        "    - max_len: The maximum length of the articles.\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_articles: A dictionary containing the input ids and attention mask of the tokenized articles.\n",
        "    \"\"\"\n",
        "    tokenized_articles = None\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    # take the articles and tokenize\n",
        "    articles = [article + \"\\nTL;DR:\" for article in articles]\n",
        "    tokenized_articles = student_tokenizer(\n",
        "        articles,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors=\"pt\")\n",
        "    return tokenized_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "iMLkUEipNX-G",
        "outputId": "17ca80e1-8cd2-4c84-ed97-3200d6cc3967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "######################################################\n",
        "#  The following code is given to you. DO NOT MODIFY.\n",
        "######################################################\n",
        "\n",
        "\n",
        "def test_prepare_articles_for_student_model():\n",
        "    # Setup\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
        "    student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "\n",
        "    # Test 2: Multiple articles\n",
        "    articles = [\"First article.\", \"Second article.\", \"Third article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "    assert (\n",
        "        output[\"attention_mask\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "\n",
        "    # Test 3: TL;DR addition\n",
        "    articles = [\"Test article.\"]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    decoded = student_tokenizer.decode(output[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    assert \"TL;DR:\" in decoded, \"TL;DR: should be added to the article\"\n",
        "    assert decoded == \"Test article.\\nTL;DR:\", \"Article format should be correct\"\n",
        "\n",
        "    # Test 4: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    short_article = \"Short.\"\n",
        "    articles = [long_article, short_article]\n",
        "    output = prepare_articles_for_student_model(articles, student_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[1] == 1024\n",
        "    ), \"Should be padded/truncated to max length\"\n",
        "    assert (output[\"attention_mask\"][1] == 0).any(), \"Short article should have padding\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_prepare_articles_for_student_model()\n",
        "######################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoBhEGfmNX-G"
      },
      "source": [
        "### 2.1.2 Summarizing with Student Model\n",
        "\n",
        "Use the `generate()` method to generate summaries from the student model. Refer to the end of the Section 1 of the notebook for an example of how to use the `generate()` method. You can also learn more about the `generate()` method [here](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate).\n",
        "\n",
        "Helpful tips:\n",
        "- Instead of generating summaries one by one, we recommend generating summaries in batches to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "deletable": false,
        "id": "N5tTbCJXNX-G",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6f8ae69bf3416a6f4160e68259fbaafc",
          "grade": false,
          "grade_id": "cell-d987ea61d6ff75c8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def summarize_wth_student_model(\n",
        "    articles,\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates a list of summaries for a list of articles using the student model.\n",
        "\n",
        "    Inputs:\n",
        "    - articles: A list of articles to be summarized.\n",
        "    - student_model: The student model to use for summarization.\n",
        "    - student_tokenizer: The tokenizer corresponding to the student model.\n",
        "    - batch_size: The batch size to use for summarization.\n",
        "    - max_new_tokens: The maximum number of new tokens to generate.\n",
        "    - do_sample: Whether to use sampling or greedy decoding.\n",
        "    - p: The p parameter for top-p sampling.\n",
        "    - temperature: The temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "    - summaries: A list of summaries for the articles.\n",
        "    \"\"\"\n",
        "\n",
        "    student_model.eval()\n",
        "    summaries = []\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(articles), batch_size)):\n",
        "            # ToDo: Tokenize the batch of articles\n",
        "            tokenized_articles = None\n",
        "            # YOUR CODE HERE\n",
        "            tokenized_articles = prepare_articles_for_student_model(articles[i : i + batch_size], student_tokenizer)\n",
        "\n",
        "            # Move the data to the device\n",
        "            tokenized_articles = {\n",
        "                key: value.to(device) for key, value in tokenized_articles.items()\n",
        "            }\n",
        "\n",
        "            # ToDo: Generate summaries from the model using the .generate method\n",
        "            generated_summaries = None\n",
        "            # YOUR CODE HERE\n",
        "            generated_summaries = student_model.generate(\n",
        "              **tokenized_articles,\n",
        "              max_new_tokens=max_new_tokens,\n",
        "              do_sample=do_sample,\n",
        "              top_p=p,\n",
        "              temperature=temperature,\n",
        "              output_hidden_states=True,  # Add this line\n",
        "              return_dict_in_generate=True,  # Add this line\n",
        "            )\n",
        "\n",
        "            # ToDo: Convert the generated summaries to text. Hint: Use the `batch_decode` method of the tokenizer.\n",
        "            # Make sure to only decode the generated tokens, i.e., ignore the input tokens.\n",
        "            # Also, make sure to set `skip_special_tokens=True` to ignore the special tokens.\n",
        "            decoded_summaries = None\n",
        "            # YOUR CODE HERE\n",
        "            # only decode generated tokens\n",
        "            decoded_summaries = student_tokenizer.batch_decode(generated_summaries.sequences[:, tokenized_articles['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "            # Add the decoded summaries to the list\n",
        "            summaries.extend(decoded_summaries)\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOnuPZlxNX-H",
        "outputId": "306a8eb8-6a6e-48ac-acad-06c8a496c1bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (1024). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 33%|███▎      | 1/3 [00:01<00:02,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 67%|██████▋   | 2/3 [00:02<00:01,  1.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 3/3 [00:03<00:00,  1.13s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/home/arunasrivastava/AttentionGate/env/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def test_summarize_wth_student_model():\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article about AI.\"]\n",
        "    summaries = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, device=device\n",
        "    )\n",
        "    assert isinstance(summaries, list), \"Output should be a list\"\n",
        "    assert len(summaries) == len(articles), \"Should generate one summary per article\"\n",
        "    assert isinstance(summaries[0], str), \"Each summary should be a string\"\n",
        "    assert len(summaries[0]) > 0, \"Summaries should not be empty\"\n",
        "\n",
        "    # Test 2: Batch processing\n",
        "    articles = [\n",
        "        \"First article.\",\n",
        "        \"Second article.\",\n",
        "        \"Third article.\",\n",
        "    ] * 4  # 12 articles\n",
        "    batch_size = 4\n",
        "    summaries = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, batch_size=batch_size, device=device\n",
        "    )\n",
        "    assert len(summaries) == len(articles), \"Should generate summary for each article\"\n",
        "\n",
        "    # Test 3: Generation parameters\n",
        "    articles = [\n",
        "        \"Test article for parameter checking.\",\n",
        "        \"Test article for parameter checking.\",\n",
        "    ]\n",
        "    # Test with different generation parameters\n",
        "    summaries_greedy = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, do_sample=False, device=device\n",
        "    )\n",
        "    summaries_sampling = summarize_wth_student_model(\n",
        "        articles,\n",
        "        student_model,\n",
        "        student_tokenizer,\n",
        "        do_sample=True,\n",
        "        p=0.9,\n",
        "        temperature=0.7,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    assert summaries_greedy[0] == summaries_greedy[1], \"Greedy decoding should give the same result\"\n",
        "    assert (\n",
        "        summaries_sampling[0] != summaries_sampling[1]\n",
        "    ), \"Sampling should give different results\"\n",
        "\n",
        "    # Test 4: Check only generated tokens are in summary\n",
        "    article = \"This is a test article about artificial intelligence. It contains specific phrases that should not appear in the summary unless generated.\"\n",
        "    summaries = summarize_wth_student_model(\n",
        "        [article], student_model, student_tokenizer, device=device\n",
        "    )\n",
        "    # Verify the input article text isn't in the summary\n",
        "    assert article not in summaries[0], \"Summary should not contain the input article\"\n",
        "\n",
        "    # Test 5: Check for special tokens in output\n",
        "    articles = [\"Test article for special token checking.\"]\n",
        "    summaries = summarize_wth_student_model(\n",
        "        articles, student_model, student_tokenizer, device=device\n",
        "    )\n",
        "\n",
        "    # Check if any special tokens exist in the summary\n",
        "    for special_token in student_tokenizer.all_special_tokens:\n",
        "        assert (\n",
        "            special_token not in summaries[0]\n",
        "        ), f\"Summary contains special token: {special_token}\"\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_summarize_wth_student_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6pSOVIlNX-H"
      },
      "source": [
        "Let's generate summaries for the validation set using the student model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1shF7ExBNX-H",
        "outputId": "6f40985a-b70d-462a-a1b4-e18e9e5edec6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/125 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  1%|          | 1/125 [00:01<02:23,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  2%|▏         | 2/125 [00:02<02:23,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  2%|▏         | 3/125 [00:03<02:23,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  3%|▎         | 4/125 [00:04<02:21,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  4%|▍         | 5/125 [00:05<02:20,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  5%|▍         | 6/125 [00:07<02:19,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  6%|▌         | 7/125 [00:08<02:19,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  6%|▋         | 8/125 [00:09<02:17,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  7%|▋         | 9/125 [00:10<02:16,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  8%|▊         | 10/125 [00:11<02:15,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  9%|▉         | 11/125 [00:12<02:14,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|▉         | 12/125 [00:14<02:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|█         | 13/125 [00:15<02:11,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 11%|█         | 14/125 [00:16<02:10,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 12%|█▏        | 15/125 [00:17<02:09,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 13%|█▎        | 16/125 [00:18<02:08,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 14%|█▎        | 17/125 [00:19<02:06,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 14%|█▍        | 18/125 [00:21<02:05,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 15%|█▌        | 19/125 [00:22<02:05,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 16%|█▌        | 20/125 [00:23<02:04,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 17%|█▋        | 21/125 [00:24<02:02,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 18%|█▊        | 22/125 [00:25<02:01,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 18%|█▊        | 23/125 [00:27<01:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 19%|█▉        | 24/125 [00:28<01:58,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 25/125 [00:29<01:57,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 21%|██        | 26/125 [00:30<01:55,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 22%|██▏       | 27/125 [00:31<01:54,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 22%|██▏       | 28/125 [00:32<01:53,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 23%|██▎       | 29/125 [00:34<01:52,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 24%|██▍       | 30/125 [00:35<01:51,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 25%|██▍       | 31/125 [00:36<01:49,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 26%|██▌       | 32/125 [00:37<01:48,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 26%|██▋       | 33/125 [00:38<01:47,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 27%|██▋       | 34/125 [00:39<01:46,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 28%|██▊       | 35/125 [00:41<01:45,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 29%|██▉       | 36/125 [00:42<01:44,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|██▉       | 37/125 [00:43<01:43,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 38/125 [00:44<01:41,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 31%|███       | 39/125 [00:45<01:41,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 32%|███▏      | 40/125 [00:46<01:39,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 33%|███▎      | 41/125 [00:48<01:38,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 34%|███▎      | 42/125 [00:49<01:37,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 34%|███▍      | 43/125 [00:50<01:35,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 35%|███▌      | 44/125 [00:51<01:34,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 36%|███▌      | 45/125 [00:52<01:34,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 37%|███▋      | 46/125 [00:53<01:32,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 38%|███▊      | 47/125 [00:55<01:31,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 38%|███▊      | 48/125 [00:56<01:31,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 39%|███▉      | 49/125 [00:57<01:29,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 50/125 [00:58<01:28,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 41%|████      | 51/125 [00:59<01:27,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 42%|████▏     | 52/125 [01:01<01:26,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 42%|████▏     | 53/125 [01:02<01:24,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 43%|████▎     | 54/125 [01:03<01:23,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 44%|████▍     | 55/125 [01:04<01:21,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 45%|████▍     | 56/125 [01:05<01:20,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 46%|████▌     | 57/125 [01:06<01:19,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 46%|████▋     | 58/125 [01:08<01:18,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 47%|████▋     | 59/125 [01:09<01:17,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 48%|████▊     | 60/125 [01:10<01:16,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 49%|████▉     | 61/125 [01:11<01:14,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|████▉     | 62/125 [01:12<01:13,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 63/125 [01:13<01:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 51%|█████     | 64/125 [01:15<01:11,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 52%|█████▏    | 65/125 [01:16<01:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 53%|█████▎    | 66/125 [01:17<01:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 54%|█████▎    | 67/125 [01:18<01:07,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 54%|█████▍    | 68/125 [01:19<01:06,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 55%|█████▌    | 69/125 [01:20<01:05,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 56%|█████▌    | 70/125 [01:22<01:04,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 57%|█████▋    | 71/125 [01:23<01:03,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 58%|█████▊    | 72/125 [01:24<01:02,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 58%|█████▊    | 73/125 [01:25<01:01,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 59%|█████▉    | 74/125 [01:26<00:59,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 75/125 [01:27<00:58,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 61%|██████    | 76/125 [01:29<00:57,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 62%|██████▏   | 77/125 [01:30<00:55,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 62%|██████▏   | 78/125 [01:31<00:54,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 63%|██████▎   | 79/125 [01:32<00:53,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 64%|██████▍   | 80/125 [01:33<00:52,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 65%|██████▍   | 81/125 [01:34<00:51,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 66%|██████▌   | 82/125 [01:36<00:50,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 66%|██████▋   | 83/125 [01:37<00:49,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 67%|██████▋   | 84/125 [01:38<00:48,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 68%|██████▊   | 85/125 [01:39<00:47,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 69%|██████▉   | 86/125 [01:40<00:46,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|██████▉   | 87/125 [01:42<00:44,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 88/125 [01:43<00:43,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 71%|███████   | 89/125 [01:44<00:42,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 72%|███████▏  | 90/125 [01:45<00:41,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 73%|███████▎  | 91/125 [01:46<00:40,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 74%|███████▎  | 92/125 [01:47<00:39,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 74%|███████▍  | 93/125 [01:49<00:37,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 75%|███████▌  | 94/125 [01:50<00:36,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 76%|███████▌  | 95/125 [01:51<00:35,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 77%|███████▋  | 96/125 [01:52<00:33,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 78%|███████▊  | 97/125 [01:53<00:32,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 78%|███████▊  | 98/125 [01:54<00:31,  1.18s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 79%|███████▉  | 99/125 [01:56<00:30,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 100/125 [01:57<00:29,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 81%|████████  | 101/125 [01:58<00:28,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 82%|████████▏ | 102/125 [01:59<00:26,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 82%|████████▏ | 103/125 [02:00<00:25,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 83%|████████▎ | 104/125 [02:01<00:24,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 84%|████████▍ | 105/125 [02:03<00:23,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 85%|████████▍ | 106/125 [02:04<00:22,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 86%|████████▌ | 107/125 [02:05<00:21,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 86%|████████▋ | 108/125 [02:06<00:19,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 87%|████████▋ | 109/125 [02:07<00:18,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 88%|████████▊ | 110/125 [02:08<00:17,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 89%|████████▉ | 111/125 [02:10<00:16,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|████████▉ | 112/125 [02:11<00:15,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 113/125 [02:12<00:14,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 91%|█████████ | 114/125 [02:13<00:12,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 92%|█████████▏| 115/125 [02:14<00:11,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 93%|█████████▎| 116/125 [02:15<00:10,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 94%|█████████▎| 117/125 [02:17<00:09,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 94%|█████████▍| 118/125 [02:18<00:08,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 95%|█████████▌| 119/125 [02:19<00:06,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 96%|█████████▌| 120/125 [02:20<00:05,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 97%|█████████▋| 121/125 [02:21<00:04,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 98%|█████████▊| 122/125 [02:22<00:03,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 98%|█████████▊| 123/125 [02:24<00:02,  1.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 99%|█████████▉| 124/125 [02:25<00:01,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 125/125 [02:26<00:00,  1.17s/it]\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "pred_summaries = summarize_wth_student_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"],\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RL-q4BaNX-H",
        "outputId": "66a7b1ca-45cd-4c84-d1ad-8f4810a2703b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article:\n",
            "('(CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he '\n",
            " 'is now \"feeling well\" after collapsing during Swansea\\'s 3-2 loss at '\n",
            " 'Tottenham in the Premier League on Wednesday. The worrying incident occurred '\n",
            " 'in the first half at White Hart Lane -- after Tottenham scored in the '\n",
            " 'seventh minute -- but the 29-year-old left the pitch conscious following '\n",
            " 'about five minutes of treatment. The Guardian added that he was wearing an '\n",
            " 'oxygen mask. Play was temporarily stopped before resuming. As the match '\n",
            " 'progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk '\n",
            " \"using the same word to describe Gomis' condition. Gomis spent the night in \"\n",
            " 'hospital as a precaution, Swansea said on its website. \"I wanted to reassure '\n",
            " 'you concerning my health,\" Gomis told the website. \"It actually looks much '\n",
            " 'scarier than it is physically dangerous, and I am feeling well now. \"I have '\n",
            " \"been under a great deal of stress and fatigue due to my father's health, \"\n",
            " 'which requires me to go back and forth from France. \"I was disappointed that '\n",
            " \"I couldn't help my team tonight, but now everything is back in order. I also \"\n",
            " 'want to thank everyone for their support and get well messages.\" Gomis had '\n",
            " 'similar fainting spells in France, which prompted the president of his '\n",
            " 'former club, Jean-Michel Aulas of Lyon, to tell French television in 2009: '\n",
            " '\"We can\\'t not be worried, it scares you each time.\" Swansea ran tests on '\n",
            " 'Gomis, said Monk, prior to signing him on a free transfer last July. \"He '\n",
            " 'just has a little bit of low blood pressure which causes you a little bit of '\n",
            " 'problems,\" Monk said in a televised interview on Sky. \"It\\'s been part of '\n",
            " \"his life. We were well aware of that when we signed him. He's done all the \"\n",
            " \"hospital checks and all the medical checks you can possibly do and it's just \"\n",
            " 'part of his life. \"It\\'s no problems whatsoever. It\\'s not as serious as it '\n",
            " 'looks.\" Gomis has scored two league goals for Swansea this season, mostly in '\n",
            " \"a backup role. He became the Welsh side's top striker when Wilfried Bony \"\n",
            " 'signed with Manchester City in January. Almost exactly three years ago at '\n",
            " 'White Hart Lane, then Bolton midfielder Fabrice Muamba collapsed after '\n",
            " 'suffering a cardiac arrest. He was near death,  according to Bolton, but '\n",
            " 'survived after being treated at the London Chest Hospital. He subsequently '\n",
            " 'retired. Other footballers, including Cameroon international Marc-Vivien Foe '\n",
            " \"in 2003 and Spanish international Antonio Puerta in 2007, didn't survive \"\n",
            " 'after collapsing on the pitch.')\n",
            "***********************\n",
            "\n",
            "\n",
            "Generated summary:\n",
            "(' Gomis fainted.\\n'\n",
            " '\"I have no idea what happened to him. We don\\'t know why he\\'s now feeling '\n",
            " 'well,\" Monk said. \"We don\\'t know why his head is spinning and that he is in '\n",
            " 'some sort of pain. We have spoken to several people and we have no idea how '\n",
            " 'this happened. That is a matter for the medical team.\" The first report of '\n",
            " 'the problem, however, came late Monday. A spokesman for the club confirmed '\n",
            " 'the problem was not a \"')\n",
            "***********************\n",
            "\n",
            "\n",
            "Gold summary:\n",
            "('Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\\n'\n",
            " 'But he reportedly left the pitch conscious and wearing an oxygen mask .\\n'\n",
            " 'Gomis later said that he was \"feeling well\"\\n'\n",
            " 'The incident came three years after Fabrice Muamba collapsed at White Hart '\n",
            " 'Lane .')\n"
          ]
        }
      ],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(pred_summaries[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIZO7Zu9NX-H"
      },
      "source": [
        "As you can see, the generated summary is not very good, with a lot of completely irrelevant text. Let's try to quantify this by evaluating the summaries using the ROUGE score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtDBvNMMNX-H"
      },
      "source": [
        "### Evaluating Student Model\n",
        "\n",
        "We will use the ROUGE score to evaluate the summaries generated by the student model. ROUGE measures how similar a generated summary is to a reference (human-written) summary by comparing overlapping words and phrases. Think of it like a sophisticated \"spot the differences\" between texts.\n",
        "\n",
        "Example:\n",
        "- **Reference**: \"The cat sat on the mat\"\n",
        "- **Generated**: \"The cat lay on the mat\"\n",
        "\n",
        "ROUGE has three main variants:\n",
        "1. **ROUGE-1**: Matches single words (In example: 5/6 words match)\n",
        "2. **ROUGE-2**: Matches word pairs (In example: \"the cat\", \"on the\", \"the mat\" match)\n",
        "3. **ROUGE-L**: Finds longest matching sequences in order\n",
        "\n",
        "Scores range from 0 to 1, with higher being better. While widely used in summarization evaluation, ROUGE isn't perfect - it focuses on matching words rather than meaning.\n",
        "\n",
        "Reference: Lin, C. Y. (2004). ROUGE: A package for automatic evaluation of summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2A637KLDNX-H",
        "outputId": "1bce0645-604f-46ae-b0b9-efce31d5a25b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0e69162a77147d68d3ac3547cc2530e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': np.float64(0.19039839851840795),\n",
              " 'rouge2': np.float64(0.03159766551782221),\n",
              " 'rougeL': np.float64(0.12444820861454539),\n",
              " 'rougeLsum': np.float64(0.16276151851450316)}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluate the summaries\n",
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "scores = rouge.compute(predictions=pred_summaries, references=cnn_dm_cse447_dataset[\"val\"][\"summary\"], use_stemmer=True)\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq0QHHD6NX-H"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.19\n",
        "- rouge2: 0.03\n",
        "- rougeL: 0.12\n",
        "- rougeLsum: 0.16\n",
        "\n",
        "Differences of +/- 0.02 in ROUGE scores are acceptable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhdM9rmtNX-H"
      },
      "source": [
        "The model performs relatively poorly on the dataset, especially considering the ROUGE-2 and ROUGE-L scores, which are very low. Let's see if we can improve this by using a teacher model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxGRVI2wNX-H"
      },
      "source": [
        "### 2.2 Generate Data from a Teacher Model\n",
        "\n",
        "We will now start with step 1 of knowledge distillation i.e. generating data from a teacher model. As we mentioned earlier, we will use the Qwen2.5-1.5B-Instruct model as our teacher model. Note that Qwen2.5-1.5B-Instruct is an instruction tuned model, which is different from a base language model like GPT-2. Instruction tuned models are obtained from base language models by fine-tuning them on a diverse set of instructions and their desired outputs. This enables the model to become better at following instructions and is the magic recipe for the success of recent large language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CiSdDdGNX-H",
        "outputId": "4b056bf0-9678-40eb-ad1e-73d7adafaaad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aedf3fe188c4e9cac2499a9d99f4817",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef9ea4de7f6c4ba1b642a18342769acb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80bb654d190d4f51a11d5283f29cddb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b77a5349b5a40f9a45b53b4499bc327",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca79cba970d346828edb25076ead0b6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f678515fa32f440daeb062c816975faa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf2bdd26c87a445da806ce238aeec738",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 1536)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
              "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
              "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
              "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load a teacher model.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        ")\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\", padding_side=\"left\"\n",
        ")\n",
        "teacher_model = teacher_model.to(device)\n",
        "teacher_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPtYBysINX-H"
      },
      "source": [
        "### 2.2.1 Preparing Data for Teacher Model\n",
        "\n",
        "We need to prepare the data for the teacher model in a specific format. Since the teacher model is an instruction-tuned model, it has been adapted to follow instructions. Hence, instead of formatting the text according to a completion problem, we need to format it according to an instruction following the problem. We will do this by adding an instruction to the beginning of each article, i.e., \"Summarize the following article.\" Further, we will also instruct the model to output the summary in a specific format by appending a suffix to the end of each article, i.e., \"Start your summary with 'TL;DR:. '\" In short, the articles should be formatted as follows:\n",
        "\n",
        "`\"Summarize the following article:\\n\\n<article>\\n\\nStart your summary with 'TL;DR:'\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "deletable": false,
        "id": "xPwDQOpTNX-H",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7723288824b6f27589989376bbd5a284",
          "grade": false,
          "grade_id": "cell-68a3da94807a2ea3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def prepare_articles_teacher(articles, teacher_tokenizer, max_len=1024):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepares the articles for the teacher model and performs tokenization.\n",
        "\n",
        "    Inputs\n",
        "    - articles: The articles to prepare and tokenize\n",
        "    - teacher_tokenizer: The tokenizer for the teacher model\n",
        "    - max_len: The max length to use for tokenizing\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_articles: The tokenized articles in the format of a dictionary with keys `input_ids` and `attention_mask`\n",
        "    \"\"\"\n",
        "\n",
        "    # ToDo: Add the instructions as specified above to each article\n",
        "    articles_with_instructions = None\n",
        "    # YOUR CODE HERE\n",
        "    articles_with_instructions = [f\"Summarize the following article:\\n\\n{article}\\n\\nStart your summary with 'TL;DR:'\" for article in articles]\n",
        "\n",
        "    # The instruction-tuned models to be in chat format. Read more here: https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "    articles_chat = [[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at summarizing articles.\"},\n",
        "        {\"role\": \"user\", \"content\": article}\n",
        "    ] for article in articles_with_instructions]\n",
        "    # Apply chat template to the articles\n",
        "    articles_chat = teacher_tokenizer.apply_chat_template(articles_chat, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # ToDo: Tokenize the articles. Hint: Set `add_special_tokens=False` since special tokens are already added in the chat template.\n",
        "    tokenized_articles = None\n",
        "    # YOUR CODE HERE\n",
        "    tokenized_articles = teacher_tokenizer(articles_chat, add_special_tokens=False, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "\n",
        "    return tokenized_articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14fiFJfqNX-H",
        "outputId": "79241f9c-63b9-4b43-be14-46155eff0f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "def test_prepare_articles_teacher():\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    articles = [\"This is a test article.\"]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer)\n",
        "\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "\n",
        "    # Test 2: Instruction addition\n",
        "    decoded = teacher_tokenizer.decode(output[\"input_ids\"][0], skip_special_tokens=True)\n",
        "    assert (\n",
        "        \"Summarize the following article:\" in decoded\n",
        "    ), \"Should contain summarization instruction\"\n",
        "    assert (\n",
        "        \"Start your summary with 'TL;DR:'\" in decoded\n",
        "    ), \"Should contain TL;DR instruction\"\n",
        "\n",
        "    # Test 3: Multiple articles\n",
        "    articles = [\"First article.\", \"Second article.\", \"Third article.\"]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "    assert (\n",
        "        output[\"attention_mask\"].shape[0] == 3\n",
        "    ), \"Batch size should match number of articles\"\n",
        "\n",
        "    # Test 4: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    short_article = \"Short.\"\n",
        "    articles = [long_article, short_article]\n",
        "    output = prepare_articles_teacher(articles, teacher_tokenizer, max_len=1024)\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[1] == 1024\n",
        "    ), \"Should be padded/truncated to max length\"\n",
        "    assert (output[\"attention_mask\"][1] == 0).any(), \"Short article should have padding\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_prepare_articles_teacher()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0zCFIfNNX-I"
      },
      "source": [
        "Let's have a look at the tokenized articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILjQgW8iNX-I",
        "outputId": "4de66f78-2685-4a42-e296-531a7c5b105e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('system\\n'\n",
            " 'You are a helpful assistant and an expert at summarizing articles.\\n'\n",
            " 'user\\n'\n",
            " 'Summarize the following article:\\n'\n",
            " '\\n'\n",
            " '(CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he '\n",
            " 'is now \"feeling well\" after collapsing during Swansea\\'s 3-2 loss at '\n",
            " 'Tottenham in the Premier League on Wednesday. The worrying incident occurred '\n",
            " 'in the first half at White Hart Lane -- after Tottenham scored in the '\n",
            " 'seventh minute -- but the 29-year-old left the pitch conscious following '\n",
            " 'about five minutes of treatment. The Guardian added that he was wearing an '\n",
            " 'oxygen mask. Play was temporarily stopped before resuming. As the match '\n",
            " 'progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk '\n",
            " \"using the same word to describe Gomis' condition. Gomis spent the night in \"\n",
            " 'hospital as a precaution, Swansea said on its website. \"I wanted to reassure '\n",
            " 'you concerning my health,\" Gomis told the website. \"It actually looks much '\n",
            " 'scarier than it is physically dangerous, and I am feeling well now. \"I have '\n",
            " \"been under a great deal of stress and fatigue due to my father's health, \"\n",
            " 'which requires me to go back and forth from France. \"I was disappointed that '\n",
            " \"I couldn't help my team tonight, but now everything is back in order. I also \"\n",
            " 'want to thank everyone for their support and get well messages.\" Gomis had '\n",
            " 'similar fainting spells in France, which prompted the president of his '\n",
            " 'former club, Jean-Michel Aulas of Lyon, to tell French television in 2009: '\n",
            " '\"We can\\'t not be worried, it scares you each time.\" Swansea ran tests on '\n",
            " 'Gomis, said Monk, prior to signing him on a free transfer last July. \"He '\n",
            " 'just has a little bit of low blood pressure which causes you a little bit of '\n",
            " 'problems,\" Monk said in a televised interview on Sky. \"It\\'s been part of '\n",
            " \"his life. We were well aware of that when we signed him. He's done all the \"\n",
            " \"hospital checks and all the medical checks you can possibly do and it's just \"\n",
            " 'part of his life. \"It\\'s no problems whatsoever. It\\'s not as serious as it '\n",
            " 'looks.\" Gomis has scored two league goals for Swansea this season, mostly in '\n",
            " \"a backup role. He became the Welsh side's top striker when Wilfried Bony \"\n",
            " 'signed with Manchester City in January. Almost exactly three years ago at '\n",
            " 'White Hart Lane, then Bolton midfielder Fabrice Muamba collapsed after '\n",
            " 'suffering a cardiac arrest. He was near death,  according to Bolton, but '\n",
            " 'survived after being treated at the London Chest Hospital. He subsequently '\n",
            " 'retired. Other footballers, including Cameroon international Marc-Vivien Foe '\n",
            " \"in 2003 and Spanish international Antonio Puerta in 2007, didn't survive \"\n",
            " 'after collapsing on the pitch.\\n'\n",
            " '\\n'\n",
            " \"Start your summary with 'TL;DR:'\\n\"\n",
            " 'assistant\\n')\n"
          ]
        }
      ],
      "source": [
        "tokenized_articles = prepare_articles_teacher(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"][:2], teacher_tokenizer\n",
        ")\n",
        "decoded_articles = teacher_tokenizer.batch_decode(\n",
        "    tokenized_articles[\"input_ids\"], skip_special_tokens=True\n",
        ")\n",
        "pprint(decoded_articles[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz18w6c9NX-I"
      },
      "source": [
        "Notice the system, user, and assistant tags in the prompt. The system tag is followed by an instruction to ground the model to be a helpful assistant and be an expert at summarizing articles. The user tag is followed by the article to be summarized and the instruction about the task. We have the assistant tag in the end and the model is expected to generate the summary after the assistant tag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoX-31KXNX-I"
      },
      "source": [
        "### 2.2.2. Generate summaries with teacher model\n",
        "\n",
        "Similar to the student model, we will use the `generate()` method to generate summaries from the teacher model. Implement the `summarize_with_teacher_model` function below to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "deletable": false,
        "id": "80yKN_KONX-I",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "82be9a3e43593d77f5fd4b86c847f3c0",
          "grade": false,
          "grade_id": "cell-ca1d4e6c170ef5d9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def summarize_with_teacher_model(\n",
        "    articles,\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates summaries for a list of articles using the teacher model.\n",
        "    Essentially the same as the function `summarize_wth_student_model`, but we will use the prepare articles function for the teacher model here.\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    teacher_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(articles), batch_size)):\n",
        "            # YOUR CODE HERE\n",
        "            tokenized_articles = prepare_articles_teacher(articles[i : i + batch_size], teacher_tokenizer)\n",
        "            tokenized_articles = {\n",
        "                key: value.to(device) for key, value in tokenized_articles.items()\n",
        "            }\n",
        "            generated_summaries = teacher_model.generate(\n",
        "                **tokenized_articles,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=do_sample,\n",
        "                top_p=p,\n",
        "                temperature=temperature,\n",
        "                output_hidden_states=True,  # Add this line\n",
        "                return_dict_in_generate=True,  # Add this line\n",
        "            )\n",
        "            decoded_summaries = teacher_tokenizer.batch_decode(generated_summaries.sequences[:, tokenized_articles['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "            # Add the decoded summaries to the list\n",
        "            summaries.extend(decoded_summaries)\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pqFSenhNX-I"
      },
      "source": [
        "Let's first check how well the teacher model performs on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz4qRg6ZNX-I",
        "outputId": "6e52acb9-5395-4923-9bb8-c321ed8cebe6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/13 [00:00<?, ?it/s]From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
            "100%|██████████| 13/13 [00:55<00:00,  4.26s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "set_seed()\n",
        "# Since the teacher model is large, we will only evaluate on a small subset of the validation set.\n",
        "val_summaries_generated = summarize_with_teacher_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"][:100],\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twnBLJl8NX-I",
        "outputId": "510cef2f-c91d-4d0a-88cb-73b1dc70c87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article:\n",
            "('(CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he '\n",
            " 'is now \"feeling well\" after collapsing during Swansea\\'s 3-2 loss at '\n",
            " 'Tottenham in the Premier League on Wednesday. The worrying incident occurred '\n",
            " 'in the first half at White Hart Lane -- after Tottenham scored in the '\n",
            " 'seventh minute -- but the 29-year-old left the pitch conscious following '\n",
            " 'about five minutes of treatment. The Guardian added that he was wearing an '\n",
            " 'oxygen mask. Play was temporarily stopped before resuming. As the match '\n",
            " 'progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk '\n",
            " \"using the same word to describe Gomis' condition. Gomis spent the night in \"\n",
            " 'hospital as a precaution, Swansea said on its website. \"I wanted to reassure '\n",
            " 'you concerning my health,\" Gomis told the website. \"It actually looks much '\n",
            " 'scarier than it is physically dangerous, and I am feeling well now. \"I have '\n",
            " \"been under a great deal of stress and fatigue due to my father's health, \"\n",
            " 'which requires me to go back and forth from France. \"I was disappointed that '\n",
            " \"I couldn't help my team tonight, but now everything is back in order. I also \"\n",
            " 'want to thank everyone for their support and get well messages.\" Gomis had '\n",
            " 'similar fainting spells in France, which prompted the president of his '\n",
            " 'former club, Jean-Michel Aulas of Lyon, to tell French television in 2009: '\n",
            " '\"We can\\'t not be worried, it scares you each time.\" Swansea ran tests on '\n",
            " 'Gomis, said Monk, prior to signing him on a free transfer last July. \"He '\n",
            " 'just has a little bit of low blood pressure which causes you a little bit of '\n",
            " 'problems,\" Monk said in a televised interview on Sky. \"It\\'s been part of '\n",
            " \"his life. We were well aware of that when we signed him. He's done all the \"\n",
            " \"hospital checks and all the medical checks you can possibly do and it's just \"\n",
            " 'part of his life. \"It\\'s no problems whatsoever. It\\'s not as serious as it '\n",
            " 'looks.\" Gomis has scored two league goals for Swansea this season, mostly in '\n",
            " \"a backup role. He became the Welsh side's top striker when Wilfried Bony \"\n",
            " 'signed with Manchester City in January. Almost exactly three years ago at '\n",
            " 'White Hart Lane, then Bolton midfielder Fabrice Muamba collapsed after '\n",
            " 'suffering a cardiac arrest. He was near death,  according to Bolton, but '\n",
            " 'survived after being treated at the London Chest Hospital. He subsequently '\n",
            " 'retired. Other footballers, including Cameroon international Marc-Vivien Foe '\n",
            " \"in 2003 and Spanish international Antonio Puerta in 2007, didn't survive \"\n",
            " 'after collapsing on the pitch.')\n",
            "***********************\n",
            "\n",
            "\n",
            "Generated summary:\n",
            "('**TL;DR:** French striker Bafetimbi Gomis revealed that he had fainted '\n",
            " 'during a match between Swansea and Tottenham in the Premier League but was '\n",
            " 'feeling well after treatment and was hospitalized overnight. His previous '\n",
            " 'episodes have led some to express concern over his fitness. The 29-year-old '\n",
            " \"has been dealing with high levels of stress related to his family's \"\n",
            " 'situation. Despite his condition, Gomis expressed gratitude for the support '\n",
            " 'received and continued to play despite struggling with injury earlier in')\n",
            "***********************\n",
            "\n",
            "\n",
            "Gold summary:\n",
            "('Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\\n'\n",
            " 'But he reportedly left the pitch conscious and wearing an oxygen mask .\\n'\n",
            " 'Gomis later said that he was \"feeling well\"\\n'\n",
            " 'The incident came three years after Fabrice Muamba collapsed at White Hart '\n",
            " 'Lane .')\n"
          ]
        }
      ],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(val_summaries_generated[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRYcxU_hNX-I"
      },
      "source": [
        "You should see that the generated summary is much better than the one generated by the student model. Let's now evaluate the summaries generated by the teacher model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_DrFGGQNX-I",
        "outputId": "67d6712c-9d12-44da-f6f7-118adeeb2c92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': np.float64(0.27801197092342655),\n",
              " 'rouge2': np.float64(0.08732214740684438),\n",
              " 'rougeL': np.float64(0.1914977740287679),\n",
              " 'rougeLsum': np.float64(0.22632031848478518)}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set_seed()\n",
        "teacher_rouge = evaluate.load(\"rouge\")\n",
        "scores = teacher_rouge.compute(\n",
        "    predictions=val_summaries_generated,\n",
        "    references=cnn_dm_cse447_dataset[\"val\"][\"summary\"][:100],\n",
        "    use_stemmer=True\n",
        ")\n",
        "scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSwFkMnKNX-I"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.261\n",
        "- rouge2: 0.082\n",
        "- rougeL: 0.181\n",
        "- rougeLsum: 0.211\n",
        "\n",
        "Differences of +/- 0.02 in ROUGE scores are acceptable.\n",
        "\n",
        "\n",
        "While these are only on a small subset of the validation set, we also ran the teacher model on the entire validation set and got the following scores:\n",
        "- rouge1: 0.284\n",
        "- rouge2: 0.085\n",
        "- rougeL: 0.188\n",
        "- rougeLsum: 0.229"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zzB2p8DNX-I"
      },
      "source": [
        "As you can see, the teacher model performs much better than the student model. While the numbers are not super high, note that our teacher model is only 1.5B parameters, which is considered a tiny model in the current LLM landscape. Bigger models with 8B, 30B, 70B, or even higher are expected to perform even better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx1wz1VfNX-I"
      },
      "source": [
        "#### Generating Data for Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtsWttDcNX-I"
      },
      "source": [
        "We can now generate data for distillation. We will use the articles from the training dataset and generate summaries for those articles using the teacher model. Note that we do have the original summaries for the training set, but we are not using them for distillation. Our goal is to show you how distillation works and how you can use it for your own use cases where you might not have labelled training data available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fL5rmRYWNX-I"
      },
      "outputs": [],
      "source": [
        "def generate_synthetic_data_for_distillation(\n",
        "    articles,\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    num_samples=1000,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    articles = articles[:num_samples]\n",
        "    summaries = summarize_with_teacher_model(\n",
        "        articles,\n",
        "        teacher_model,\n",
        "        teacher_tokenizer,\n",
        "        batch_size=batch_size,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        p=p,\n",
        "        temperature=temperature,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnBT_de-NX-I",
        "outputId": "ce2849f2-6116-4fe2-99d3-0da8a3d243ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/125 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [09:04<00:00,  4.36s/it]\n"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "train_summaries_generated = generate_synthetic_data_for_distillation(\n",
        "    cnn_dm_cse447_dataset[\"train\"][\"article\"],\n",
        "    teacher_model,\n",
        "    teacher_tokenizer,\n",
        "    num_samples=1000,\n",
        "    batch_size=8,\n",
        "    max_new_tokens=100,\n",
        "    do_sample=True,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwZcQLcrNX-I",
        "outputId": "a25db275-0608-4335-bae1-3fa04b113f2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['article', 'summary', 'gold_summary'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kd_dataset = {\n",
        "    \"article\": cnn_dm_cse447_dataset[\"train\"][\"article\"][:1000],\n",
        "    \"summary\": train_summaries_generated,\n",
        "    \"gold_summary\": cnn_dm_cse447_dataset[\"train\"][\"summary\"][:1000]\n",
        "}\n",
        "kd_dataset = Dataset.from_dict(kd_dataset)\n",
        "kd_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5aac24d98b5240b081f343bd43212d7d",
            "068ef550ea1e428e955948c0e3d510ff",
            "858250ece8524f408dad2cf66aaa963f",
            "c476c984dcb746f09f155c75feb02a01",
            "70ceefda43d148d699a596640612b106",
            "9bff251202bb4c3a92ad490f42063fdc",
            "48d3f9b720464e1dae640b73c2a57449",
            "9637bbdedeae46d7b7e8798216412289",
            "a97be6479505400d827d5da57b6c0e25",
            "6d2408b056c6441b8de21c7ce3a064fa",
            "7af589e463b34f65a693b34b0f3ef452"
          ]
        },
        "id": "HWBxUvIcNX-I",
        "outputId": "6c825879-4871-4851-8f70-cf5d0bba1e06"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e2b7f502bee4d8088a0a81b0901937f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save the dataset to disk, so that you can load it later without re-generating the data.\n",
        "kd_dataset.save_to_disk(f\"{data_dir}/kd_dataset.hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo5q2dESNX-I"
      },
      "source": [
        "In case you are running into compute issues while generating data for distillation, we provide you with the distilled dataset in the `data` folder, which you can directly load using the `datasets` library.\n",
        "\n",
        "```python\n",
        "kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owkJR8zJNX-I"
      },
      "source": [
        "### 3. Fine-tuning student model with teacher model's generated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByU13tmNX-I"
      },
      "source": [
        "#### 3.1 Preparing data for distillation\n",
        "\n",
        "You will now implement the `prepare_data_for_distillation` function to prepare the data for distillation. This function will format the data in a specific way so that it can be used to fine-tune the student model. You will follow pretty much the same process as you did for the student model in `prepare_articles_for_student_model` with a few changes.\n",
        "\n",
        "- First we will include the summaries in the input text along with the articles. This is done because we are now training the student model to generate summaries from the articles. Hence the format of the input text will be `<article>\\nTL;DR:<summary>`.\n",
        "- In the tokenization dictionary, we now need to add a new key, `labels,` which contains the labels to train the language model. For language models, the labels are the same as the input IDs since the model is expected to generate the next word in the sequence. However, while fine-tuning, we want the model to learn how to generate the summaries from the articles and we do not care about the model learning to predict tokens in the original articles. Therefore, we replace the labels for the prompt tokens with -100, which is a special token id that is used to signal the loss function to ignore the loss for those tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "deletable": false,
        "id": "bj0xXkCWNX-I",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fa365ea8a33018ce8a35c8865f04dc61",
          "grade": false,
          "grade_id": "cell-b5ae7090ac4d3b51",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_distillation(article, summary, student_tokenizer, max_length=1024):\n",
        "    \"\"\"\n",
        "    Prepares the data for distillation.\n",
        "\n",
        "    Inputs:`\n",
        "    - article: The article to be summarized.\n",
        "    - summary: Summary to be used as labels for distillation.\n",
        "    - student_tokenizer: Tokenizer for the student model.\n",
        "    - max_length: Maximum length of the tokenized input.\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_data: Tokenized data in the format of a dictionary with keys `input_ids`, `attention_mask`, and `labels`.\n",
        "\n",
        "    Note: Unlike the functions preparing data for student and techer models before, this function only takes a single\n",
        "    article as input instead of a list of articles. We are doing this because while training we want to create batches\n",
        "    dynamically such that they are padded based on the longest article in the batch. Earlier functions were padding all\n",
        "    the articles with the same length. While it doesn't make any difference in the performance whether you use\n",
        "    dynamic padding or fixed padding, dynamic padding can be much more compute and memory efficient, which is especially\n",
        "    important for training\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Format the article and summary for distillation.\n",
        "    # Hint: `summary` might already contain the TL;DR: prefix, make sure you are not adding it twice!\n",
        "    article_wth_summary = None\n",
        "    article_prompt = None  # This should only contain the article and <article>\\nTL;DR: you will need this to set the labels for the prompt tokens to -100\n",
        "    # YOUR CODE HERE\n",
        "    article_wth_summary = f\"{article}\\nTL;DR: {summary}\"\n",
        "    article_prompt = f\"{article}\\nTL;DR:\"\n",
        "\n",
        "    # TODO: Tokenize `articles_wth_summaries`. Note set padding=False, since we want to do dyanmic padding during training.\n",
        "    tokenized_data = None\n",
        "    tokenized_prompt = None\n",
        "    # YOUR CODE HERE\n",
        "    tokenized_data = student_tokenizer(article_wth_summary, padding=False, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    tokenized_prompt = student_tokenizer(article_prompt, padding=False, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenizer adds a batch dimension to the input IDs and attention mask when the input is a single sequence.\n",
        "    # We need to remove the batch dimension since we are only processing a single article.\n",
        "    tokenized_data[\"input_ids\"], tokenized_data[\"attention_mask\"] = (\n",
        "        tokenized_data[\"input_ids\"][0],\n",
        "        tokenized_data[\"attention_mask\"][0],\n",
        "    )\n",
        "    tokenized_prompt[\"input_ids\"], tokenized_prompt[\"attention_mask\"] = (\n",
        "        tokenized_prompt[\"input_ids\"][0],\n",
        "        tokenized_prompt[\"attention_mask\"][0],\n",
        "    )\n",
        "\n",
        "    # Set the labels to the input IDs\n",
        "    tokenized_data[\"labels\"] = tokenized_data[\"input_ids\"].clone()\n",
        "\n",
        "    # TODO: Replace the labels for the prompt tokens with -100.\n",
        "    # Rember both labels and input_ids are pytorch tensors of shape (seq_len,)\n",
        "    # YOUR CODE HERE\n",
        "    tokenized_data[\"labels\"][: len(tokenized_prompt[\"input_ids\"])] = -100\n",
        "\n",
        "    return tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45NnwKXwNX-J",
        "outputId": "bc6da19c-fd53-4c7b-e539-14e2422b2f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed!\n"
          ]
        }
      ],
      "source": [
        "def test_prepare_data_for_distillation():\n",
        "    # Setup\n",
        "    student_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
        "    student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    article = \"This is a test article.\"\n",
        "    summary = \"This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    assert \"input_ids\" in output, \"Output should contain input_ids\"\n",
        "    assert \"attention_mask\" in output, \"Output should contain attention_mask\"\n",
        "    assert \"labels\" in output, \"Output should contain labels\"\n",
        "    assert torch.is_tensor(output[\"input_ids\"]), \"input_ids should be a tensor\"\n",
        "    assert torch.is_tensor(\n",
        "        output[\"attention_mask\"]\n",
        "    ), \"attention_mask should be a tensor\"\n",
        "    assert torch.is_tensor(output[\"labels\"]), \"labels should be a tensor\"\n",
        "\n",
        "    # Test 2: Correct label masking\n",
        "    article = \"This is a test article.\"\n",
        "    summary = \"This is a test summary.\"\n",
        "    output = prepare_data_for_distillation(article, summary, student_tokenizer)\n",
        "    input_ids = output[\"input_ids\"]\n",
        "    labels = output[\"labels\"]\n",
        "\n",
        "    # Ensure labels for the article part are -100\n",
        "    article_length = len(student_tokenizer(article + \"\\nTL;DR:\")[\"input_ids\"])\n",
        "    assert all(\n",
        "        label == -100 for label in labels[:article_length]\n",
        "    ), \"Labels for article tokens should be -100\"\n",
        "\n",
        "    # Ensure labels for the summary part are not -100\n",
        "    assert all(\n",
        "        label != -100 for label in labels[article_length:]\n",
        "    ), \"Labels for summary tokens should not be -100\"\n",
        "\n",
        "    # Test 3: Padding and truncation\n",
        "    long_article = \"This is a very \" * 1000  # Create a very long article\n",
        "    long_summary = \"Summary \" * 1000\n",
        "    short_article = \"Short.\"\n",
        "    short_summary = \"Short.\"\n",
        "    output = prepare_data_for_distillation(\n",
        "        long_article, long_summary, student_tokenizer, max_length=1024\n",
        "    )\n",
        "    assert (\n",
        "        output[\"input_ids\"].shape[0] == 1024\n",
        "    ), \"Long inputs should be truncated to max length\"\n",
        "\n",
        "    output = prepare_data_for_distillation(\n",
        "        short_article, short_summary, student_tokenizer, max_length=1024\n",
        "    )\n",
        "    assert (\n",
        "        output[\"attention_mask\"][1] != 0\n",
        "    ).all(), \"Short article should no have padding\"\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "\n",
        "test_prepare_data_for_distillation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc5Outj0NX-J"
      },
      "source": [
        "Let's now prepare the data for distillation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dpp8s4U_NX-J"
      },
      "outputs": [],
      "source": [
        "kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCX_NjPfNX-J",
        "outputId": "3151511d-f73c-4c42-ae41-2204fb7ea928"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f523df16f0fc40b691e29f6380d196d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_tokenized_data = kd_dataset.map(\n",
        "    lambda example: prepare_data_for_distillation(\n",
        "        example[\"article\"],\n",
        "        example[\"summary\"],\n",
        "        student_tokenizer,\n",
        "        max_length=1024,\n",
        "    ),\n",
        "    batched=False,\n",
        "    remove_columns=kd_dataset.column_names,\n",
        ")\n",
        "train_tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "e49edba82f7d45ddb69e47e6d1407a2a",
            "4206d5739b98468797a3552085c40a4f",
            "a5f846707ddd4926a966a48eeda6770d",
            "fad7d980afb84a1284d9b603954b32f0",
            "41ebe120d5b8491c886a394e31996d6b",
            "5ad8da4faaec430cb199f04cd8c402da",
            "a88b68e8531a49018ba727e49ebe077f",
            "14fecebca32e4132a5bdb4da792f3490",
            "46dba5afb6a440868585a12b4fef1e29",
            "262d325ce86b43a7a2016a12c2bbb4bb",
            "46aa76c6d92843fe9fb05de05d9477c6"
          ]
        },
        "id": "Bx3hrq2vNX-J",
        "outputId": "8fe27794-dfe6-4fac-a64c-8151850ab889"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d10b1252fa2e4d74b63df8a2a85a538a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_tokenized_data = cnn_dm_cse447_dataset[\"val\"].map(\n",
        "    lambda example: prepare_data_for_distillation(\n",
        "        example[\"article\"],\n",
        "        example[\"summary\"],\n",
        "        student_tokenizer,\n",
        "        max_length=1024,\n",
        "    ),\n",
        "    batched=False,\n",
        "    remove_columns=cnn_dm_cse447_dataset[\"val\"].column_names,\n",
        ")\n",
        "val_tokenized_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAz1NZ_4NX-J",
        "outputId": "b6b765ef-bde3-411f-b130-33a5d6f0b5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs:\n",
            "[43, 47383, 11, 4492, 357, 12637, 8, 1377, 5850, 14179, 3491, 7806, 5325, 33783, 8810, 1895, 284, 257, 2098, 4248, 1238, 1510, 7198, 3901, 13, 16, 1510, 8, 15807, 355, 339, 4962, 1248, 319, 3321, 11, 475, 339, 17424, 262, 1637, 1839, 470, 3350, 257, 4822, 319, 683, 13, 7806, 5325, 33783, 355, 5850, 14179, 287, 366, 18308, 14179, 290, 262, 8284, 286, 262, 9643, 1, 1675, 262, 18641, 286, 30914, 5721, 1023, 1088, 262, 995, 11, 262, 1862, 8674, 1139, 339, 468, 645, 3352, 284, 277, 799, 353, 465, 5003, 1497, 319, 3049, 5006, 11, 4144, 290, 16527, 4671, 13, 366, 40, 836, 470, 1410, 284, 307, 530, 286, 883, 661, 508, 11, 355, 2582, 355, 484, 1210, 1248, 11, 6451, 2822, 2405, 257, 4858, 5701, 1097, 4947, 393, 1223, 2092, 553, 339, 1297, 281, 6638, 39877, 2961, 428, 1227, 13, 366, 40, 836, 470, 892, 314, 1183, 307, 3573, 45997, 13, 366, 464, 1243, 314, 588, 7067, 389, 1243, 326, 1575, 546, 838, 8059, 1377, 3835, 290, 36731, 290, 35962, 526, 1629, 1248, 11, 5325, 33783, 481, 307, 1498, 284, 32385, 287, 257, 21507, 11, 2822, 257, 4144, 287, 257, 2240, 393, 766, 262, 9961, 2646, 366, 17932, 417, 25, 2142, 2873, 553, 3058, 2237, 4113, 2174, 465, 1271, 530, 3807, 319, 262, 3482, 3091, 2607, 8262, 13, 14890, 286, 703, 339, 1183, 1317, 465, 20533, 10955, 389, 739, 27521, 13, 2399, 5797, 290, 1171, 396, 550, 645, 2912, 319, 465, 3352, 13, 366, 40, 1183, 4753, 423, 617, 3297, 286, 2151, 553, 339, 531, 287, 281, 2720, 13, 366, 32365, 4844, 286, 345, 481, 307, 3555, 546, 340, 526, 5325, 33783, 338, 12042, 422, 262, 717, 1936, 14179, 7328, 423, 587, 2714, 287, 257, 3774, 1814, 543, 339, 468, 407, 587, 1498, 284, 3638, 13, 7945, 465, 3957, 16117, 290, 35000, 11, 262, 8674, 1139, 339, 318, 5291, 465, 3625, 14245, 319, 262, 2323, 13, 366, 8061, 389, 1464, 2045, 284, 910, 705, 38439, 3491, 2925, 572, 262, 27587, 16078, 339, 1297, 7638, 938, 1227, 13, 366, 1537, 314, 1949, 845, 1327, 407, 284, 467, 326, 835, 780, 340, 561, 307, 1165, 2562, 329, 606, 526, 2399, 3452, 32922, 355, 262, 2933, 18731, 287, 366, 18308, 14179, 290, 262, 8284, 286, 262, 9643, 1, 318, 7163, 4406, 319, 1111, 5389, 286, 262, 10596, 290, 339, 481, 302, 7919, 262, 2597, 287, 262, 938, 734, 7328, 13, 220, 6305, 314, 12, 6207, 4337, 1577, 607, 2423, 286, 14179, 338, 3452, 13355, 764, 1318, 318, 1204, 3675, 14179, 11, 2158, 13, 383, 3576, 263, 468, 18976, 257, 3195, 3807, 1444, 366, 3666, 6387, 3619, 553, 546, 1772, 17421, 9413, 21927, 11347, 290, 465, 3367, 11, 2233, 329, 2650, 1568, 428, 614, 13, 679, 481, 635, 1656, 287, 366, 20588, 17528, 553, 281, 6638, 2646, 546, 1440, 6510, 508, 6654, 281, 26051, 496, 13, 20635, 428, 614, 11, 339, 925, 465, 3800, 8886, 2712, 257, 22485, 15287, 287, 5613, 911, 31183, 338, 366, 23588, 385, 526, 11214, 11, 339, 318, 865, 2286, 329, 772, 5699, 2056, 14521, 783, 326, 339, 338, 11119, 281, 4044, 25, 366, 40, 655, 892, 314, 1101, 1016, 284, 307, 517, 3297, 286, 3148, 983, 553, 339, 1297, 8428, 13, 412, 12, 4529, 284, 257, 1545, 764, 15069, 4343, 8428, 13, 1439, 2489, 10395, 13, 1212, 2587, 743, 407, 307, 3199, 11, 7025, 11, 30101, 11, 393, 38913, 13, 198, 14990, 26, 7707, 25, 24811, 26, 7707, 25, 7806, 5325, 33783, 4962, 1248, 319, 2795, 1542, 400, 290, 981, 339, 468, 7478, 39025, 257, 15807, 6108, 379, 4248, 1238, 76, 11, 339, 318, 5410, 284, 4341, 465, 40000, 5129, 25220, 1473, 13, 679, 16047, 326, 339, 481, 691, 4328, 333, 469, 319, 3835, 11, 36731, 11, 290, 35962, 2138, 621, 13064, 3709, 884, 355, 3049, 5006, 393, 5789, 5548, 13, 2399, 7865, 9176, 287, 5850, 14179, 7906, 12, 8210, 11, 355, 880, 355, 465, 10270, 287, 584, 9739, 4493, 1390, 5581, 290, 2646, 11, 1950, 326, 612, 318, 6088, 286]\n",
            "Labels:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 24811, 26, 7707, 25, 7806, 5325, 33783, 4962, 1248, 319, 2795, 1542, 400, 290, 981, 339, 468, 7478, 39025, 257, 15807, 6108, 379, 4248, 1238, 76, 11, 339, 318, 5410, 284, 4341, 465, 40000, 5129, 25220, 1473, 13, 679, 16047, 326, 339, 481, 691, 4328, 333, 469, 319, 3835, 11, 36731, 11, 290, 35962, 2138, 621, 13064, 3709, 884, 355, 3049, 5006, 393, 5789, 5548, 13, 2399, 7865, 9176, 287, 5850, 14179, 7906, 12, 8210, 11, 355, 880, 355, 465, 10270, 287, 584, 9739, 4493, 1390, 5581, 290, 2646, 11, 1950, 326, 612, 318, 6088, 286]\n",
            "***********************\n",
            "\n",
            "\n",
            "Input Text:\n",
            "('LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access '\n",
            " 'to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, '\n",
            " \"but he insists the money won't cast a spell on him. Daniel Radcliffe as \"\n",
            " 'Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the '\n",
            " 'disappointment of gossip columnists around the world, the young actor says '\n",
            " 'he has no plans to fritter his cash away on fast cars, drink and celebrity '\n",
            " 'parties. \"I don\\'t plan to be one of those people who, as soon as they turn '\n",
            " '18, suddenly buy themselves a massive sports car collection or something '\n",
            " 'similar,\" he told an Australian interviewer earlier this month. \"I don\\'t '\n",
            " 'think I\\'ll be particularly extravagant. \"The things I like buying are '\n",
            " 'things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, '\n",
            " 'Radcliffe will be able to gamble in a casino, buy a drink in a pub or see '\n",
            " 'the horror film \"Hostel: Part II,\" currently six places below his number one '\n",
            " \"movie on the UK box office chart. Details of how he'll mark his landmark \"\n",
            " 'birthday are under wraps. His agent and publicist had no comment on his '\n",
            " 'plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. '\n",
            " '\"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from '\n",
            " 'the first five Potter films have been held in a trust fund which he has not '\n",
            " 'been able to touch. Despite his growing fame and riches, the actor says he '\n",
            " 'is keeping his feet firmly on the ground. \"People are always looking to say '\n",
            " '\\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try '\n",
            " 'very hard not to go that way because it would be too easy for them.\" His '\n",
            " 'latest outing as the boy wizard in \"Harry Potter and the Order of the '\n",
            " 'Phoenix\" is breaking records on both sides of the Atlantic and he will '\n",
            " 'reprise the role in the last two films.  Watch I-Reporter give her review of '\n",
            " \"Potter's latest » . There is life beyond Potter, however. The Londoner has \"\n",
            " 'filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his '\n",
            " 'son, due for release later this year. He will also appear in \"December '\n",
            " 'Boys,\" an Australian film about four boys who escape an orphanage. Earlier '\n",
            " 'this year, he made his stage debut playing a tortured teenager in Peter '\n",
            " 'Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny '\n",
            " 'now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of '\n",
            " 'fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. '\n",
            " 'All rights reserved.This material may not be published, broadcast, '\n",
            " 'rewritten, or redistributed.\\n'\n",
            " 'TL;DR: TL;DR: Daniel Radcliffe turns 18 on June 30th and while he has '\n",
            " 'reportedly amassed a fortune estimated at £20m, he is planning to spend his '\n",
            " 'newfound wealth prudently. He maintains that he will only splurge on books, '\n",
            " 'CDs, and DVDs rather than luxury items such as fast cars or expensive '\n",
            " 'alcohol. His upcoming roles in Harry Potter spin-offs, as well as his '\n",
            " 'participation in other entertainment projects including television and film, '\n",
            " 'suggest that there is plenty of')\n"
          ]
        }
      ],
      "source": [
        "# Let's have a look at the tokenized data\n",
        "tokenized_data = train_tokenized_data[0]\n",
        "print(\"Input IDs:\")\n",
        "print(tokenized_data[\"input_ids\"])\n",
        "print(\"Labels:\")\n",
        "print(tokenized_data[\"labels\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "\n",
        "print(\"Input Text:\")\n",
        "pprint(student_tokenizer.decode(tokenized_data[\"input_ids\"], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IENDBQN3NX-J"
      },
      "source": [
        "### Fine-tuning the student model\n",
        "\n",
        "We are now ready to fine-tune the student model. While this might sound like a daunting task, we actually need not write much code for this. Transformers library has a `Trainer` class that takes care of the fine-tuning process. We will use the `TrainingArguments` class to set the training arguments and the `Trainer` class to fine-tune the model. Below is the code to fine-tune the student model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "V1qTarNvNX-J"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "nO8Qc_UiNX-J",
        "outputId": "c2ca480f-956f-4e73-8ed0-187d2cb2165a"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[64], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m set_seed()\n\u001b[1;32m     84\u001b[0m student_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[43mfine_tune_student_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_tokenized_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_tokenized_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[64], line 46\u001b[0m, in \u001b[0;36mfine_tune_student_model\u001b[0;34m(student_model, train_tokenized_data, val_tokenized_data, student_tokenizer, output_dir, logging_steps, eval_steps, logging_first_step, learning_rate, warmup_steps, per_device_train_batch_size, per_device_eval_batch_size, gradient_accumulation_steps, num_train_epochs, weight_decay, fp16, seed)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mFine-tunes the student (language) model.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m- seed: The random seed to use for training. This is used to ensure reproducibility of the training process.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Set training arguments.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# This involve things like learning rate, batch size, number of epochs, etc.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_first_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Create a data collator for dynamic padding\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# A data collator is used to construct batches of data from the dataset.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\n\u001b[1;32m     67\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mstudent_tokenizer, model\u001b[38;5;241m=\u001b[39mstudent_model, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m )\n",
            "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:1780\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:2306\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2305\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:2179\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2181\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2182\u001b[0m         )\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
        "import accelerate\n",
        "\n",
        "def fine_tune_student_model(\n",
        "    student_model,\n",
        "    train_tokenized_data,\n",
        "    val_tokenized_data,\n",
        "    student_tokenizer,\n",
        "    output_dir=\"checkpoints/\",\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    logging_first_step=True,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        "):\n",
        "\n",
        "    \"\"\"\n",
        "    Fine-tunes the student (language) model.\n",
        "\n",
        "    Inputs:\n",
        "    - student_model: The student (language) model to be fine-tuned.\n",
        "    - train_tokenized_data: The tokenized training data.\n",
        "    - val_tokenized_data: The tokenized validation data.\n",
        "    - student_tokenizer: The tokenizer for the student model.\n",
        "    - output_dir: The directory to save the fine-tuned model.\n",
        "    - learning_rate: The learning rate for the optimizer.\n",
        "    - warmup_steps: The number of steps to warm up the learning rate. This is used to gradually increase the learning rate during training.\n",
        "    - per_device_train_batch_size: The batch size for training per device.\n",
        "    - per_device_eval_batch_size: The batch size for evaluation per device.\n",
        "    - gradient_accumulation_steps: The number of steps to accumulate gradients before updating the model weights. # This is a trick to use a larger effective batch size when low on memory. Basically, we accumulate gradients for `gradient_accumulation_steps` steps before updating the model weights using SGD, which is equivalent to using a batch size of `gradient_accumulation_steps * per_device_train_batch_size` times the per_device_train_batch_size.\n",
        "    - num_train_epochs: The number of training epochs.\n",
        "    - weight_decay: The weight decay for the optimizer. This is a regularization technique to prevent overfitting.\n",
        "    - fp16: Whether to use 16-bit precision. This is used to speed up the training process and reduce the memory usage.\n",
        "    - seed: The random seed to use for training. This is used to ensure reproducibility of the training process.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set training arguments.\n",
        "    # This involve things like learning rate, batch size, number of epochs, etc.\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"checkpoints/\",\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        logging_first_step=True,\n",
        "        learning_rate=learning_rate,\n",
        "        warmup_steps=warmup_steps,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=weight_decay,\n",
        "        fp16=fp16,\n",
        "        seed=seed,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Create a data collator for dynamic padding\n",
        "    # A data collator is used to construct batches of data from the dataset.\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=student_tokenizer, model=student_model, padding=\"longest\"\n",
        "    )\n",
        "\n",
        "    # Create a trainer\n",
        "    trainer = Trainer(\n",
        "        model=student_model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_tokenized_data,\n",
        "        eval_dataset=val_tokenized_data,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "set_seed()\n",
        "student_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "fine_tune_student_model(\n",
        "    student_model,\n",
        "    train_tokenized_data,\n",
        "    val_tokenized_data,\n",
        "    student_tokenizer,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=100,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iifom3OcNX-J"
      },
      "source": [
        "The reference training and validation losses are 2.61 and 3.01 respectively. Differences of +/- 0.2 are acceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7VAEe4PNX-J"
      },
      "source": [
        "Let's now generate summaries with the fine-tuned student model and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T-aPSVnNX-J",
        "outputId": "1bf1e32b-d417-4892-c091-a1369d21e0d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/125 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/home/arunasrivastava/AttentionGate/env/lib/python3.12/site-packages/transformers/generation/utils.py:2134: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/125 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m set_seed()\n\u001b[1;32m      2\u001b[0m student_tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#Padding side left for generation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pred_summaries_ft_student \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_wth_student_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnn_dm_cse447_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[30], line 47\u001b[0m, in \u001b[0;36msummarize_wth_student_model\u001b[0;34m(articles, student_model, student_tokenizer, batch_size, max_new_tokens, do_sample, p, temperature, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m generated_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m generated_summaries \u001b[38;5;241m=\u001b[39m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenized_articles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m  \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add this line\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m  \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add this line\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# ToDo: Convert the generated summaries to text. Hint: Use the `batch_decode` method of the tokenizer.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Make sure to only decode the generated tokens, i.e., ignore the input tokens.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Also, make sure to set `skip_special_tokens=True` to ignore the special tokens.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m decoded_summaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/generation/utils.py:2252\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2245\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2246\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2247\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2249\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2252\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2265\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2266\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2272\u001b[0m     )\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/generation/utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1272\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1272\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1030\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1030\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m   1032\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
          ]
        }
      ],
      "source": [
        "set_seed()\n",
        "student_tokenizer.padding_side = \"left\" #Padding side left for generation\n",
        "pred_summaries_ft_student = summarize_wth_student_model(\n",
        "    cnn_dm_cse447_dataset[\"val\"][\"article\"],\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    p=0.9,\n",
        "    temperature=1.0,\n",
        "    device=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZAH9iy9NX-J",
        "outputId": "889a5664-b683-4354-85b4-2eee79948c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article:\n",
            "('(CNN)French striker Bafetimbi Gomis, who has a history of fainting, said he '\n",
            " 'is now \"feeling well\" after collapsing during Swansea\\'s 3-2 loss at '\n",
            " 'Tottenham in the Premier League on Wednesday. The worrying incident occurred '\n",
            " 'in the first half at White Hart Lane -- after Tottenham scored in the '\n",
            " 'seventh minute -- but the 29-year-old left the pitch conscious following '\n",
            " 'about five minutes of treatment. The Guardian added that he was wearing an '\n",
            " 'oxygen mask. Play was temporarily stopped before resuming. As the match '\n",
            " 'progressed, Swansea tweeted that Gomis was \"fine,\" with manager Garry Monk '\n",
            " \"using the same word to describe Gomis' condition. Gomis spent the night in \"\n",
            " 'hospital as a precaution, Swansea said on its website. \"I wanted to reassure '\n",
            " 'you concerning my health,\" Gomis told the website. \"It actually looks much '\n",
            " 'scarier than it is physically dangerous, and I am feeling well now. \"I have '\n",
            " \"been under a great deal of stress and fatigue due to my father's health, \"\n",
            " 'which requires me to go back and forth from France. \"I was disappointed that '\n",
            " \"I couldn't help my team tonight, but now everything is back in order. I also \"\n",
            " 'want to thank everyone for their support and get well messages.\" Gomis had '\n",
            " 'similar fainting spells in France, which prompted the president of his '\n",
            " 'former club, Jean-Michel Aulas of Lyon, to tell French television in 2009: '\n",
            " '\"We can\\'t not be worried, it scares you each time.\" Swansea ran tests on '\n",
            " 'Gomis, said Monk, prior to signing him on a free transfer last July. \"He '\n",
            " 'just has a little bit of low blood pressure which causes you a little bit of '\n",
            " 'problems,\" Monk said in a televised interview on Sky. \"It\\'s been part of '\n",
            " \"his life. We were well aware of that when we signed him. He's done all the \"\n",
            " \"hospital checks and all the medical checks you can possibly do and it's just \"\n",
            " 'part of his life. \"It\\'s no problems whatsoever. It\\'s not as serious as it '\n",
            " 'looks.\" Gomis has scored two league goals for Swansea this season, mostly in '\n",
            " \"a backup role. He became the Welsh side's top striker when Wilfried Bony \"\n",
            " 'signed with Manchester City in January. Almost exactly three years ago at '\n",
            " 'White Hart Lane, then Bolton midfielder Fabrice Muamba collapsed after '\n",
            " 'suffering a cardiac arrest. He was near death,  according to Bolton, but '\n",
            " 'survived after being treated at the London Chest Hospital. He subsequently '\n",
            " 'retired. Other footballers, including Cameroon international Marc-Vivien Foe '\n",
            " \"in 2003 and Spanish international Antonio Puerta in 2007, didn't survive \"\n",
            " 'after collapsing on the pitch.')\n",
            "***********************\n",
            "\n",
            "\n",
            "Generated summary:\n",
            "(' TL;DR: French striker Bafetimbi Gomis has suffered a catastrophic brain '\n",
            " \"injury during Swansea's 3-2 loss at Tottenham. The 19-year-old striker \"\n",
            " 'experienced severe dehydration during the match and went into cardiac arrest '\n",
            " 'due to his severe fatigue and pain. His medical team initially assessed him '\n",
            " 'as stable enough for the match to go ahead without further treatment. '\n",
            " 'However, due to persistent medical issues, the referee stopped playing at '\n",
            " 'White Hart Lane due to the need to provide oxygen. He')\n",
            "***********************\n",
            "\n",
            "\n",
            "Gold summary:\n",
            "('Bafetimbi Gomis collapses within 10 minutes of kickoff at Tottenham .\\n'\n",
            " 'But he reportedly left the pitch conscious and wearing an oxygen mask .\\n'\n",
            " 'Gomis later said that he was \"feeling well\"\\n'\n",
            " 'The incident came three years after Fabrice Muamba collapsed at White Hart '\n",
            " 'Lane .')\n"
          ]
        }
      ],
      "source": [
        "# Inspect the summaries\n",
        "print(\"Article:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"article\"])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Generated summary:\")\n",
        "pprint(pred_summaries_ft_student[0])\n",
        "print(\"***********************\\n\\n\")\n",
        "print(\"Gold summary:\")\n",
        "pprint(cnn_dm_cse447_dataset[\"val\"][0][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRXS1wP9NX-J"
      },
      "source": [
        "While not perfect, the summaries generated by the fine-tuned student model are much better than the ones generated by the student model before fine-tuning. Let's now evaluate the performance of the fine-tuned student model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uZbThz4NX-J",
        "outputId": "604407ba-7bc5-46a5-e041-bf77fcd4e80b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': 0.22421165598839,\n",
              " 'rouge2': 0.05214754077970379,\n",
              " 'rougeL': 0.1423070127293639,\n",
              " 'rougeLsum': 0.17934854885629864}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "teacher_rouge = evaluate.load(\"rouge\")\n",
        "scores = teacher_rouge.compute(\n",
        "    predictions=pred_summaries_ft_student,\n",
        "    references=cnn_dm_cse447_dataset[\"val\"][\"summary\"],\n",
        "    use_stemmer=True,\n",
        ")\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEbjaHAbNX-J"
      },
      "source": [
        "You should see the following scores:\n",
        "- rouge1: 0.230\n",
        "- rouge2: 0.055\n",
        "- rougeL: 0.145\n",
        "- rougeLsum: 0.185\n",
        "\n",
        "Differences of +/- 0.02 in ROUGE scores are acceptable.\n",
        "\n",
        "As you can see, the fine-tuned student model performs much better than the student model before fine-tuning. This is impressive because we used very small teacher and student models and only used 1000 examples for distillation.\n",
        "\n",
        "In practice, you would want to use a much bigger teacher model and a bigger student model to get better performance. The purpose of this exercise is to give you an idea of how distillation works and how you can use it for your own use cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mNvBnJvc2UES"
      },
      "outputs": [],
      "source": [
        "# clear cache with this command\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39aa5144d6914a65b522f3897961815e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def prepare_kd_data(examples, tokenizer, max_len=1024):\n",
        "    # Combine article and teacher summary as input\n",
        "    inputs = [article + \"\\nTL;DR:\" + summary for article, summary in zip(examples['article'], examples['summary'])]\n",
        "    # Tokenize the inputs\n",
        "    tokenized_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the function to the KD dataset\n",
        "tokenized_kd_dataset = kd_dataset.map(prepare_kd_data, batched=True, fn_kwargs={'tokenizer': student_tokenizer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[84], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Output directory\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size per device\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Batch size for evaluation\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Number of training epochs\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Directory for storing logs\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Learning rate\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Weight decay\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# Enable mixed precision training\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define the Trainer\u001b[39;00m\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mstudent_model,                \u001b[38;5;66;03m# The model to train\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_kd_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Training dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_kd_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m],    \u001b[38;5;66;03m# Evaluation dataset\u001b[39;00m\n\u001b[1;32m     26\u001b[0m )\n",
            "File \u001b[0;32m<string>:134\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:1780\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:2306\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2305\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/utils/generic.py:60\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
            "File \u001b[0;32m~/AttentionGate/env/lib/python3.12/site-packages/transformers/training_args.py:2179\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2181\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{ACCELERATE_MIN_VERSION}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2182\u001b[0m         )\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2184\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Output directory\n",
        "    per_device_train_batch_size=8,  # Batch size per device\n",
        "    per_device_eval_batch_size=8,   # Batch size for evaluation\n",
        "    num_train_epochs=3,              # Number of training epochs\n",
        "    logging_dir=\"./logs\",            # Directory for storing logs\n",
        "    learning_rate=2e-5,             # Learning rate\n",
        "    weight_decay=0.01,              # Weight decay\n",
        "    fp16=True,                       # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=student_model,                # The model to train\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=tokenized_kd_dataset[\"train\"],  # Training dataset\n",
        "    eval_dataset=tokenized_kd_dataset[\"val\"],    # Evaluation dataset\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./kd_fine_tuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "8VptDmNYl2LI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
        "from datasets import load_dataset, Dataset as HFDataset, load_from_disk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from rouge_score import rouge_scorer\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SummaryDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for handling summary data\"\"\"\n",
        "    def __init__(self, articles, summaries, tokenizer, max_length=1024):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Set padding token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "        \n",
        "        # Create prompt\n",
        "        prompt = f\"Summarize the following article:\\n\\nArticle: {article}\\n\\nSummary:\"\n",
        "        target = f\"{summary}</s>\"\n",
        "        \n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        labels = self.tokenizer(\n",
        "            target,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": inputs.input_ids.squeeze(),\n",
        "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
        "            \"labels\": labels.input_ids.squeeze()\n",
        "        }\n",
        "\n",
        "def prepare_datasets(cnn_dm_cse447_dataset, data_dir, student_tokenizer):\n",
        "    \"\"\"Prepare both KD and human datasets\"\"\"\n",
        "    # Load KD dataset\n",
        "    try:\n",
        "        kd_dataset = load_from_disk(f\"{data_dir}/kd_dataset.hf\")\n",
        "        kd_articles = kd_dataset[\"article\"]\n",
        "        kd_summaries = kd_dataset[\"summary\"]\n",
        "    except:\n",
        "        print(\"Could not load pre-saved KD dataset. Please ensure the dataset file exists.\")\n",
        "        return None, None\n",
        "    \n",
        "    # Get human dataset\n",
        "    human_articles = cnn_dm_cse447_dataset[\"train\"][\"article\"][:1000]\n",
        "    human_summaries = cnn_dm_cse447_dataset[\"train\"][\"summary\"][:1000]\n",
        "    \n",
        "    # Create dataset objects\n",
        "    kd_dataset = SummaryDataset(kd_articles, kd_summaries, student_tokenizer)\n",
        "    human_dataset = SummaryDataset(human_articles, human_summaries, student_tokenizer)\n",
        "    \n",
        "    return kd_dataset, human_dataset\n",
        "\n",
        "def fine_tune_student_model(model, train_dataset, learning_rate=2e-5, epochs=3, batch_size=4):\n",
        "    \"\"\"Fine-tune the student model\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    \n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "        \n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            \n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "        \n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch + 1} average loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_data, scorer):\n",
        "    \"\"\"Evaluate model using ROUGE scores\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Configure tokenizer\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    \n",
        "    # Make sure we have distinct padding token\n",
        "    original_eos = tokenizer.eos_token\n",
        "    tokenizer.eos_token = '</s>'\n",
        "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids('</s>')\n",
        "    \n",
        "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "    \n",
        "    for example in tqdm(test_data):\n",
        "        article = example[\"article\"]\n",
        "        reference_summary = example[\"gold_summary\"] if \"gold_summary\" in example else example[\"summary\"]\n",
        "        \n",
        "        prompt = f\"Summarize the following article:\\n\\nArticle: {article}\\n\\nSummary:\"\n",
        "        \n",
        "        # Tokenize with explicit padding and attention mask\n",
        "        tokenized_input = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=1024,\n",
        "            padding=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        \n",
        "        input_ids = tokenized_input.input_ids.to(device)\n",
        "        attention_mask = tokenized_input.attention_mask.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=150,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                bos_token_id=tokenizer.bos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3,  # Prevent repetitive text\n",
        "                num_beams=4,  # Use beam search for better quality\n",
        "                length_penalty=2.0  # Encourage slightly longer summaries\n",
        "            )\n",
        "        \n",
        "        generated_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Remove the input prompt from the generated text if it appears\n",
        "        if \"Summary:\" in generated_summary:\n",
        "            generated_summary = generated_summary.split(\"Summary:\")[-1].strip()\n",
        "        \n",
        "        scores = scorer.score(reference_summary, generated_summary)\n",
        "        \n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key].append(scores[key].fmeasure)\n",
        "    \n",
        "    # Restore original eos token\n",
        "    tokenizer.eos_token = original_eos\n",
        "    tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(original_eos)\n",
        "    \n",
        "    return {k: np.mean(v) for k, v in rouge_scores.items()}\n",
        "\n",
        "\n",
        "def main(cnn_dm_cse447_dataset, data_dir, student_model, student_tokenizer, teacher_model, teacher_tokenizer):\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Initialize ROUGE scorer\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    # Prepare datasets\n",
        "    kd_dataset, human_dataset = prepare_datasets(\n",
        "        cnn_dm_cse447_dataset,\n",
        "        data_dir,\n",
        "        student_tokenizer\n",
        "    )\n",
        "    \n",
        "    if kd_dataset is None:\n",
        "        return None\n",
        "    \n",
        "    # Get test data\n",
        "    test_data = cnn_dm_cse447_dataset[\"test\"].select(range(100))\n",
        "    \n",
        "    # Initialize models\n",
        "    base_model = student_model\n",
        "    kd_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    human_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    \n",
        "    # Train models\n",
        "    print(\"Training KD model...\")\n",
        "    kd_trained = fine_tune_student_model(kd_model, kd_dataset)\n",
        "    print(\"Training human data model...\")\n",
        "    human_trained = fine_tune_student_model(human_model, human_dataset)\n",
        "    \n",
        "    # Evaluate all models\n",
        "    scores = {\n",
        "        \"GPT-2 (no fine-tuning)\": evaluate_model(base_model, student_tokenizer, test_data, scorer),\n",
        "        \"GPT-2 (KD fine-tuning)\": evaluate_model(kd_trained, student_tokenizer, test_data, scorer),\n",
        "        \"GPT-2 (real-data fine-tuning)\": evaluate_model(human_trained, student_tokenizer, test_data, scorer),\n",
        "        \"Qwen 1.5B-instruct\": evaluate_model(teacher_model, teacher_tokenizer, test_data, scorer)\n",
        "    }\n",
        "    # Plot results\n",
        "    plot_rouge_scores(scores)\n",
        "    plt.savefig('rouge_scores_comparison.png')\n",
        "    plt.close()\n",
        "    \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rouge_scores(scores_dict):\n",
        "    \"\"\"\n",
        "    Plot ROUGE scores comparison for different models.\n",
        "    \n",
        "    Args:\n",
        "        scores_dict (dict): Dictionary with model names as keys and their ROUGE scores as values\n",
        "                           Example format: {\n",
        "                               'model_name': {\n",
        "                                   'rouge1': score,\n",
        "                                   'rouge2': score,\n",
        "                                   'rougeL': score\n",
        "                               }\n",
        "                           }\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: The generated plot\n",
        "    \"\"\"\n",
        "    models = list(scores_dict.keys())\n",
        "    rouge_types = list(scores_dict[models[0]].keys())\n",
        "    \n",
        "    x = np.arange(len(models))  # Label locations\n",
        "    width = 0.25  # Width of the bars\n",
        "    \n",
        "    # Create figure and axis with larger size\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    # Plot bars for each ROUGE metric\n",
        "    for i, rouge_type in enumerate(rouge_types):\n",
        "        scores = [scores_dict[model][rouge_type] for model in models]\n",
        "        ax.bar(x + i * width, scores, width, label=rouge_type)\n",
        "    \n",
        "    # Customize the plot\n",
        "    ax.set_ylabel('ROUGE Score')\n",
        "    ax.set_title('ROUGE Scores Comparison Across Models')\n",
        "    ax.set_xticks(x + width)\n",
        "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax.legend()\n",
        "    \n",
        "    # Add grid for better readability\n",
        "    ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "    \n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "15fcaad9131b48f89ef3920b693785be",
            "99e45f44024c498a93a9ef3d6fc31cff",
            "a1e88dd9d9924baaaf0a53e8bbafa828",
            "8ce0dcf3fca24b4ca42bb511c40873a8",
            "325eb7eab03f4248895f6d8b0afa652e",
            "cb0cffe4c4fe484188325cbf9b42fc69",
            "67883a0d301e4a74b555364ecbc57abd",
            "12b5c3f3c5be4d3caf33bd7a2639816d",
            "55fd544ade8c4003a29e9efdd2a72431",
            "368229fe185246ce94c73656feb4a394",
            "4e9426074f5b44968e990d7d9f70e1a2"
          ]
        },
        "id": "ylvb1mY267Oj",
        "outputId": "88ce45d0-e3b4-46ad-99a6-f4ddf5baa128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training KD model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.472]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 average loss: 0.6670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.516]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 average loss: 0.4819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.379]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 average loss: 0.4570\n",
            "Training human data model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.568]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 average loss: 0.6103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.427]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 average loss: 0.4274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 250/250 [01:06<00:00,  3.74it/s, loss=0.383]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 average loss: 0.4038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [03:03<00:00,  1.84s/it]\n",
            "100%|██████████| 100/100 [02:15<00:00,  1.35s/it]\n",
            "100%|██████████| 100/100 [02:52<00:00,  1.73s/it]\n",
            "100%|██████████| 100/100 [09:11<00:00,  5.51s/it]\n"
          ]
        }
      ],
      "source": [
        "scores = main(\n",
        "    cnn_dm_cse447_dataset,\n",
        "    data_dir,\n",
        "    student_model,\n",
        "    student_tokenizer,\n",
        "    teacher_model,\n",
        "    teacher_tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zDFRGO7d7vG9"
      },
      "outputs": [],
      "source": [
        "\n",
        "    # Plot results\n",
        "plot_rouge_scores(scores)\n",
        "plt.savefig('rouge_scores_comparison.png')\n",
        "plt.close()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00cf2f5a574140d4a5111aebba111127": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f4f85404014b62be66d1426829080e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06478dbf7f2f4234a2ccb93ef900e43a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "068ef550ea1e428e955948c0e3d510ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bff251202bb4c3a92ad490f42063fdc",
            "placeholder": "​",
            "style": "IPY_MODEL_48d3f9b720464e1dae640b73c2a57449",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "07ad5fe1ee4e4005a016268689c23f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08b60c893e954991ae0bf64a4fd04e18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09cf1022197040ecb24d8b78e541d686": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08ed45d57284684add54ee75dcc25d3",
            "placeholder": "​",
            "style": "IPY_MODEL_8cac2c9087ea4030b950957dcebbb775",
            "value": "100%"
          }
        },
        "0a39d44d81ab41ac9104d2a787ad1d10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dea714846de44ffdaceb66bdf6c4cd50",
              "IPY_MODEL_522ff7c612654580a7bbef585f8796dc",
              "IPY_MODEL_cd56ce6f651a42afbf7227d8de08d83d"
            ],
            "layout": "IPY_MODEL_04f4f85404014b62be66d1426829080e"
          }
        },
        "0a9d9a96f21a481a8853f9654e9887f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f931d45a0fd4d67ac212fe57ba4c77a",
            "placeholder": "​",
            "style": "IPY_MODEL_edf78046ee2c4d99917942916de3a3ac",
            "value": " 13/13 [00:01&lt;00:00,  8.22it/s]"
          }
        },
        "0d222aa7cc7042879d009ac7bc99b145": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1212dd3253fb49edb5e362f6b797ec29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d85fe4f1a46b4c329b20d931ade92360",
            "placeholder": "​",
            "style": "IPY_MODEL_769ed1beb70e4732a16d71d4503336b0",
            "value": " 10/10 [00:10&lt;00:00,  1.01it/s]"
          }
        },
        "12b5c3f3c5be4d3caf33bd7a2639816d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c823f863c544d5b24dfa2674486ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f628e1abbfdc4aef959c080a53d9f2db",
              "IPY_MODEL_ad84bbb2a4134a999a650e2063289728",
              "IPY_MODEL_ddbad6291eb640beb3efd2f84670b7c2"
            ],
            "layout": "IPY_MODEL_6fa4b9d5b1794b3b93b4d144a7ad5267"
          }
        },
        "1302aae912b64d65b99746da130df0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e20a307307a34310a9c3181a7b37fcd7",
            "placeholder": "​",
            "style": "IPY_MODEL_5cc9d42e07b442349fed6fca9ae891b6",
            "value": "100%"
          }
        },
        "13e050f1891c40f2bb61087fc9f9611f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f298208a7b704f10ba857f9c40914484",
            "placeholder": "​",
            "style": "IPY_MODEL_437feca94d5a4336b033ad29469189ea",
            "value": " 1000/1000 [00:04&lt;00:00, 245.81 examples/s]"
          }
        },
        "14fecebca32e4132a5bdb4da792f3490": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15fcaad9131b48f89ef3920b693785be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99e45f44024c498a93a9ef3d6fc31cff",
              "IPY_MODEL_a1e88dd9d9924baaaf0a53e8bbafa828",
              "IPY_MODEL_8ce0dcf3fca24b4ca42bb511c40873a8"
            ],
            "layout": "IPY_MODEL_325eb7eab03f4248895f6d8b0afa652e"
          }
        },
        "18161ad66ece4e9b9acffdfdadd3ae3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c9bb27828db438db7026190f14ecdb8",
            "placeholder": "​",
            "style": "IPY_MODEL_55c59c7003754c788a80a1ab3568c3ea",
            "value": "100%"
          }
        },
        "190ff54ebfe346a29b6536d595e4ce04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c563890b5bb40e888dc2de2baba68fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbbcfffb7f224122a9dbcf2824b227ac",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c64ec5d84874dd2a3dac0810c3c7471",
            "value": 10
          }
        },
        "1c9bb27828db438db7026190f14ecdb8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e3a6e26a9b04e459662d65e3b81dc6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e99d3606d0c4f4eb65f01345e7899f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ed7772108974312b0f0d65af13d13e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff2f822cce56465181886e60b28f2e9e",
            "placeholder": "​",
            "style": "IPY_MODEL_3d127a2d216a457698f54cc894ff929a",
            "value": "100%"
          }
        },
        "21a2ee88eeb64dcc8f714d6f82c516da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2234678abba549ed86e6183ac111d1cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2559491073d040fbbf4b128d8ffd8ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25fa2ac88ccf48ea9b669e4a030a9c7e",
            "placeholder": "​",
            "style": "IPY_MODEL_1e99d3606d0c4f4eb65f01345e7899f5",
            "value": "100%"
          }
        },
        "25fa2ac88ccf48ea9b669e4a030a9c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262d325ce86b43a7a2016a12c2bbb4bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f6ed4ff17e4f0aa36fe7f96ae29cc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2707158304974681906467abd0474e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ed7772108974312b0f0d65af13d13e1",
              "IPY_MODEL_f23ebc273c574980bcff2e8f514aa472",
              "IPY_MODEL_be642fd8b7774de693a76948984c0e5f"
            ],
            "layout": "IPY_MODEL_48202f7b8d3d49b9ae2bd3c6e8228ff9"
          }
        },
        "27523999de3249c59cd4df216503f06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513a8bf5e2d744c49502fa796d2e72b9",
            "placeholder": "​",
            "style": "IPY_MODEL_9306ba1297684b70962bbd370b4c3e12",
            "value": "100%"
          }
        },
        "2c4274bebf9d4e88beb202b55ac1e8b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c64ec5d84874dd2a3dac0810c3c7471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f36ee03d67b4051aa5e6c31572a4670": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325eb7eab03f4248895f6d8b0afa652e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3416784c4c3d4d368f714bcd32876811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "368229fe185246ce94c73656feb4a394": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e2c882574042a7957c16e3d1589a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b91911ef5e649489357efbb861bd4d4",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcdf90bc4b0a4918851a9c8508419f9e",
            "value": 10
          }
        },
        "38e8b483e91d4971807516c73d13ecfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d127a2d216a457698f54cc894ff929a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e5531f60bf24b1aa10e554c7b17e3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41cfab33fbf346c78a2782e3c8d94512",
            "placeholder": "​",
            "style": "IPY_MODEL_4b032cafbf5940af893300d84d8236d8",
            "value": " 10/10 [00:09&lt;00:00,  1.01s/it]"
          }
        },
        "3e6c000848314849a8c69ff3d6348fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41c7b16f4c41425091a3bba8dfd50dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41cfab33fbf346c78a2782e3c8d94512": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41ebe120d5b8491c886a394e31996d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4206d5739b98468797a3552085c40a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad8da4faaec430cb199f04cd8c402da",
            "placeholder": "​",
            "style": "IPY_MODEL_a88b68e8531a49018ba727e49ebe077f",
            "value": "Map: 100%"
          }
        },
        "424d79968dfd4870950d8c62f39e99d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4302920f04324c02afa69d62acd12fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa6786b6cfaf48f39f5c792296c8785c",
            "placeholder": "​",
            "style": "IPY_MODEL_d4beb39e76f14b6cab3e92b1e2441bec",
            "value": "100%"
          }
        },
        "437feca94d5a4336b033ad29469189ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46aa76c6d92843fe9fb05de05d9477c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46dba5afb6a440868585a12b4fef1e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48202f7b8d3d49b9ae2bd3c6e8228ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48d3f9b720464e1dae640b73c2a57449": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a60e5af0cec40148dcd18dd9d71e894": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b032cafbf5940af893300d84d8236d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b91911ef5e649489357efbb861bd4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5787921c8641b48b8c498729cc0cae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3d83a0c48d471889a37dd595d907e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e9426074f5b44968e990d7d9f70e1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4623b070f54d1892f36f4dbdfd6aea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513a8bf5e2d744c49502fa796d2e72b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "519c697bba9d41f5ac11e076a5246f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd40d911e2a54ff28e9e728694a17422",
            "placeholder": "​",
            "style": "IPY_MODEL_b4e51bfc19f54fbcb1b9c08f68637f03",
            "value": "100%"
          }
        },
        "522ff7c612654580a7bbef585f8796dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f9bae012e44466f850584f5702371b5",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f73045ef39ba458ba30219a9b2f008a1",
            "value": 13
          }
        },
        "52909928eb4e4525b14c6612397253a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55c59c7003754c788a80a1ab3568c3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55fd544ade8c4003a29e9efdd2a72431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5764c110fb494e548068408c1f6f0a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1302aae912b64d65b99746da130df0f0",
              "IPY_MODEL_b47455c08af84049bfcee26a9f94902b",
              "IPY_MODEL_9f8ac4d6b28349f4b71e5e0766b461e1"
            ],
            "layout": "IPY_MODEL_1e3a6e26a9b04e459662d65e3b81dc6a"
          }
        },
        "57aef1b4e4074d41a317e3bfe58cda3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18161ad66ece4e9b9acffdfdadd3ae3e",
              "IPY_MODEL_a9d882393ccf4d79890f943f8cae9049",
              "IPY_MODEL_830b423d68d6428f8b72230e697907ca"
            ],
            "layout": "IPY_MODEL_d559bf96a475449eb70b1d44a8f042d8"
          }
        },
        "591a47cc73a3455cb8c2093eaee75285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c31fa5e482d84a329009a6a2a6047518",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e6c000848314849a8c69ff3d6348fb1",
            "value": 13
          }
        },
        "5aac24d98b5240b081f343bd43212d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_068ef550ea1e428e955948c0e3d510ff",
              "IPY_MODEL_858250ece8524f408dad2cf66aaa963f",
              "IPY_MODEL_c476c984dcb746f09f155c75feb02a01"
            ],
            "layout": "IPY_MODEL_70ceefda43d148d699a596640612b106"
          }
        },
        "5ad8da4faaec430cb199f04cd8c402da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc9d42e07b442349fed6fca9ae891b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f0a736aeb324da3b6d38c762b6817aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f931d45a0fd4d67ac212fe57ba4c77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66da42c134fc4153848e5b8e9e577f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67883a0d301e4a74b555364ecbc57abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c6cf396ccbc4d2eb01ca72e6be646f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c5787921c8641b48b8c498729cc0cae",
            "placeholder": "​",
            "style": "IPY_MODEL_bf837e49d2334633b95d976a2e777e97",
            "value": " 10/10 [00:19&lt;00:00,  1.52s/it]"
          }
        },
        "6d2408b056c6441b8de21c7ce3a064fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f9bae012e44466f850584f5702371b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa4b9d5b1794b3b93b4d144a7ad5267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ceefda43d148d699a596640612b106": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b22ad8d7a44f3fa6e4e0dc0c4e7ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "769ed1beb70e4732a16d71d4503336b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7af589e463b34f65a693b34b0f3ef452": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "819d5af2feda4d5f98306a155e85584c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc6c70e8ce9f4c738752c37c45ba6146",
              "IPY_MODEL_ac2d6229c5b34bd7a4e1f39974d3177b",
              "IPY_MODEL_13e050f1891c40f2bb61087fc9f9611f"
            ],
            "layout": "IPY_MODEL_d96035053de64470bdd95863041935f7"
          }
        },
        "81df800d520d4fbfaaf1453a726bfdd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30315c03c7d48679cdf1d702d3a88de",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3416784c4c3d4d368f714bcd32876811",
            "value": 10
          }
        },
        "830b423d68d6428f8b72230e697907ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae615258a56d4d299ba4e84139d3c0db",
            "placeholder": "​",
            "style": "IPY_MODEL_190ff54ebfe346a29b6536d595e4ce04",
            "value": " 13/13 [00:01&lt;00:00,  7.90it/s]"
          }
        },
        "858250ece8524f408dad2cf66aaa963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9637bbdedeae46d7b7e8798216412289",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a97be6479505400d827d5da57b6c0e25",
            "value": 1000
          }
        },
        "85a0018232f24d07bd9be4093d766fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e5272e896448b780baafbcaede6669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8966e34911a148fcb133ef649cf1d694": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b807569247448cc90e815da1e035f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09cf1022197040ecb24d8b78e541d686",
              "IPY_MODEL_37e2c882574042a7957c16e3d1589a62",
              "IPY_MODEL_3e5531f60bf24b1aa10e554c7b17e3cf"
            ],
            "layout": "IPY_MODEL_2234678abba549ed86e6183ac111d1cb"
          }
        },
        "8cac2c9087ea4030b950957dcebbb775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ce0dcf3fca24b4ca42bb511c40873a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_368229fe185246ce94c73656feb4a394",
            "placeholder": "​",
            "style": "IPY_MODEL_4e9426074f5b44968e990d7d9f70e1a2",
            "value": " 1000/1000 [00:02&lt;00:00, 420.04 examples/s]"
          }
        },
        "8ce95a9e130f41f7a7b4bb01ad34d3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d667eba73924577bbd0e442744460bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9306ba1297684b70962bbd370b4c3e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9637bbdedeae46d7b7e8798216412289": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9944825679674618aebf69e5300ac922": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99e45f44024c498a93a9ef3d6fc31cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0cffe4c4fe484188325cbf9b42fc69",
            "placeholder": "​",
            "style": "IPY_MODEL_67883a0d301e4a74b555364ecbc57abd",
            "value": "Map: 100%"
          }
        },
        "9bff251202bb4c3a92ad490f42063fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f8ac4d6b28349f4b71e5e0766b461e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a0018232f24d07bd9be4093d766fa3",
            "placeholder": "​",
            "style": "IPY_MODEL_2c4274bebf9d4e88beb202b55ac1e8b4",
            "value": " 10/10 [00:09&lt;00:00,  1.03it/s]"
          }
        },
        "a1c329e1829b4e2daf0b5a4fb60a7096": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1e88dd9d9924baaaf0a53e8bbafa828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12b5c3f3c5be4d3caf33bd7a2639816d",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fd544ade8c4003a29e9efdd2a72431",
            "value": 1000
          }
        },
        "a30315c03c7d48679cdf1d702d3a88de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5aa27e56f574bfb8f484b8aad226e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5f846707ddd4926a966a48eeda6770d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fecebca32e4132a5bdb4da792f3490",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46dba5afb6a440868585a12b4fef1e29",
            "value": 1000
          }
        },
        "a88b68e8531a49018ba727e49ebe077f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8b50f92094240498a39a8e69b61a6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a97be6479505400d827d5da57b6c0e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9d882393ccf4d79890f943f8cae9049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1c329e1829b4e2daf0b5a4fb60a7096",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74b22ad8d7a44f3fa6e4e0dc0c4e7ea6",
            "value": 13
          }
        },
        "aa6786b6cfaf48f39f5c792296c8785c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2d6229c5b34bd7a4e1f39974d3177b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccd1ed2638184a01b58fb8c174fa2b13",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5aa27e56f574bfb8f484b8aad226e0d",
            "value": 1000
          }
        },
        "ad84bbb2a4134a999a650e2063289728": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec7118a5b75b4f3eb9e33ef4da7804b0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07ad5fe1ee4e4005a016268689c23f21",
            "value": 1
          }
        },
        "ae615258a56d4d299ba4e84139d3c0db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8ec75ebddf47d5847b6817df04f1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87e5272e896448b780baafbcaede6669",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ce95a9e130f41f7a7b4bb01ad34d3fc",
            "value": 13
          }
        },
        "b08ed45d57284684add54ee75dcc25d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b47455c08af84049bfcee26a9f94902b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b60c893e954991ae0bf64a4fd04e18",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52909928eb4e4525b14c6612397253a8",
            "value": 10
          }
        },
        "b4e51bfc19f54fbcb1b9c08f68637f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbbcfffb7f224122a9dbcf2824b227ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcdf90bc4b0a4918851a9c8508419f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be642fd8b7774de693a76948984c0e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc3c0b08d24e4cb0a128852fc655e538",
            "placeholder": "​",
            "style": "IPY_MODEL_c75deccf62ed42ef9a0596c44ee69222",
            "value": " 13/13 [00:01&lt;00:00,  8.07it/s]"
          }
        },
        "bf023783acf54eefbcf4215831f03f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d61761b8ecf44d779a267693ff84b3a1",
              "IPY_MODEL_1c563890b5bb40e888dc2de2baba68fd",
              "IPY_MODEL_c12fca0c23004325b6fa2715d8ffd4bc"
            ],
            "layout": "IPY_MODEL_41c7b16f4c41425091a3bba8dfd50dc1"
          }
        },
        "bf34b64526e148a4803908ac09e616b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf837e49d2334633b95d976a2e777e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c12fca0c23004325b6fa2715d8ffd4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b884fecb4e4dca91ea8ea8d94fb6cf",
            "placeholder": "​",
            "style": "IPY_MODEL_06478dbf7f2f4234a2ccb93ef900e43a",
            "value": " 10/10 [00:10&lt;00:00,  1.12s/it]"
          }
        },
        "c31fa5e482d84a329009a6a2a6047518": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c476c984dcb746f09f155c75feb02a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2408b056c6441b8de21c7ce3a064fa",
            "placeholder": "​",
            "style": "IPY_MODEL_7af589e463b34f65a693b34b0f3ef452",
            "value": " 1000/1000 [00:00&lt;00:00, 25807.93 examples/s]"
          }
        },
        "c68bf9a905ab48b3a039068f46ed7d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c75deccf62ed42ef9a0596c44ee69222": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb0cffe4c4fe484188325cbf9b42fc69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd1ed2638184a01b58fb8c174fa2b13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd40d911e2a54ff28e9e728694a17422": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd56ce6f651a42afbf7227d8de08d83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9944825679674618aebf69e5300ac922",
            "placeholder": "​",
            "style": "IPY_MODEL_21a2ee88eeb64dcc8f714d6f82c516da",
            "value": " 13/13 [00:01&lt;00:00,  8.19it/s]"
          }
        },
        "d06660b031934cf68845e8103b416e53": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c8a6bfa33740c98c03ddc6e535641f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4beb39e76f14b6cab3e92b1e2441bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d559bf96a475449eb70b1d44a8f042d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d61761b8ecf44d779a267693ff84b3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06660b031934cf68845e8103b416e53",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3d83a0c48d471889a37dd595d907e2",
            "value": "100%"
          }
        },
        "d85fe4f1a46b4c329b20d931ade92360": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d924c0e6d25f480ba3e4d53b5bc8f15c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96035053de64470bdd95863041935f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daab73c3ee7945ec8548217db68fcd50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4302920f04324c02afa69d62acd12fe4",
              "IPY_MODEL_af8ec75ebddf47d5847b6817df04f1e0",
              "IPY_MODEL_0a9d9a96f21a481a8853f9654e9887f4"
            ],
            "layout": "IPY_MODEL_2f36ee03d67b4051aa5e6c31572a4670"
          }
        },
        "dc6c70e8ce9f4c738752c37c45ba6146": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b50f92094240498a39a8e69b61a6bd",
            "placeholder": "​",
            "style": "IPY_MODEL_38e8b483e91d4971807516c73d13ecfa",
            "value": "Map: 100%"
          }
        },
        "ddbad6291eb640beb3efd2f84670b7c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d222aa7cc7042879d009ac7bc99b145",
            "placeholder": "​",
            "style": "IPY_MODEL_bf34b64526e148a4803908ac09e616b8",
            "value": " 1/1 [00:02&lt;00:00,  2.30s/it]"
          }
        },
        "dea714846de44ffdaceb66bdf6c4cd50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f0a736aeb324da3b6d38c762b6817aa",
            "placeholder": "​",
            "style": "IPY_MODEL_424d79968dfd4870950d8c62f39e99d7",
            "value": "100%"
          }
        },
        "e04af0731a5f43608f2f2e56b0cb8b69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27523999de3249c59cd4df216503f06c",
              "IPY_MODEL_e0d5ffa378d143a7851a8013c083c451",
              "IPY_MODEL_1212dd3253fb49edb5e362f6b797ec29"
            ],
            "layout": "IPY_MODEL_d924c0e6d25f480ba3e4d53b5bc8f15c"
          }
        },
        "e0d5ffa378d143a7851a8013c083c451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8966e34911a148fcb133ef649cf1d694",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f735cca63c344f8fb2d2cea307d1b561",
            "value": 10
          }
        },
        "e20a307307a34310a9c3181a7b37fcd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28545e1e09b4a8dbd022495ad9444fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_519c697bba9d41f5ac11e076a5246f22",
              "IPY_MODEL_591a47cc73a3455cb8c2093eaee75285",
              "IPY_MODEL_ffca8a23ba5944679de5a2fd8a6584d1"
            ],
            "layout": "IPY_MODEL_26f6ed4ff17e4f0aa36fe7f96ae29cc1"
          }
        },
        "e47bde74cbdf44fb93dc4a3705428827": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2559491073d040fbbf4b128d8ffd8ef4",
              "IPY_MODEL_81df800d520d4fbfaaf1453a726bfdd5",
              "IPY_MODEL_6c6cf396ccbc4d2eb01ca72e6be646f9"
            ],
            "layout": "IPY_MODEL_4a60e5af0cec40148dcd18dd9d71e894"
          }
        },
        "e49edba82f7d45ddb69e47e6d1407a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4206d5739b98468797a3552085c40a4f",
              "IPY_MODEL_a5f846707ddd4926a966a48eeda6770d",
              "IPY_MODEL_fad7d980afb84a1284d9b603954b32f0"
            ],
            "layout": "IPY_MODEL_41ebe120d5b8491c886a394e31996d6b"
          }
        },
        "ec7118a5b75b4f3eb9e33ef4da7804b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edf78046ee2c4d99917942916de3a3ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f23ebc273c574980bcff2e8f514aa472": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d667eba73924577bbd0e442744460bc",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66da42c134fc4153848e5b8e9e577f2c",
            "value": 13
          }
        },
        "f298208a7b704f10ba857f9c40914484": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b884fecb4e4dca91ea8ea8d94fb6cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f628e1abbfdc4aef959c080a53d9f2db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4623b070f54d1892f36f4dbdfd6aea",
            "placeholder": "​",
            "style": "IPY_MODEL_c68bf9a905ab48b3a039068f46ed7d66",
            "value": "100%"
          }
        },
        "f73045ef39ba458ba30219a9b2f008a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f735cca63c344f8fb2d2cea307d1b561": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fad7d980afb84a1284d9b603954b32f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_262d325ce86b43a7a2016a12c2bbb4bb",
            "placeholder": "​",
            "style": "IPY_MODEL_46aa76c6d92843fe9fb05de05d9477c6",
            "value": " 1000/1000 [00:04&lt;00:00, 234.01 examples/s]"
          }
        },
        "fc3c0b08d24e4cb0a128852fc655e538": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2f822cce56465181886e60b28f2e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffca8a23ba5944679de5a2fd8a6584d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00cf2f5a574140d4a5111aebba111127",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c8a6bfa33740c98c03ddc6e535641f",
            "value": " 13/13 [00:01&lt;00:00,  8.22it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
